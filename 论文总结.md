# Sphinteract 论文创新点与实验步骤总结

## 一、论文核心创新点

### 1. **交互式澄清问题机制（Interactive Clarification Questions）**
   - **核心思想**：通过主动询问用户澄清问题来解决自然语言查询中的歧义性
   - **创新之处**：不是被动接受用户输入，而是主动识别歧义并询问用户以获取更精确的信息

### 2. **四类歧义分类框架（Ambiguity Taxonomy）**
   代码中定义了四种歧义类型：
   - **AmbQuestion**：问题本身存在歧义
   - **AmbTableColumn**：从问题到数据库表和列的映射存在歧义
   - **AmbOutput**：输出表应包含哪些字段和多少字段存在歧义
   - **AmbValue**：用于过滤结果的谓词值存在歧义

### 3. **SRA机制（Self-Reflection and Asking）**
   - **自我反思**：分析之前生成的错误SQL查询，识别问题所在
   - **智能提问**：基于错误SQL和数据库模式，生成有针对性的多选题澄清问题
   - **逐步消歧**：通过多轮交互逐步消除歧义，直到生成正确的SQL

### 4. **反馈驱动的SQL生成**
   - 使用用户对澄清问题的回答来指导SQL生成
   - 结合之前的错误SQL查询和用户反馈，生成改进的SQL
   - 支持少样本学习，从用户研究数据中选择相似示例

### 5. **自动反馈生成机制**
   - 使用GPT-4o模型根据黄金SQL查询自动生成对澄清问题的反馈
   - 这允许在没有真实用户参与的情况下进行大规模实验

### 6. **早停机制（Early Stopping）**
   - 当SRA机制判断没有剩余歧义时（"NO AMBIGUITY"），提前停止提问
   - 提高效率，避免不必要的交互

## 二、详细实验步骤

### 实验设置

#### 1. **数据集**
   - **KaggleDBQA**：包含272个测试问题（排除用户研究问题后）
   - **BIRD**：包含500个测试问题
   - 两个数据集都包含自然语言问题、对应的SQL查询和数据库模式

#### 2. **模型配置**
   - **GPT-3.5-turbo**：用于SQL生成和澄清问题生成
   - **GPT-4-turbo**：用于SQL生成和澄清问题生成
   - **GPT-4o**：专门用于生成对澄清问题的反馈
   - **Claude-3-Haiku**：作为替代模型进行实验

#### 3. **实验方法对比**

##### 方法1：Baseline（基线方法）
   - **零样本基线**：直接生成SQL，如果错误则进行自我调试（self-debug）
   - **少样本基线**：使用few-shot示例进行SQL生成和自我调试
   - **特点**：不询问澄清问题，仅基于错误SQL进行自我修正

##### 方法2：Clarification Questions（澄清问题方法）
   - **流程**：
     1. 生成初始SQL查询
     2. 如果SQL执行结果不匹配，使用SRA机制生成澄清问题
     3. 使用GPT-4o根据黄金SQL自动生成反馈
     4. 基于反馈重新生成SQL
     5. 重复步骤2-4，最多4轮
   - **特点**：持续询问澄清问题直到SQL正确或达到最大轮数

##### 方法3：Break No Ambiguity（早停澄清问题方法）
   - **流程**：与方法2相同，但增加了早停机制
   - **特点**：当SRA判断没有剩余歧义时，停止提问并直接生成SQL

### 详细实验流程

#### 步骤1：初始化
```python
# 从batch API加载初始SQL查询（第0轮）
# 使用与DAIL-SQL相同的提示格式（代码表示+规则）
history_log[index]['sql_log'] = [[0, 'see batch api', sql_query, perplexity_score]]
```

#### 步骤2：SQL执行与验证
```python
# 执行SQL并验证结果
execution, exception = evalfunc(sql_query, gold, dbname, data_source)
# evalfunc比较执行结果：
# - 结果行数必须匹配
# - 对于有ORDER BY的查询，逐行比较
# - 对于无ORDER BY的查询，使用集合比较
```

#### 步骤3：处理执行异常
```python
# 如果SQL执行出错，使用fix_invalid_v1提示修复
if exception:
    invalid_prompt = fix_invalid_v1.format(
        schema=dbschema, 
        question=nlq,
        invalidSQL=most_recent_sql, 
        ex=exception[0]
    )
    sql = generation(invalid_prompt)
```

#### 步骤4：生成澄清问题（仅限方法2和方法3）
```python
# 使用SRA提示生成澄清问题
cq_prompt = SRA.format(
    schema=dbschema, 
    question=nlq,
    sqls=",\n".join(query),  # 之前的错误SQL
    cqs=cqas  # 之前的澄清问题和回答
)
cq = generation(cq_prompt)
```

#### 步骤5：生成反馈（仅限方法2和方法3）
```python
# 使用GPT-4o根据黄金SQL生成反馈
feedback_prompt = feedback_v2.format(
    query=gold,  # 黄金SQL
    question=cq,  # 澄清问题
    nlq=nlq  # 原始问题
)
feedback = GPT4o_generation(feedback_prompt)
```

#### 步骤6：基于反馈重新生成SQL
```python
# 结合澄清问题和反馈生成新SQL
sql_prompt = sql_generation_v2.format(
    schema=dbschema,
    question=nlq,
    sqls="\n".join(query),  # 之前的错误SQL
    cqas=cqas,  # 澄清问题和反馈
    metadata=evidence  # BIRD数据集的元数据（可选）
)
sql_query = generation(sql_prompt)
```

#### 步骤7：少样本学习（Few-Shot）
```python
# 使用语义相似度选择示例
feedback_example_selector = SemanticSimilarityExampleSelector(
    vectorstore=vectorstore_feedback,
    k=num_examples,  # 1, 3, 5个示例
)
# 从用户研究数据中选择相似示例
sql_prompt = sql_generation_feedback_few_shot_prompt.format(...)
```

### 实验配置参数

#### 零样本实验（zeroshot_experiments.ipynb）
- **轮数**：最多4轮
- **Shot数**：0-shot
- **数据集**：KaggleDBQA和BIRD
- **元数据**：BIRD数据集可选择是否使用evidence元数据

#### 少样本实验（FewShotAmbSQL.ipynb）
- **轮数**：最多4轮
- **Shot数**：1-shot, 3-shot, 5-shot
- **数据集**：KaggleDBQA和BIRD
- **示例选择**：基于语义相似度从用户研究数据中选择

### 评估指标

1. **执行匹配率（Execution Match）**：
   - 比较生成的SQL和黄金SQL的执行结果
   - 考虑行数、行内容、ORDER BY等

2. **澄清问题数量（num_cq_asked）**：
   - 记录每个问题需要询问的澄清问题数量
   - 0表示初始SQL就正确
   - "Failed"表示4轮后仍未成功

3. **困惑度分数（Perplexity Score）**：
   - 使用logprobs计算生成SQL的困惑度
   - 用于分析模型对生成SQL的置信度

### 实验日志结构

每个实验日志（pkl文件）包含：
```python
history_log[index] = {
    'sql_log': [(order, prompt, sql, pscore), ...],  # SQL生成历史
    'cq_log': [(order, prompt, cq, pscore), ...],    # 澄清问题历史
    'feedback_log': [(order, prompt, feedback, pscore), ...],  # 反馈历史
    'num_cq_asked': int or "Failed"  # 询问的澄清问题数量
}
```

## 三、关键技术细节

### 1. SQL清理函数
```python
def clean_query(sql_query):
    sql_query = sql_query.replace("```sql", '')
    sql_query = sql_query.replace("```", '')
    sql_query = sql_query.replace(';', '')
    sql_query = sql_query.replace('"""', '')
    if 'SELECT' not in sql_query.upper()[:10]:
        sql_query = 'SELECT ' + sql_query
    return sql_query
```

### 2. 执行结果比较
- 使用xxhash进行结果哈希比较
- 对于ORDER BY查询，逐行比较
- 对于无ORDER BY查询，使用集合比较（列顺序无关）

### 3. 超时处理
- SQL执行超时设置为120秒
- 使用多进程执行SQL，避免阻塞主进程

### 4. 提示工程
- 使用代码表示格式（code representation）
- 包含详细的few-shot示例
- 明确指导模型生成多选题格式的澄清问题

## 四、用户研究

- **目的**：验证同一问题可能对应不同的SQL答案
- **数据**：64个KaggleDBQA问题
- **发现**：用户对同一问题可能有完全不同的SQL期望
- **应用**：用户研究数据用于少样本学习的示例选择

## 五、实验创新总结

1. **首次系统性地使用交互式澄清问题解决NL2SQL歧义**
2. **提出了四类歧义分类框架，系统化处理不同类型的歧义**
3. **结合自我反思和主动提问，形成完整的交互式SQL生成流程**
4. **使用自动反馈生成机制，实现大规模实验**
5. **支持零样本和少样本两种学习范式**
6. **在KaggleDBQA和BIRD两个真实数据集上验证有效性**

