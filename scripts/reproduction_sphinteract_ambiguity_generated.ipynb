{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e84ce24",
   "metadata": {},
   "source": [
    "# Reproduction of Sphinteract on KaggleDBQA (Ambiguity Filtered)\n",
    "\n",
    "This notebook reproduces the three methods described in the paper \"Sphinteract: ...\":\n",
    "1.  **Baseline (Method 1)**: Zero-shot generation with Self-Correction (Fix Invalid).\n",
    "2.  **Sphinteract (Method 2)**: Interactive framework with Clarification Questions (SRA) and Feedback.\n",
    "3.  **Break No Ambiguity (Method 3)**: Similar to Method 2 but with an Early Stopping mechanism.\n",
    "\n",
    "We use the **KaggleDBQA** dataset.\n",
    "\n",
    "**Modification**: Instead of random sampling, we filter for 20 \"Ambiguous\" samples using GPT-3.5-Turbo.\n",
    "\n",
    "## Requirements\n",
    "- `openai`\n",
    "- `pandas`\n",
    "- `sqlite3`\n",
    "- `numpy`\n",
    "- `python-dotenv`\n",
    "- `tiktoken`\n",
    "- `xxhash`\n",
    "\n",
    "A `.env` file with `OPENAI_API_KEY` and `OPENAI_BASE_URL` (optional) is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d9cb324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: userstudy_chroma directory not found. Few-shot retrieval may fail.\n",
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import nbformat as nbf\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import time\n",
    "import xxhash\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Process, Queue\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "# Add current directory and parent directory to path to allow importing local modules\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from reproduction_utils import execute_query_worker\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in your .env file.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url if base_url else \"https://api.openai.com/v1\"\n",
    ")\n",
    "\n",
    "# Initialize Vectorstore for Few-Shot\n",
    "try:\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "    if os.path.exists(\"./userstudy_chroma\"):\n",
    "        vectorstore = Chroma(persist_directory=\"./userstudy_chroma\", embedding_function=embeddings)\n",
    "        print(\"Vectorstore loaded successfully.\")\n",
    "    else:\n",
    "        print(\"Warning: userstudy_chroma directory not found. Few-shot retrieval may fail.\")\n",
    "        vectorstore = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading vectorstore: {e}\")\n",
    "    vectorstore = None\n",
    "\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe6b637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle_dataset.csv not found. Set KAGGLE_DATASET_PATH or place file under data/datasets.\n",
      "Loaded 0 records.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_FILENAME = os.getenv('DATASET_FILENAME', 'kaggle_dataset.csv')\n",
    "KAGGLE_DATASET_PATH = os.getenv('KAGGLE_DATASET_PATH')\n",
    "\n",
    "def resolve_dataset_path():\n",
    "    if KAGGLE_DATASET_PATH:\n",
    "        p = Path(KAGGLE_DATASET_PATH).expanduser()\n",
    "        if p.exists():\n",
    "            return p\n",
    "    p2 = Path(DATASET_FILENAME).expanduser()\n",
    "    if p2.exists():\n",
    "        return p2\n",
    "    p3 = resolve_file(DATASET_FILENAME)\n",
    "    return p3\n",
    "\n",
    "dataset_p = resolve_dataset_path()\n",
    "if dataset_p is None:\n",
    "    print('kaggle_dataset.csv not found. Set KAGGLE_DATASET_PATH or place file under data/datasets.')\n",
    "    df = pd.DataFrame()\n",
    "    print(f'Loaded {len(df)} records.')\n",
    "else:\n",
    "    dataset_path = str(dataset_p)\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f'Loaded {len(df)} records from {dataset_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea2bc7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "83337741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "03151604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(sql_query):\n",
    "    # Remove markdown code blocks\n",
    "    pattern = r\"```sql(.*?)```\"\n",
    "    match = re.search(pattern, sql_query, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        sql_query = match.group(1)\n",
    "    else:\n",
    "        sql_query = sql_query.replace(\"```sql\", '').replace(\"```\", '')\n",
    "\n",
    "    sql_query = sql_query.replace(';', '')\n",
    "    sql_query = sql_query.replace('\"\"\"', '')\n",
    "    \n",
    "    # Find the start of the SQL statement (SELECT or WITH)\n",
    "    match_select = re.search(r'\\bSELECT\\b', sql_query, re.IGNORECASE)\n",
    "    match_with = re.search(r'\\bWITH\\b', sql_query, re.IGNORECASE)\n",
    "    \n",
    "    start_index = -1\n",
    "    if match_with and match_select:\n",
    "        start_index = min(match_with.start(), match_select.start())\n",
    "    elif match_with:\n",
    "        start_index = match_with.start()\n",
    "    elif match_select:\n",
    "        start_index = match_select.start()\n",
    "        \n",
    "    if start_index != -1:\n",
    "        sql_query = sql_query[start_index:]\n",
    "    else:\n",
    "        # If no SELECT/WITH found, assume it's a completion and prepend SELECT\n",
    "        # But ensure we don't double prepend if the user output \" * FROM ...\"\n",
    "        if 'FROM' in sql_query.upper():\n",
    "             sql_query = 'SELECT ' + sql_query\n",
    "        else:\n",
    "             # Fallback: just prepend SELECT\n",
    "             sql_query = 'SELECT ' + sql_query\n",
    "             \n",
    "    return sql_query.strip()\n",
    "\n",
    "def generate_db_schema(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    schemas = []\n",
    "    for table in tables:\n",
    "        if table[0] == 'sqlite_sequence':\n",
    "            continue\n",
    "        cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table[0]}'\")\n",
    "        create_prompt = cursor.fetchone()[0]\n",
    "        schemas.append(create_prompt)\n",
    "    conn.close()\n",
    "    return \"\\n\\n\".join(schemas)\n",
    "\n",
    "def evalfunc(sql_source, sql_target, db_path):\n",
    "    if not os.path.isfile(db_path):\n",
    "        return False, [FileNotFoundError(f\"Database not found: {db_path}\")]\n",
    "    \n",
    "    timeout = 30 # seconds\n",
    "    output = Queue()\n",
    "    p = Process(target=execute_query_worker, args=(db_path, sql_source, output))\n",
    "    p.start()\n",
    "    \n",
    "    try:\n",
    "        source_results = output.get(timeout=timeout)\n",
    "        p.join()\n",
    "        if isinstance(source_results, Exception):\n",
    "            return False, [source_results]\n",
    "    except Exception as e:\n",
    "        p.terminate()\n",
    "        return False, [e]\n",
    "        \n",
    "    # Execute Gold SQL (Target) - assumed safe and fast\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        target_results = cursor.execute(sql_target).fetchall()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        return False, [e]\n",
    "\n",
    "    # Compare results\n",
    "    if len(source_results) != len(target_results):\n",
    "        return False, []\n",
    "        \n",
    "    # Heuristic comparison (order-independent if no ORDER BY, else strict)\n",
    "    if 'ORDER BY' in sql_target.upper():\n",
    "        return source_results == target_results, []\n",
    "    else:\n",
    "        # Sort both by a stable key (str representation)\n",
    "        s_sorted = sorted(list(source_results), key=lambda x: str(x))\n",
    "        t_sorted = sorted(list(target_results), key=lambda x: str(x))\n",
    "        return s_sorted == t_sorted, []\n",
    "\n",
    "def LLM_generation(prompt, model='gpt-3.5-turbo', temperature=0.0, retries=3, retry_delay=1.5, log_each_retry=False, fallback_models=None):\n",
    "    last_err = None\n",
    "    models_to_try = [model]\n",
    "    if fallback_models is None:\n",
    "        fallback_models = [os.getenv(\"AMBIGUITY_MODEL\", \"gpt-4o-mini\"), \"gpt-4o\"]\n",
    "    for m in fallback_models:\n",
    "        if m not in models_to_try:\n",
    "            models_to_try.append(m)\n",
    "    for try_model in models_to_try:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=try_model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=4096\n",
    "                )\n",
    "                return response.choices[0].message.content.strip(), 0.0\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                err_str = str(e)\n",
    "                l = err_str.lower()\n",
    "                if \"502\" in err_str or \"bad gateway\" in l:\n",
    "                    err_label = \"502 Bad Gateway\"\n",
    "                elif \"429\" in err_str or \"too many requests\" in l or \"rate limit\" in l:\n",
    "                    err_label = \"rate_limit\"\n",
    "                elif \"timeout\" in l or \"timed out\" in l:\n",
    "                    err_label = \"timeout\"\n",
    "                elif \"connection reset\" in l or \"reset by peer\" in l:\n",
    "                    err_label = \"conn_reset\"\n",
    "                elif \"ssl\" in l:\n",
    "                    err_label = \"ssl_error\"\n",
    "                elif \"unauthorized\" in l or \"401\" in err_str:\n",
    "                    err_label = \"unauthorized\"\n",
    "                elif \"model\" in l and \"not found\" in l:\n",
    "                    err_label = \"model_not_found\"\n",
    "                else:\n",
    "                    err_label = \"error\"\n",
    "                if log_each_retry:\n",
    "                    print(f\"LLM error ({err_label}), retry {attempt+1}/{retries}\")\n",
    "                time.sleep(retry_delay * (1.5 ** attempt))\n",
    "    if last_err is not None:\n",
    "        err_str = str(last_err)\n",
    "        l = err_str.lower()\n",
    "        if \"502\" in err_str or \"bad gateway\" in l:\n",
    "            err_label = \"502 Bad Gateway\"\n",
    "        elif \"429\" in err_str or \"too many requests\" in l or \"rate limit\" in l:\n",
    "            err_label = \"rate_limit\"\n",
    "        elif \"timeout\" in l or \"timed out\" in l:\n",
    "            err_label = \"timeout\"\n",
    "        elif \"connection reset\" in l or \"reset by peer\" in l:\n",
    "            err_label = \"conn_reset\"\n",
    "        elif \"ssl\" in l:\n",
    "            err_label = \"ssl_error\"\n",
    "        elif \"unauthorized\" in l or \"401\" in err_str:\n",
    "            err_label = \"unauthorized\"\n",
    "        elif \"model\" in l and \"not found\" in l:\n",
    "            err_label = \"model_not_found\"\n",
    "        else:\n",
    "            err_label = \"error\"\n",
    "        print(f\"LLM error ({err_label}); giving up\")\n",
    "    return \"SELECT * FROM error\", 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e991f",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "695981f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts (Aligned with FewShotAmbSQL.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "92700726",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRA = \"\"\"/* Ask the user a new multiple choice clarification question to help you find the correct SQL answer for the following question: */\n",
    "{question}\n",
    "/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following previous clarification questions and user replies: */\n",
    "{cqs}\n",
    "\n",
    "/* Consider the following ambiguity categories:\n",
    "    - AmbQuestion: Is the question itself ambiguous?\n",
    "    - AmbTableColumn: Is there ambiguity in mapping the entities from the QUESTION to tables and columns in the DATABASE SCHEMA?\n",
    "    - AmbOutput: What fields and how many fields should be included in the output table?\n",
    "    - AmbValue: What predicate value should be used to filter results?\n",
    "*/\n",
    "\n",
    "/* The clarification question should be easy to understand for people with no coding experience. */\n",
    "\n",
    "/* Let's think step by step to generate the helpful multiple choice clarification question.\n",
    "1. Summarize the clear information based on previous clarification questions and incorrect queries.\n",
    "2. Evaluate whether AmbQuestion, AmbTableColumn, AmbOutput, and AmbValue remain in formulating an SQL query, considering each category individually.\n",
    "3. Ask a new multiple-choice question to address the remaining ambiguities and assist in identifying the correct SQL query. Use format: mul_choice_cq = \"\".\n",
    "4. Prioritize granularity alignment and valid join keys; avoid suggesting joins across incompatible levels (e.g., district vs state) or metrics that cannot be computed at a common grain.\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "SRA_ES = \"\"\"/* Ask the user a new multiple choice clarification question to help you find the correct SQL answer for the following question: */\n",
    "{question}\n",
    "/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following previous clarification questions and user replies: */\n",
    "{cqs}\n",
    "\n",
    "/* Consider the following ambiguity categories:\n",
    "    - AmbQuestion: Is the question itself ambiguous?\n",
    "    - AmbTableColumn: Is there ambiguity in mapping the entities from the QUESTION to tables and columns in the DATABASE SCHEMA?\n",
    "    - AmbOutput: What fields and how many fields should be included in the output table?\n",
    "    - AmbValue: What predicate value should be used to filter results?\n",
    "*/\n",
    "\n",
    "/* The clarification question should be easy to understand for people with no coding experience. */\n",
    "\n",
    "/* Let's think step by step to generate the helpful multiple choice clarification question.\n",
    "1. Summarize the clear information based on previous clarification questions and incorrect queries.\n",
    "2. Evaluate whether AmbQuestion, AmbTableColumn, AmbOutput, and AmbValue remain in formulating an SQL query, considering each category individually.\n",
    "3. If no remaining ambiguities are identified, then output \"NO AMBIGUITY\".\n",
    "   Else, ask a new multiple-choice question to address the remaining ambiguities and assist in identifying the correct SQL query. Use format: mul_choice_cq = \"\".\n",
    "4. Prioritize granularity alignment and valid join keys; avoid suggesting joins across incompatible levels (e.g., district vs state) or metrics that cannot be computed at a common grain.\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "sql_generation_v2 = \"\"\"/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following user replies to help you write the correct sql query: */\n",
    "{cqas}\n",
    "\n",
    "{metadata}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "/* Output ONLY SQL wrapped in a markdown block: ```sql */\n",
    "\"\"\"\n",
    "\n",
    "fix_invalid_v1 = \"\"\"/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following inexecutable sql query */\n",
    "{invalidSQL}\n",
    "/* And the following exception message */\n",
    "{ex}\n",
    "\n",
    "/* Fix the exception and write a new executable SQL query with no explanation */\n",
    "/* Output ONLY SQL wrapped in a markdown block: ```sql */\n",
    "\"\"\"\n",
    "\n",
    "sql_generation_selfdebug = \"\"\"/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "\n",
    "{metadata}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "/* Output ONLY SQL wrapped in a markdown block: ```sql */\n",
    "\"\"\"\n",
    "\n",
    "def build_metadata_constraints(nlq, schema):\n",
    "    s_lower = schema.lower()\n",
    "    n_lower = nlq.lower()\n",
    "    cons = []\n",
    "    cons.append(\"Constraints: SQL must be executable on the given schema and use a single consistent granularity across tables.\")\n",
    "    cons.append(\"When tables differ in granularity, aggregate to a common key before joining; do not join district/school rows directly to state-level aggregates.\")\n",
    "    cons.append(\"Use only valid join keys present in the schema with matching types; prefer exact equality joins.\")\n",
    "    cons.append(\"If feedback conflicts with these constraints, follow the constraints.\")\n",
    "    if (\"finrev_fed_17\" in s_lower) and (\"ndecorexcel_math_grade8\" in s_lower):\n",
    "        cons.append(\"For FINREV_FED_17 with NDECoreExcel_Math_Grade8, compute metrics at state+year granularity: aggregate revenue by state_code and yr_data, join with NDECoreExcel_Math_Grade8 state and year; do not select district-level columns unless scores exist at the same granularity.\")\n",
    "    if (\"resultsdata15\" in s_lower) and (\"lod\" in s_lower or \"limit of detection\" in n_lower):\n",
    "        cons.append(\"For 'easiest to be tested' tasks, use the pesticide with lowest average LOD; if LOD is unavailable, use the highest count of test records.\")\n",
    "    meta = \"\\n\".join([f\"/* {c} */\" for c in cons])\n",
    "    return meta\n",
    "\n",
    "cq_prefix_v1 = '''/* some examples are provided */\n",
    "/* example question: */\n",
    "Which artist/group is most productive?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: How to rank artist/group productivity? a) rank by the number of records produced, b) rank by the total number of downloads, c) other (please specify).\n",
    "user: b) rank by the total number of downloads```\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer should use ORDER BY and LIMIT 1 based on the sum of total downloads. However, it is unclear what columns should be used to represent the 'artist/group'.  Both the `artist` and the `groupName` columns contain information about 'artist/group'. ‚Äô‚ÄòAmbTableColumn‚Äô remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \"Which columns represent the 'artist/group' information? a) the artist column only, b) the groupName column only, c) both the artist column and the groupName column, d) other (please specify).‚Äù```\n",
    "\n",
    "/* example question: */\n",
    "Which Premier League matches ended in a draw in 2016?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: Is the year '2016' referring to? a) season is 2016, b) season is either 2015/2016 or 2016/2017, c) the date time is at year 2016, d) other (specify).\n",
    "user: a) season is 2016,\n",
    "clarification questions: How to find the 'Premier league'? a) consider all leagues, b) consider only the league with name 'Premier League', c) other (specify).\n",
    "user: b) consider only the league with name 'Premier League'\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer to this question needs to contain a WHERE clause for three conditions: 'Premier League', 'draw', and 'in 2016'. However, the question did not specify what fields should be contained in the output table. 'AmbOutput' remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = ‚ÄúWhat fields represent the target 'matches'? a) all fields from football data table, b) the `league` column, c) other (specify).‚Äù\n",
    "\n",
    "/* example question: */\n",
    "Which type of crime has the highest rate of ‚ÄòInvestigation complete‚Äô?\n",
    "/* example previous clarification questions and user replies: */\n",
    "No previous clarification questions.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer to this question needs to contain a WHERE clause to find crimes that have 'Investigation complete' outcomes, uses ORDER BY and LIMIT 1 to find the type of crime with the highest rate, and the output table has only one row. However, it needs to be clarified i) what predicate value should be used for 'Investigation complete', and ii) how to represent the 'rate', and iii) if the output table contains only the crime type column or the crime type column with the highest rate aggregate. Hence, this question is ambiguous because of 'AmbVal', 'AmbQuestion', and ‚ÄòAmbOutput‚Äô.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = ‚ÄúWhat information should be used to find 'Investigation complete'? a) see if outcome contains the phrase 'Investigation complete', b)  see if outcome is 'Investigation complete; no suspect identified', c) other (please specify).‚Äù\n",
    "\n",
    "/* example question: */\n",
    "For award winners, which position has the most hall of fame players?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: How should the 'position' for players be identified? a) by the `award_id` column, b) by the `category` column, c) other (please specify).\n",
    "user: c)  by the `note` column\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the answer should use the `note` column for player ‚Äòpositions‚Äô. However, it is unclear what fields should contain in the output table. Hence ‚ÄòAmbOutput‚Äô remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = ‚ÄúWhat fields should be contained in the output table? a) one field: the position, b) two fields: the position and the number of hall-of-fame players, c) other (please specify).‚Äù\n",
    "\n",
    "/* example question: */\n",
    "How many Wisconsin school districts receive federal funding?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: How to determine if a district has received federal funding? a) based on the t_fed_rev is larger than 0, b) the answer does not need to consider this aspect, c) other (please specify).\n",
    "user: c) every school in `FINREV_FED_17` table has received federal funding.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that every school in `FINREV_FED_17` table have received federal funding. However, it is unclear if the word ‚ÄòWisconsin‚Äô refers to the state or the school district. Hence, ‚ÄòAmbQuestion‚Äô remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = ‚ÄúIs 'Wisconsin school districts' referring to? a) all school districts in the state Wisconsin, b) school districts with names that contain Wisconsin, c) other (please specify).‚Äù\n",
    "\n",
    "/* example question: */\n",
    "How many 2-year public schools are there in \"California\"?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: Which column(s) should be used to find ‚Äò2-year public schools‚Äô? a) `level` column, b) `control` column, c) other (please specify).\n",
    "user: c) use both `level` and `control` columns to find ‚Äò2-year public schools‚Äô information.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the correct SQL answer should have a WHERE clause with filters based on the `level` and `control` columns. However, it is unclear what predicate values should be used for these two columns. Hence, ‚ÄòAmbValue‚Äô remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = ‚ÄúWhat predicate values should be used for the `level` and `control` columns to find  ‚Äò2-year public schools‚Äô? a) ‚Äò2-year‚Äô and ‚Äòpublic‚Äô b) ‚Äò2‚Äô and ‚Äòpublic, c) other (please specify)‚Äô.‚Äù \n",
    "\n",
    "/* example question: */\n",
    "Calculate the total beat of the crimes reported in a community area in the central side with a population of 50,000 and above.\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: What column and predicate value should be used to determine ‚Äòcentral side‚Äô? a) Column `side` in table `Community_Area` with value ‚Äòcentral‚Äô, b) Column `side` in table `Community_Area` with value ‚ÄòCentral‚Äô, c) other (please specify).\n",
    "user: b) Column `side` in table `Community_Area` with value ‚ÄòCentral‚Äô\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the output table should contain a single number and use the predicate ‚ÄòCentral‚Äô in `Community_Area`.`side`; However, it is not clear which column of statistics is ‚Äòtotal beat‚Äô referring to. Hence, AmbTableColumn remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = ‚ÄúWhich column is related to ‚Äòtotal beat‚Äô? a) `Crime`.`beat`, b) `Crime`.`report_no`, c) other (please specify).‚Äù\n",
    "\n",
    "/* example question: */\n",
    "Of all the nonessential genes that are not of the motorprotein class and whose phenotype is cell cycle defects, how many do not have a physical type of interaction?\n",
    "/* example previous clarification questions and user replies: */\n",
    "No previous clarification questions.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that ‚Äòphenotype‚Äô is referring to the `Phenotype` column, ‚Äòmotorprotein class‚Äô is referring to the `class` column, ‚Äònonessential genes‚Äô is referring to the `essential` column, and `physical type` is referring to the `type` column. However, it is unclear what fields should be contained in the output table, and hence ‚ÄòAmbOutput‚Äô remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = ‚ÄúWhat fields should be included in the output table? a) One column for the number of genes b) Two columns for GeneID and physical type c) Other (please specify).‚Äù\n",
    "\\n\\n\n",
    "'''\n",
    "\n",
    "feedback_v2 = \"\"\"/* Given the following Natural Language Question: */\n",
    "{nlq}\n",
    "/* And the following Gold Query: */\n",
    "{query}\n",
    "/* Answer the following multiple choice clarification question truthfully based on the Gold Query: */\n",
    "{question}\n",
    "\n",
    "/* Follow these steps:\n",
    "1. Identify which portion of the Gold Query answers the clarification question.\n",
    "2. Evaluate the correctness of each multiple choice answer based only on the Gold Query.\n",
    "3. If none of the choices are correct or you select \"other (please specify)\", provide a short answer for the clarification question.\n",
    "4. Output the final answer in the format: answer_to_cq = \"\".\n",
    "\n",
    "Let's proceed step by step. */\n",
    "/* Only use information from the Gold Query; do not guess.\n",
    "   Prefer answers that maintain consistent granularity and valid join semantics. */\n",
    "\"\"\"\n",
    "\n",
    "feedback_prefix_v1='''/* some examples are provided */\n",
    "/* example question: */\n",
    "How many acres burned in fires in California each year between 2000 and 2005?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  SUM(FIRE_SIZE),\\n  FIRE_YEAR\\nFROM Fires\\nWHERE\\n  State = \"CA\" AND FIRE_YEAR BETWEEN 2000 AND 2005\\nGROUP BY\\n  FIRE_YEAR\n",
    "/* example clarification question*/\n",
    "What information should the output table contain? a) two columns: the total acres burned and the year, b) one column: the total acres burned for each year, c) one column: the total acres burned across all target years, d) other (please specify).\n",
    "/* example reasoning */\n",
    "Output table is determined by the SELECT clause in the gold sql query. The gold query uses ‚ÄòSELECT  SUM(FIRE_SIZE), FIRE_YEAR‚Äô. As a result, the output table has two columns, the total acres burned and the year. Hence, choice a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"a) two columns: the total acres burned and the year\"\n",
    "\n",
    "/* example question: */\n",
    "What was the most common cause of fire between 2000 and 2005?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  STAT_CAUSE_DESCR\\nFROM Fires\\nWHERE\\n  FIRE_YEAR BETWEEN 2000 AND 2005\\nGROUP BY\\n  STAT_CAUSE_DESCR\\nORDER BY\\n  COUNT(*) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Which information should be used to represent the 'cause of fire'? a) the code that represents the cause, b) the description of the cause, c) both the code and the description of the cause, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The clarification question is asking for which column should be used to represent the cause of fire. The gold query uses the STAT_CAUSE_DESCR to represent the cause. As a result, choice b is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) the description of the cause\"\n",
    "\n",
    "/* example question: */\n",
    "Whose CDs sells best?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  artist\\nFROM torrents\\nGROUP BY\\n  artist\\nORDER BY\\n  SUM(totalSnatched) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Which column should be used to identify music related to 'CD'? a) groupName, b) tag, c) releaseType, d) other (please specify)\n",
    "/* example reasoning */\n",
    "The gold query does not use a WHERE clause to filter the CDs. Hence, the CD information is not contained in the tag column or the release type column. As a result, choice a, b, and c are all wrong.\n",
    "/* example  answer*/\n",
    "answer_to_cq = ‚Äúd) Consider all music; No filter on ‚ÄòCD‚Äô ‚Äù\n",
    "\n",
    "/* example question: */\n",
    "How many people wrote comments for the question \"Any additional notes or comments.\"? */\n",
    "/* example gold sql query*/\n",
    "SELECT COUNT(T1.UserID) FROM Answer AS T1 INNER JOIN Question AS T2 ON T1.QuestionID = T2.questionid WHERE T2.questiontext LIKE 'Any additional notes or comments' AND T1.AnswerText IS NOT NULL\n",
    "/* example clarification question*/\n",
    "How to determine if a user has provided comments? a) no check needed, b) see if `AnswerText` column has empty string, c) other (please specify).\n",
    "/* example reasoning */\n",
    "In the gold SQL query, it checks ‚ÄúT1.AnswerText IS NOT NULL‚Äù. Hence, choice a and b are both wrong.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"c) ‚Äòwrote comments‚Äô imply `AnswerText` is not a NULL value\".\n",
    "\n",
    "/* example question: */\n",
    "Calculate the difference between the number of customers and the number of subscribers who did the trip in June 2013. \n",
    "/* example gold sql query*/\n",
    "SELECT SUM(IIF(subscription_type = 'Subscriber', 1, 0)) - SUM(IIF(subscription_type = 'Customer', 1, 0)) FROM trip WHERE start_date LIKE '6/%/2013%'\n",
    "/* example clarification question*/\n",
    "What predicate value should be used to determine a trip in June 2013? a) start_data > 06/2013, b) start_data = ‚ÄòJune 2013‚Äô, c) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold sql query uses start_date LIKE '6/%/2013%' to find trips in June 2013.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"c) start_date LIKE '6/%/2013%'\"\n",
    "\n",
    "\n",
    "/* example question: */\n",
    "Identify the players who weigh 120 kg.\n",
    "/* example gold sql query*/\n",
    "SELECT T2.PlayerName FROM weight_info AS T1 INNER JOIN PlayerInfo AS T2 ON T1.weight_id = T2.weight WHERE T1.weight_in_kg = 120\n",
    "/* example clarification question*/\n",
    "What fields should be contained in the output? a) one column of player name, b) one column of player id, c) two columns of player name and player ids, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold query selects ‚ÄòSELECT T2.PlayerName‚Äô. Hence, a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"a) one column of player name\"\n",
    "\n",
    "/* example question: */\n",
    "How many reviews are created for the podcast \"Scaling Global\" under?\n",
    "/* example gold sql query*/\n",
    "SELECT COUNT(T2.content) FROM podcasts AS T1 INNER JOIN reviews AS T2 ON T2.podcast_id = T1.podcast_id WHERE T1.title = 'Scaling Global'\n",
    "/* example clarification question*/\n",
    "Which column represents the reviews? a) `podcast` column, b) `content` column, c) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold query uses ‚ÄúCOUNT(T2.content)‚Äù to determine the number of reviews. Hence, b is correct in which the `content` column represents the reviews.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) `content` column\"\n",
    "\\n\\n\n",
    "'''\n",
    "\n",
    "selfdebug_examples_prefix = '''/* Given the following incorrect sql asnwers: */\n",
    "SELECT creation, COUNT(*) FROM department GROUP BY creation ORDER BY\n",
    "COUNT(*) DESC LIMIT 1\n",
    "/* Answer the following with no explanation: In which year were most departments established? */\n",
    "SELECT creation FROM department GROUP BY creation ORDER BY COUNT(*) DESC LIMIT 1\n",
    "-------\n",
    "/* Given the following incorrect sql asnwers: */\n",
    "SELECT customers.customer_name FROM customers JOIN orders ON customers.customer_id = orders.customer_id WHERE orders.order_status = \"On Road\" AND orders.order_status = \"Shipped\"\n",
    "/* Answer the following with no explanation: Which customers have both \"On Road\" and \"Shipped\" as order status? List the customer names. */\n",
    "SELECT customers.customer_name FROM customers JOIN orders ON customers.customer_id = orders.customer_id WHERE orders.order_status = \"On Road\" INTERSECT SELECT customers.customer_name FROM customers JOIN orders ON customers.customer_id = orders.customer_id WHERE orders.order_status = \"Shipped\"\n",
    "-------\n",
    "/* Given the following incorrect sql asnwers: */\n",
    "SELECT COUNT(status) FROM city\n",
    "/* How many different statuses do cities have? */\n",
    "SELECT COUNT(DISTINCT status) FROM city\n",
    "-------'''\n",
    "selfdebug_examples = selfdebug_examples_prefix.split('-------')\n",
    "\n",
    "# Generate few-shot strings for self-debug (Method 1 & Refinement)\n",
    "selfdebug_few_shot = []\n",
    "for i in range(1, 4): # We only need up to 3 for this reproduction\n",
    "    prefix = []\n",
    "    for j in range(i):\n",
    "        prefix.append(selfdebug_examples[j])\n",
    "    selfdebug_few_shot.append('\\n'.join(prefix))\n",
    "\n",
    "fewshot_prefix = \"/* some examples are provided */\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07f318a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55257df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 272 records.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/Users/liuzichun/Desktop/Classes/SDSC_5003/Final_Project/kaggle_dataset.csv'\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(\"kaggle_dataset.csv not found.\")\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"Loaded {len(df)} records.\")\n",
    "\n",
    "# Prepare Schema Cache\n",
    "db_schema_cache = {}\n",
    "def get_schema(db_name):\n",
    "    if db_name not in db_schema_cache:\n",
    "        db_path = f'./databases/{db_name}/{db_name}.sqlite'\n",
    "        if not os.path.exists(db_path):\n",
    "             db_path = f'./databases/{db_name}.sqlite'\n",
    "        \n",
    "        if os.path.exists(db_path):\n",
    "             db_schema_cache[db_name] = generate_db_schema(db_path)\n",
    "        else:\n",
    "             db_schema_cache[db_name] = \"\"\n",
    "    return db_schema_cache[db_name]\n",
    "\n",
    "def get_db_path(db_name):\n",
    "    paths = [\n",
    "        f'./databases/{db_name}/{db_name}.sqlite',\n",
    "        f'./databases/{db_name}.sqlite'\n",
    "    ]\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def get_few_shot_examples(target_nlq, n_shots=3):\n",
    "    if not vectorstore or n_shots <= 0:\n",
    "        return \"\"\n",
    "    try:\n",
    "        docs = vectorstore.similarity_search(target_nlq, k=n_shots)\n",
    "        examples = []\n",
    "        for doc in docs:\n",
    "            nl = doc.metadata.get('nl', '')\n",
    "            gold = doc.metadata.get('gold', '')\n",
    "            if nl and gold:\n",
    "                examples.append(f\"/* Example */\\n/* Question: {nl} */\\n/* SQL: */\\n{gold}\")\n",
    "        \n",
    "        if examples:\n",
    "            return \"/* some examples are provided */\\n\" + \"\\n\\n\".join(examples) + \"\\n\\n\"\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving examples: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_feedback_few_shot_examples(target_nlq, n_shots=3):\n",
    "    if not vectorstore or n_shots <= 0:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Just use NLQ to find similar examples, as per notebook logic which uses vectorstore_feedback\n",
    "        # Note: In notebook, vectorstore_feedback is created from examples that have feedback.\n",
    "        # Here we assume 'vectorstore' has feedback metadata if available.\n",
    "        docs = vectorstore.similarity_search(target_nlq, k=n_shots)\n",
    "        examples = []\n",
    "        for doc in docs:\n",
    "            nl = doc.metadata.get('nl', '')\n",
    "            gold = doc.metadata.get('gold', '')\n",
    "            fb = doc.metadata.get('feedback', '')\n",
    "            \n",
    "            if nl and gold and fb:\n",
    "                # Format matches notebook: \\nExample Question: {nl}\\nExample Feedback:{feedback}\\nExample Answer: {gold}\n",
    "                examples.append(f\"\\nExample Question: {nl}\\nExample Feedback:{fb}\\nExample Answer: {gold}\")\n",
    "        \n",
    "        if examples:\n",
    "            return \"/* some examples are provided */\\n\" + \"\".join(examples) + \"\\n\\n\"\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving feedback examples: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646bab9",
   "metadata": {},
   "source": [
    "## Method 1: Baseline with Self-Correction (Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e72313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Baseline with Self-Correction (Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b3ddf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_m1_sample(args):\n",
    "    idx, row, model, max_rounds, n_shots = args\n",
    "    nlq = row['nl']\n",
    "    gold_sql = row['sql']\n",
    "    db_name = row['target_db'] if 'target_db' in row else row['db_id']\n",
    "    schema = get_schema(db_name)\n",
    "    db_path = get_db_path(db_name)\n",
    "    \n",
    "    if not db_path: return None\n",
    "    \n",
    "    print(f\"Processing [{idx}]: {nlq}\")\n",
    "    \n",
    "    # Initial Generation\n",
    "    initial_prompt = \"\"\n",
    "    meta = build_metadata_constraints(nlq, schema)\n",
    "    if n_shots > 0:\n",
    "        examples_str = get_few_shot_examples(nlq, n_shots)\n",
    "        initial_prompt = f\"{examples_str}/* Given the following database schema: */\\n{schema}\\n{meta}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "    else:\n",
    "        initial_prompt = f\"/* Given the following database schema: */\\n{schema}\\n{meta}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "        \n",
    "    sql, _ = LLM_generation(initial_prompt, model=model)\n",
    "    sql = clean_query(sql)\n",
    "    \n",
    "    # Initial Execution & Fix\n",
    "    is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "    syntax_fix = False\n",
    "    if not is_correct and errors:\n",
    "            print(f\"  [{idx}] ‚ùå Execution Error (Initial): {errors[0]}\")\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "            sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "            sql = clean_query(sql)\n",
    "            print(f\"  [{idx}] üîß Fixed SQL (Initial): {sql}\")\n",
    "            is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "            if is_correct:\n",
    "                syntax_fix = True\n",
    "    \n",
    "    sqls_history = [sql]\n",
    "    \n",
    "    if is_correct:\n",
    "        print(f\"  [{idx}] ‚úÖ Initial SQL Correct.\")\n",
    "        return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': 0, 'is_correct': True, 'syntax_fix': syntax_fix}\n",
    "    \n",
    "    print(f\"  [{idx}] ‚ùå Initial SQL Incorrect. Starting Self-Correction...\")\n",
    "    success = False\n",
    "    \n",
    "    for round_i in range(max_rounds):\n",
    "        sqls_str = \";\\n\".join(sorted(list(set(sqls_history)), key=lambda x: sqls_history.index(x)))\n",
    "        prompt = sql_generation_selfdebug.format(schema=schema, sqls=sqls_str, question=nlq, metadata=\"\")\n",
    "        \n",
    "        # Few-Shot for Self-Debug\n",
    "        if n_shots > 0 and len(selfdebug_few_shot) >= 1:\n",
    "            idx_shot = min(n_shots, len(selfdebug_few_shot)) - 1\n",
    "            if idx_shot < 0: idx_shot = 0\n",
    "            prompt = fewshot_prefix + selfdebug_few_shot[idx_shot] + prompt\n",
    "        \n",
    "        sql, _ = LLM_generation(prompt, model=model)\n",
    "        sql = clean_query(sql)\n",
    "        sqls_history.append(sql)\n",
    "        \n",
    "        # Verify\n",
    "        is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "        \n",
    "            # If error, fix it immediately\n",
    "        if not is_correct and errors:\n",
    "            print(f\"    [{idx}] ‚ùå Execution Error: {errors[0]}\")\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "            fixed_sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "            fixed_sql = clean_query(fixed_sql)\n",
    "            print(f\"    [{idx}] üîß Fixed SQL: {fixed_sql}\")\n",
    "            \n",
    "            sqls_history.pop() \n",
    "            sqls_history.append(fixed_sql)\n",
    "            sql = fixed_sql\n",
    "            \n",
    "            is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "            if is_correct:\n",
    "                syntax_fix = True\n",
    "\n",
    "        if is_correct:\n",
    "            print(f\"    [{idx}] ‚úÖ Success!\")\n",
    "            success = True\n",
    "            return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': round_i+1, 'is_correct': True, 'syntax_fix': syntax_fix}\n",
    "            \n",
    "    if not success:\n",
    "        print(f\"    [{idx}] ‚ùå Failed after max rounds.\")\n",
    "        return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': max_rounds, 'is_correct': False, 'syntax_fix': False}\n",
    "\n",
    "def run_simple_feedback_experiment(samples, df_full, model='gpt-3.5-turbo', max_rounds=6, n_shots=0):\n",
    "    print(f\"--- Running Method 1: Baseline (Self-Correction) on {len(samples)} samples (n_shots={n_shots}) in Parallel ---\")\n",
    "    results = []\n",
    "    \n",
    "    args_list = []\n",
    "    for idx, row in samples.iterrows():\n",
    "        args_list.append((idx, row, model, max_rounds, n_shots))\n",
    "        \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        futures = {executor.submit(run_m1_sample, args): args[0] for args in args_list}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    results.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in parallel execution: {e}\")\n",
    "            \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fc8fb",
   "metadata": {},
   "source": [
    "## Method 2: Sphinteract (Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "492e3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Sphinteract (Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "646c5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_m2_sample(args):\n",
    "    idx, row, model, max_rounds, n_shots = args\n",
    "    nlq = row['nl']\n",
    "    gold_sql = row['sql']\n",
    "    db_name = row['target_db'] if 'target_db' in row else row['db_id']\n",
    "    schema = get_schema(db_name)\n",
    "    db_path = get_db_path(db_name)\n",
    "    \n",
    "    if not db_path: return None\n",
    "    \n",
    "    print(f\"Processing [{idx}]: {nlq}\")\n",
    "    \n",
    "    # Initial Generation\n",
    "    initial_prompt = \"\"\n",
    "    meta = build_metadata_constraints(nlq, schema)\n",
    "    if n_shots > 0:\n",
    "        examples_str = get_few_shot_examples(nlq, n_shots)\n",
    "        initial_prompt = f\"{examples_str}/* Given the following database schema: */\\n{schema}\\n{meta}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "    else:\n",
    "        initial_prompt = f\"/* Given the following database schema: */\\n{schema}\\n{meta}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "        \n",
    "    sql, _ = LLM_generation(initial_prompt, model=model)\n",
    "    sql = clean_query(sql)\n",
    "    \n",
    "    # Initial Fix Invalid\n",
    "    is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "    syntax_fix = False\n",
    "    if not is_correct and errors:\n",
    "            print(f\"  [{idx}] ‚ùå Execution Error (Initial): {errors[0]}\")\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "            sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "            sql = clean_query(sql)\n",
    "            print(f\"  [{idx}] üîß Fixed SQL (Initial): {sql}\")\n",
    "            is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "            if is_correct:\n",
    "                syntax_fix = True\n",
    "            \n",
    "    if is_correct:\n",
    "        print(f\"  [{idx}] ‚úÖ Initial SQL Correct.\")\n",
    "        return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': 0, 'is_correct': True, 'syntax_fix': syntax_fix}\n",
    "    \n",
    "    print(f\"  [{idx}] ‚ùå Initial SQL Incorrect. Starting Sphinteract...\")\n",
    "    \n",
    "    sqls_history = [sql]\n",
    "    cqas_history = []\n",
    "    \n",
    "    success = False\n",
    "    \n",
    "    for round_i in range(max_rounds):\n",
    "        # 1. Generate Clarification Question (SRA)\n",
    "        cqas_str = \"\"\n",
    "        for i in range(0, len(cqas_history), 2):\n",
    "            if i+1 < len(cqas_history):\n",
    "                cqas_str += f\"multiple choice clarification question: {cqas_history[i]}\\n\"\n",
    "                cqas_str += f\"user: {cqas_history[i+1]}\\n\"\n",
    "        if not cqas_str:\n",
    "            cqas_str = \"no previous clarification question.\\n\"\n",
    "\n",
    "        sqls_unique = \";\\n\".join(sorted(list(set(sqls_history)), key=lambda x: sqls_history.index(x)))\n",
    "        cq_prompt = SRA.format(schema=schema, question=nlq, sqls=sqls_unique, cqs=cqas_str)\n",
    "        cq_prompt = cq_prefix_v1 + cq_prompt\n",
    "        \n",
    "        cq, _ = LLM_generation(cq_prompt, model=model)\n",
    "        if \"mul_choice_cq =\" in cq:\n",
    "            cq = cq.split(\"mul_choice_cq =\")[-1].strip().strip('\"')\n",
    "        elif \"mul_choice_cq=\" in cq:\n",
    "            cq = cq.split(\"mul_choice_cq=\")[-1].strip().strip('\"')\n",
    "        elif len(cq.split('\\n')) < 5: \n",
    "            pass \n",
    "        else:\n",
    "            lines = cq.strip().split('\\n')\n",
    "            cq = lines[-1]\n",
    "\n",
    "        # print(f\"    [{idx}] ‚ùì CQ: {cq}\")\n",
    "        \n",
    "        # 2. Simulate User Feedback\n",
    "        feedback_prompt = feedback_v2.format(nlq=nlq, query=gold_sql, question=cq)\n",
    "        feedback_prompt = feedback_prefix_v1 + feedback_prompt\n",
    "        \n",
    "        feedback, _ = LLM_generation(feedback_prompt, model=\"gpt-4o\")\n",
    "        if \"answer_to_cq =\" in feedback:\n",
    "            feedback = feedback.split(\"answer_to_cq =\")[-1].strip().strip('\"')\n",
    "        elif \"answer_to_cq=\" in feedback:\n",
    "            feedback = feedback.split(\"answer_to_cq=\")[-1].strip().strip('\"')\n",
    "        \n",
    "        # print(f\"    [{idx}] üó£Ô∏è Feedback: {feedback}\")\n",
    "        \n",
    "        cqas_history.append(cq)\n",
    "        cqas_history.append(feedback)\n",
    "        \n",
    "        # 3. Generate New SQL\n",
    "        cqas_block = \"\"\n",
    "        for i in range(0, len(cqas_history), 2):\n",
    "            if i+1 < len(cqas_history):\n",
    "                cqas_block += f\"multiple choice clarification question: {cqas_history[i]}\\n\"\n",
    "                cqas_block += f\"user: {cqas_history[i+1]}\\n\"\n",
    "        if not cqas_block:\n",
    "            cqas_block = \"no previous clarification questions are asked.\\n\"\n",
    "        \n",
    "        sqls_unique = \";\\n\".join(sorted(list(set(sqls_history)), key=lambda x: sqls_history.index(x)))\n",
    "        meta = build_metadata_constraints(nlq, schema)\n",
    "        sql_prompt = sql_generation_v2.format(schema=schema, question=nlq, sqls=sqls_unique, cqas=cqas_block, metadata=meta)\n",
    "        \n",
    "        if n_shots > 0:\n",
    "            fs_ex = get_feedback_few_shot_examples(nlq, n_shots)\n",
    "            sql_prompt = fs_ex + sql_prompt\n",
    "        \n",
    "        sql, _ = LLM_generation(sql_prompt, model=model)\n",
    "        sql = clean_query(sql)\n",
    "        # print(f\"    [{idx}] üìù New SQL: {sql}\")\n",
    "        sqls_history.append(sql)\n",
    "        \n",
    "        # 4. Verify & Fix if needed\n",
    "        is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "        \n",
    "        if not is_correct and errors:\n",
    "                # print(f\"    [{idx}] ‚ùå Execution Error: {errors[0]}\")\n",
    "                invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "                fixed_sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "                fixed_sql = clean_query(fixed_sql)\n",
    "                # print(f\"    [{idx}] üîß Fixed SQL: {fixed_sql}\")\n",
    "                \n",
    "                sqls_history.pop()\n",
    "                sqls_history.append(fixed_sql)\n",
    "                sql = fixed_sql\n",
    "                \n",
    "                is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "                if is_correct:\n",
    "                    syntax_fix = True\n",
    "        \n",
    "        if is_correct:\n",
    "            print(f\"    [{idx}] ‚úÖ Success!\")\n",
    "            success = True\n",
    "            return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': round_i+1, 'is_correct': True, 'syntax_fix': syntax_fix}\n",
    "            \n",
    "    if not success:\n",
    "        print(f\"    [{idx}] ‚ùå Failed after max rounds.\")\n",
    "        return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': max_rounds, 'is_correct': False, 'syntax_fix': False}\n",
    "\n",
    "def run_sphinteract_experiment_seq(samples, df_full, model='gpt-3.5-turbo', max_rounds=6, n_shots=0):\n",
    "    print(f\"--- Running Method 2: Sphinteract on {len(samples)} samples (n_shots={n_shots}) ---\")\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        nlq = row['nl']\n",
    "        gold_sql = row['sql']\n",
    "        db_name = row['target_db'] if 'target_db' in row else row['db_id']\n",
    "        schema = get_schema(db_name)\n",
    "        db_path = get_db_path(db_name)\n",
    "        \n",
    "        if not db_path: continue\n",
    "        \n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"Processing [{idx}]: {nlq}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initial Generation\n",
    "        meta = build_metadata_constraints(nlq, schema)\n",
    "        if n_shots > 0:\n",
    "            examples_str = get_few_shot_examples(nlq, n_shots)\n",
    "            initial_prompt = f\"{examples_str}/* Given the following database schema: */\\n{schema}\\n{meta}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "        else:\n",
    "            initial_prompt = f\"/* Given the following database schema: */\\n{schema}\\n{meta}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "            \n",
    "        sql, _ = LLM_generation(initial_prompt, model=model)\n",
    "        sql = clean_query(sql)\n",
    "        \n",
    "        # Initial Fix Invalid\n",
    "        is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "        syntax_fix = False\n",
    "        if not is_correct and errors:\n",
    "             print(f\"  ‚ùå Execution Error (Initial): {errors[0]}\")\n",
    "             invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "             sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "             sql = clean_query(sql)\n",
    "             print(f\"  üîß Fixed SQL (Initial): {sql}\")\n",
    "             is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "             if is_correct:\n",
    "                 syntax_fix = True\n",
    "        \n",
    "        sqls_history = [sql]\n",
    "        cqas_history = [] # Stores alternating CQ and User Answer\n",
    "        \n",
    "        if is_correct:\n",
    "            print(\"  ‚úÖ Initial SQL Correct.\")\n",
    "            results.append({'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': 0, 'is_correct': True, 'syntax_fix': syntax_fix})\n",
    "            continue\n",
    "        \n",
    "        print(f\"  ‚ùå Initial SQL Incorrect. Starting Interaction...\")\n",
    "        success = False\n",
    "        \n",
    "        for round_i in range(max_rounds):\n",
    "            print(f\"\\\\n  --- Round {round_i+1} ---\")\n",
    "            \n",
    "            # 1. Generate Clarification Question (SRA)\n",
    "            # Construct cqas string\n",
    "            cqas_str = \"\"\n",
    "            for i in range(0, len(cqas_history), 2):\n",
    "                if i+1 < len(cqas_history):\n",
    "                    cqas_str += f\"multiple choice clarification question: {cqas_history[i]}\\n\"\n",
    "                    cqas_str += f\"user: {cqas_history[i+1]}\\n\"\n",
    "            if not cqas_str:\n",
    "                cqas_str = \"no previous clarification question.\\n\"\n",
    "\n",
    "            sqls_unique = \";\\n\".join(sorted(list(set(sqls_history)), key=lambda x: sqls_history.index(x)))\n",
    "            cq_prompt = SRA.format(schema=schema, question=nlq, sqls=sqls_unique, cqs=cqas_str)\n",
    "            cq_prompt = cq_prefix_v1 + cq_prompt\n",
    "            \n",
    "            cq, _ = LLM_generation(cq_prompt, model=model)\n",
    "            if \"mul_choice_cq =\" in cq:\n",
    "                cq = cq.split(\"mul_choice_cq =\")[-1].strip().strip('\"')\n",
    "            elif \"mul_choice_cq=\" in cq:\n",
    "                cq = cq.split(\"mul_choice_cq=\")[-1].strip().strip('\"')\n",
    "            # Fallback: if no specific format found, use the whole response if it's short, or last line\n",
    "            elif len(cq.split('\\n')) < 5: \n",
    "                pass \n",
    "            else:\n",
    "                # Try to extract the last sentence or question\n",
    "                lines = cq.strip().split('\\n')\n",
    "                cq = lines[-1]\n",
    "\n",
    "            print(f\"    ‚ùì CQ: {cq}\")\n",
    "            \n",
    "            # 2. Simulate User Feedback\n",
    "            feedback_prompt = feedback_v2.format(nlq=nlq, query=gold_sql, question=cq)\n",
    "            feedback_prompt = feedback_prefix_v1 + feedback_prompt\n",
    "            \n",
    "            feedback, _ = LLM_generation(feedback_prompt, model=\"gpt-4o\")\n",
    "            if \"answer_to_cq =\" in feedback:\n",
    "                feedback = feedback.split(\"answer_to_cq =\")[-1].strip().strip('\"')\n",
    "            elif \"answer_to_cq=\" in feedback:\n",
    "                feedback = feedback.split(\"answer_to_cq=\")[-1].strip().strip('\"')\n",
    "            \n",
    "            print(f\"    üó£Ô∏è Feedback: {feedback}\")\n",
    "            \n",
    "            cqas_history.append(cq)\n",
    "            cqas_history.append(feedback)\n",
    "            \n",
    "            # 3. Generate New SQL\n",
    "            # Re-construct cqas string for SQL prompt (format slightly different in notebook?)\n",
    "            # Notebook: \"multiple choice clarification question: ...\\nuser: ...\\n\"\n",
    "            # It seems consistent.\n",
    "            cqas_block = \"\"\n",
    "            for i in range(0, len(cqas_history), 2):\n",
    "                if i+1 < len(cqas_history):\n",
    "                    cqas_block += f\"multiple choice clarification question: {cqas_history[i]}\\n\"\n",
    "                    cqas_block += f\"user: {cqas_history[i+1]}\\n\"\n",
    "            if not cqas_block:\n",
    "                cqas_block = \"no previous clarification questions are asked.\\n\"\n",
    "            \n",
    "            sqls_unique = \";\\n\".join(sorted(list(set(sqls_history)), key=lambda x: sqls_history.index(x)))\n",
    "            meta = build_metadata_constraints(nlq, schema)\n",
    "            sql_prompt = sql_generation_v2.format(schema=schema, question=nlq, sqls=sqls_unique, cqas=cqas_block, metadata=meta)\n",
    "            \n",
    "            # Few-Shot for Interaction\n",
    "            if n_shots > 0:\n",
    "                fs_ex = get_feedback_few_shot_examples(nlq, n_shots)\n",
    "                sql_prompt = fs_ex + sql_prompt\n",
    "            \n",
    "            sql, _ = LLM_generation(sql_prompt, model=model)\n",
    "            sql = clean_query(sql)\n",
    "            print(f\"    üìù New SQL: {sql}\")\n",
    "            sqls_history.append(sql)\n",
    "            \n",
    "            # 4. Verify & Fix if needed\n",
    "            is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "            \n",
    "            if not is_correct and errors:\n",
    "                 print(f\"    ‚ùå Execution Error: {errors[0]}\")\n",
    "                 invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "                 fixed_sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "                 fixed_sql = clean_query(fixed_sql)\n",
    "                 print(f\"    üîß Fixed SQL: {fixed_sql}\")\n",
    "                 \n",
    "                 # Replace invalid SQL with fixed SQL in history\n",
    "                 sqls_history.pop()\n",
    "                 sqls_history.append(fixed_sql)\n",
    "                 sql = fixed_sql\n",
    "                 \n",
    "                 is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "                 if is_correct:\n",
    "                     syntax_fix = True\n",
    "            \n",
    "            if is_correct:\n",
    "                print(\"    ‚úÖ Success!\")\n",
    "                success = True\n",
    "                results.append({'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': round_i+1, 'is_correct': True, 'syntax_fix': syntax_fix})\n",
    "                break\n",
    "            else:\n",
    "                print(\"    ‚ùå Still Incorrect.\")\n",
    "                \n",
    "        if not success:\n",
    "            print(\"    ‚ùå Failed after max rounds.\")\n",
    "            results.append({'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': max_rounds, 'is_correct': False, 'syntax_fix': False})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def run_sphinteract_experiment(samples, df_full, model='gpt-3.5-turbo', max_rounds=6, n_shots=0):\n",
    "    print(f\"--- Running Method 2: Sphinteract on {len(samples)} samples (n_shots={n_shots}) in Parallel ---\")\n",
    "    results = []\n",
    "\n",
    "    args_list = []\n",
    "    for idx, row in samples.iterrows():\n",
    "        args_list.append((idx, row, model, max_rounds, n_shots))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        futures = {executor.submit(run_m2_sample, args): args[0] for args in args_list}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    results.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in parallel execution: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fe39f",
   "metadata": {},
   "source": [
    "## Method 3: Break No Ambiguity (Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a09e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Break No Ambiguity (Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ed9a40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_m3_sample(args):\n",
    "    idx, row, model, max_rounds, n_shots = args\n",
    "    nlq = row['nl']\n",
    "    gold_sql = row['sql']\n",
    "    db_name = row['target_db'] if 'target_db' in row else row['db_id']\n",
    "    schema = get_schema(db_name)\n",
    "    db_path = get_db_path(db_name)\n",
    "    \n",
    "    if not db_path: return None\n",
    "    \n",
    "    print(f\"Processing [{idx}]: {nlq}\")\n",
    "    \n",
    "    # Initial Generation\n",
    "    if n_shots > 0:\n",
    "        examples_str = get_few_shot_examples(nlq, n_shots)\n",
    "        initial_prompt = f\"{examples_str}/* Given the following database schema: */\\n{schema}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "    else:\n",
    "        initial_prompt = f\"/* Given the following database schema: */\\n{schema}\\n/* Answer the following with no explanation: {nlq} */\"\n",
    "        \n",
    "    sql, _ = LLM_generation(initial_prompt, model=model)\n",
    "    sql = clean_query(sql)\n",
    "    \n",
    "    # Initial Fix\n",
    "    is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "    syntax_fix = False\n",
    "    if not is_correct and errors:\n",
    "         print(f\"  [{idx}] ‚ùå Execution Error (Initial): {errors[0]}\")\n",
    "         invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "         sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "         sql = clean_query(sql)\n",
    "         print(f\"  [{idx}] üîß Fixed SQL (Initial): {sql}\")\n",
    "         is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "         if is_correct:\n",
    "             syntax_fix = True\n",
    "    \n",
    "    sqls_history = [sql]\n",
    "    cqas_history = []\n",
    "    \n",
    "    if is_correct:\n",
    "        print(f\"  [{idx}] ‚úÖ Initial SQL Correct.\")\n",
    "        return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': 0, 'is_correct': True, 'syntax_fix': syntax_fix}\n",
    "        \n",
    "    print(f\"  [{idx}] ‚ùå Initial SQL Incorrect. Starting Interaction (CQs & ES)...\")\n",
    "    success = False\n",
    "    \n",
    "    for round_i in range(max_rounds):\n",
    "        # 1. Generate CQ with Early Stopping (SRA_ES)\n",
    "        cqas_str = \"\"\n",
    "        for i in range(0, len(cqas_history), 2):\n",
    "            if i+1 < len(cqas_history):\n",
    "                cqas_str += f\"multiple choice clarification question: {cqas_history[i]}\\n\"\n",
    "                cqas_str += f\"user: {cqas_history[i+1]}\\n\"\n",
    "        if not cqas_str:\n",
    "            cqas_str = \"no previous clarification question.\\n\"\n",
    "\n",
    "        sqls_unique = \";\\n\".join(sorted(list(set(sqls_history)), key=lambda x: sqls_history.index(x)))\n",
    "        cq_prompt = SRA_ES.format(schema=schema, question=nlq, sqls=sqls_unique, cqs=cqas_str)\n",
    "        cq_prompt = cq_prefix_v1 + cq_prompt\n",
    "        \n",
    "        cq, _ = LLM_generation(cq_prompt, model=model)\n",
    "        \n",
    "        # Check for Early Stopping\n",
    "        if \"NO AMBIGUITY\" in cq:\n",
    "            print(f\"    [{idx}] üõë Detected NO AMBIGUITY. Stopping early.\")\n",
    "            break\n",
    "            \n",
    "        if \"mul_choice_cq =\" in cq:\n",
    "            cq = cq.split(\"mul_choice_cq =\")[-1].strip().strip('\"')\n",
    "        elif \"mul_choice_cq=\" in cq:\n",
    "            cq = cq.split(\"mul_choice_cq=\")[-1].strip().strip('\"')\n",
    "        elif len(cq.split('\\n')) < 5:\n",
    "            pass\n",
    "        else:\n",
    "            lines = cq.strip().split('\\n')\n",
    "            cq = lines[-1]\n",
    "\n",
    "        # print(f\"    [{idx}] ‚ùì CQ: {cq}\")\n",
    "        \n",
    "        # 2. Feedback\n",
    "        feedback_prompt = feedback_v2.format(nlq=nlq, query=gold_sql, question=cq)\n",
    "        feedback_prompt = feedback_prefix_v1 + feedback_prompt\n",
    "        feedback, _ = LLM_generation(feedback_prompt, model=\"gpt-4o\")\n",
    "        if \"answer_to_cq =\" in feedback:\n",
    "            feedback = feedback.split(\"answer_to_cq =\")[-1].strip().strip('\"')\n",
    "        elif \"answer_to_cq=\" in feedback:\n",
    "            feedback = feedback.split(\"answer_to_cq=\")[-1].strip().strip('\"')\n",
    "        \n",
    "        # print(f\"    [{idx}] üó£Ô∏è Feedback: {feedback}\")\n",
    "        cqas_history.append(cq)\n",
    "        cqas_history.append(feedback)\n",
    "        \n",
    "        # 3. New SQL\n",
    "        cqas_block = \"\"\n",
    "        for i in range(0, len(cqas_history), 2):\n",
    "            if i+1 < len(cqas_history):\n",
    "                cqas_block += f\"multiple choice clarification question: {cqas_history[i]}\\n\"\n",
    "                cqas_block += f\"user: {cqas_history[i+1]}\\n\"\n",
    "        if not cqas_block:\n",
    "            cqas_block = \"no previous clarification questions are asked.\\n\"\n",
    "        \n",
    "        sqls_unique = \";\\n\".join(sorted(list(set(sqls_history)), key=lambda x: sqls_history.index(x)))\n",
    "        meta = build_metadata_constraints(nlq, schema)\n",
    "        sql_prompt = sql_generation_v2.format(schema=schema, question=nlq, sqls=sqls_unique, cqas=cqas_block, metadata=meta)\n",
    "        \n",
    "        if n_shots > 0:\n",
    "            fs_ex = get_feedback_few_shot_examples(nlq, n_shots)\n",
    "            sql_prompt = fs_ex + sql_prompt\n",
    "        \n",
    "        sql, _ = LLM_generation(sql_prompt, model=model)\n",
    "        sql = clean_query(sql)\n",
    "        # print(f\"    [{idx}] üìù New SQL: {sql}\")\n",
    "        sqls_history.append(sql)\n",
    "        \n",
    "        # 4. Verify & Fix\n",
    "        is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "        \n",
    "        if not is_correct and errors:\n",
    "             # print(f\"    [{idx}] ‚ùå Execution Error: {errors[0]}\")\n",
    "             invalid_prompt = fix_invalid_v1.format(schema=schema, question=nlq, invalidSQL=sql, ex=str(errors[0]))\n",
    "             fixed_sql, _ = LLM_generation(invalid_prompt, model=model)\n",
    "             fixed_sql = clean_query(fixed_sql)\n",
    "             # print(f\"    [{idx}] üîß Fixed SQL: {fixed_sql}\")\n",
    "             \n",
    "             sqls_history.pop()\n",
    "             sqls_history.append(fixed_sql)\n",
    "             sql = fixed_sql\n",
    "             \n",
    "             is_correct, errors = evalfunc(sql, gold_sql, db_path)\n",
    "             if is_correct:\n",
    "                 syntax_fix = True\n",
    "\n",
    "        if is_correct:\n",
    "            print(f\"    [{idx}] ‚úÖ Success!\")\n",
    "            success = True\n",
    "            return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': round_i+1, 'is_correct': True, 'syntax_fix': syntax_fix}\n",
    "            \n",
    "    if not success:\n",
    "        print(f\"    [{idx}] ‚ùå Failed after max rounds.\")\n",
    "        return {'id': idx, 'nlq': nlq, 'final_sql': sql, 'rounds': max_rounds, 'is_correct': False, 'syntax_fix': False}\n",
    "\n",
    "def run_break_no_ambiguity_experiment(samples, df_full, model='gpt-3.5-turbo', max_rounds=6, n_shots=0):\n",
    "    print(f\"--- Running Method 3: Break No Ambiguity on {len(samples)} samples (n_shots={n_shots}) in Parallel ---\")\n",
    "    results = []\n",
    "    \n",
    "    args_list = []\n",
    "    for idx, row in samples.iterrows():\n",
    "        args_list.append((idx, row, model, max_rounds, n_shots))\n",
    "        \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        futures = {executor.submit(run_m3_sample, args): args[0] for args in args_list}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    results.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in parallel execution: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819daf0",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "69d9a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization & Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cecaa67",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "abced6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping notebook generation under Jupyter environment.\n",
      "Skipping notebook generation under Jupyter environment.\n",
      "--- Selecting 30 Ambiguous Samples using GPT-3.5-Turbo ---\n",
      "Searching for 30 ambiguous samples from 272 candidates...\n",
      "Checking batch 0-8 (Found: 0/30)...\n",
      "  [Clear] Who is the highest paid player since 2010?\n",
      "  [Ambiguous] Average date of year that fire was discovered from 2000~2004?\n",
      "  [Clear] What was the most common cause of fire between 2000 and 2005?\n",
      "  [Ambiguous] What are the most likely outcome of the police investigation if the crime happen on \"street\"?\n",
      "  [Ambiguous] What is the average Title 1 fund in Virginia?\n",
      "  [Clear] What is the average age of players from USA?\n",
      "  [Clear] Which matches has the highest draw opening so far?\n",
      "  [Clear] How much did the federal government spend in No Child Left Behind funding in 2017?\n",
      "Checking batch 8-16 (Found: 3/30)...\n",
      "  [Clear] What‚Äôs the year that have the largest acres in the fire area?\n",
      "  [Ambiguous] Rank the country of product origins in terms of pesticide residues detection.\n",
      "  [Clear] How many total acres of land in Texas have seen a wildfire in the decade between 2000-2010?\n",
      "  [Clear] What's the unit of measure for sample 3879?\n",
      "  [Clear] Which state has the most wildfires?\n",
      "  [Ambiguous] which entry have been downloaded the least?\n",
      "  [Clear] Where is the first BWR type power plant built and located?\n",
      "  [Clear] which foods are captured in the data set?\n",
      "Checking batch 16-24 (Found: 5/30)...\n",
      "  [Clear] Which country lead the total capacity of the power plants it held?\n",
      "  [Clear] Which school district receive the most of federal revenue through state in Wisconsin?\n",
      "  [Clear] Which are the top 10 commodities that have the highest residue during 2015?\n",
      "  [Ambiguous] When was the last instance of a violent or sexual offense in Manchester?\n",
      "  [Clear] Which year has the most released song?\n",
      "  [Clear] Where is the place with the largest number of sexual offenses crime events?\n",
      "  [Ambiguous] What is the top league that pays the most to their players?\n",
      "  [Clear] State with highest average math score\n",
      "Checking batch 24-32 (Found: 7/30)...\n",
      "  [Ambiguous] How many crimes has been conducted?\n",
      "  [Clear] How many crimes are still \"Under investigation\" to date?\n",
      "  [Clear] On what type of land (public or private) do more wildfires occur?\n",
      "  [Ambiguous] What are the top 100 single musics?\n",
      "  [Ambiguous] What were the years when any special elections happened in hall of fame?\n",
      "  [Clear] Which counties in Washington had fires in 2012?\n",
      "  [Ambiguous] Which city the most players were born?\n",
      "  [Ambiguous] What are the birth places of players won on hall of fame since 1871?\n",
      "Checking batch 32-40 (Found: 12/30)...\n",
      "  [Clear] Which reactor type has the largest average capacity?\n",
      "  [Ambiguous] What's the odds for draw on Bet365 for the game Swindon v.s. Millwall for 2016/2017 season?\n",
      "  [Ambiguous] what year was each specific entry released?\n",
      "  [Clear] What is the average pay for players inducted into the hall of fame?\n",
      "  [Ambiguous] Name the most popular and least popular releases of lasean camry?\n",
      "  [Ambiguous] Do other leagues have referee name records outside of Scotland and England?\n",
      "  [Clear] How many number of units are there in sample 9628?\n",
      "  [Clear] How many league division does football_data database has?\n",
      "Checking batch 40-48 (Found: 16/30)...\n",
      "  [Clear] Which Premier League matches ended in a draw in 2016?\n",
      "  [Clear] Which neighborhood/area has the highest burglary rate?\n",
      "  [Clear] Which birth place has the most player awards?\n",
      "  [Clear] Which states had the largest number of fires in 2001?\n",
      "  [Clear] How many Wisconsin school districts receive federal funding?\n",
      "  [Clear] How many matches did Pinnacle have betting records?\n",
      "  [Ambiguous] For every award, who is the oldest winner?\n",
      "  [Clear] What's the most common extraction method?\n",
      "Checking batch 48-56 (Found: 17/30)...\n",
      "  [Clear] Which type of crime happen the most in Salford?\n",
      "  [Clear] Which is the most popular voting method for Hall of Fame in 2000?\n",
      "  [Clear] What is the total area that has been burned until now?\n",
      "  [Clear] Which country has only one nuclear power plants?\n",
      "  [Ambiguous] how was a specific sample tested?\n",
      "  [Clear] How many nuclear power plants are in preparation to be used in Japan?\n",
      "  [Ambiguous] For award winners, which position that has the most hall of fame players?\n",
      "  [Ambiguous] What's the code for confirmation for the latest sample?\n",
      "Checking batch 56-64 (Found: 20/30)...\n",
      "  [Ambiguous] In 2014, how many wildfires were the result of mismanaged campfires?\n",
      "  [Clear] How many years of data are recorded in this database?\n",
      "  [Ambiguous] Which leage has higher average salaries for player?\n",
      "  [Clear] How many players were awarded more than ten times?\n",
      "  [Clear] Which state has the highest average score in math exam?\n",
      "  [Clear] Which year had the largest number of fires?\n",
      "  [Clear] show all imported samples?\n",
      "  [Ambiguous] For award winners, what's average weight for each position\n",
      "Checking batch 64-72 (Found: 23/30)...\n",
      "  [Clear] Top 10 teams with the most hall of fame players\n",
      "  [Clear] If sample 6480 is imported, which country is it originally from?\n",
      "  [Ambiguous] What is the main source of the information for this table?\n",
      "  [Clear] What are name of top 10 artists or groups?\n",
      "  [Clear] Which state has the most number of fires being recorded?\n",
      "  [Clear] Which country first started using nuclear power plant(s)?\n",
      "  [Clear] What are the planed nuclear power plants and their located countries?\n",
      "  [Clear] Find me top 10 albums ranked by their popularity.\n",
      "Checking batch 72-80 (Found: 24/30)...\n",
      "  [Clear] How many operating nuclear station in France?\n",
      "  [Clear] Which year has the most wildfires?\n",
      "  [Clear] How many kinds of nuclear reactor model in the world?\n",
      "  [Ambiguous] Who was responsible for the land of the biggest fire in Oregon in 2015?\n",
      "  [Clear] Which kind of release type is the most popular?\n",
      "  [Ambiguous] What is the top 3 area of crime conducted?\n",
      "  [Ambiguous] What‚Äôs the most common cause of the fire (code) in the database?\n",
      "  [Ambiguous] Which state spent the least revenue towards schools and whats the state average score\n",
      "Checking batch 80-88 (Found: 28/30)...\n",
      "  [Ambiguous] What's the code for test for sample 7498?\n",
      "  [Clear] Which year has most matches?\n",
      "  [Clear] which pesticides are most used?\n",
      "  [Ambiguous] How many matches did Bet365 gives higher home win odds than Pinnacle?\n",
      "Reached target 30; early stop.\n",
      "Selected 30 samples.\n",
      "difficulty\n",
      "Hard      14\n",
      "Medium    10\n",
      "Simple     6\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== Method 1: Baseline (Zero-Shot) ===\n",
      "--- Running Method 1: Baseline (Self-Correction) on 30 samples (n_shots=0) in Parallel ---\n",
      "\\n=== Method 1: Baseline (Few-Shot) ===\n",
      "--- Running Method 1: Baseline (Self-Correction) on 30 samples (n_shots=3) in Parallel ---\n",
      "\\n=== Method 2: Sphinteract (Zero-Shot) ===\n",
      "--- Running Method 2: Sphinteract on 30 samples (n_shots=0) in Parallel ---\n",
      "\\n=== Method 2: Sphinteract (Few-Shot) ===\n",
      "--- Running Method 2: Sphinteract on 30 samples (n_shots=3) in Parallel ---\n",
      "\\n=== Method 3: Break No Ambiguity (Zero-Shot) ===\n",
      "--- Running Method 3: Break No Ambiguity on 30 samples (n_shots=0) in Parallel ---\n",
      "\\n=== Method 3: Break No Ambiguity (Few-Shot) ===\n",
      "--- Running Method 3: Break No Ambiguity on 30 samples (n_shots=3) in Parallel ---\n",
      "\n",
      "--- Visualization of Results ---\n",
      "Results saved to experiment_results.json\n",
      "Reading results from experiment_results.json for plotting...\n",
      "No data to visualize.\n"
     ]
    }
   ],
   "source": [
    "def visualize_results(res_m1_zero, res_m1_few, res_m2_zero, res_m2_few, res_m3_zero, res_m3_few, test_subset=None):\n",
    "    print(\"\\n--- Visualization of Results ---\")\n",
    "    \n",
    "    # 1. Construct unified data structure & Save to JSON\n",
    "    data_map = {\n",
    "        'M1_Zero': res_m1_zero, 'M1_Few': res_m1_few,\n",
    "        'M2_Zero': res_m2_zero, 'M2_Few': res_m2_few,\n",
    "        'M3_Zero': res_m3_zero, 'M3_Few': res_m3_few\n",
    "    }\n",
    "    \n",
    "    json_data = []\n",
    "    for key, df in data_map.items():\n",
    "        if df.empty: continue\n",
    "        method, mode = key.split('_')\n",
    "        records = df.to_dict(orient='records')\n",
    "        for r in records:\n",
    "            r['Method'] = method\n",
    "            r['Mode'] = mode\n",
    "            # Calculate status for stacked bar\n",
    "            if r['is_correct']:\n",
    "                if r.get('syntax_fix', False):\n",
    "                    r['Status'] = 'Syntax Fix Correct'\n",
    "                elif r.get('rounds', 0) == 0:\n",
    "                    r['Status'] = 'Initial Correct'\n",
    "                else:\n",
    "                    r['Status'] = 'Interactive Correct'\n",
    "            else:\n",
    "                r['Status'] = 'Incorrect'\n",
    "        json_data.extend(records)\n",
    "        \n",
    "    json_path = 'experiment_results.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2, default=str)\n",
    "    print(f\"Results saved to {json_path}\")\n",
    "    \n",
    "    # 2. Read from JSON for plotting (as requested)\n",
    "    print(f\"Reading results from {json_path} for plotting...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        loaded_data = json.load(f)\n",
    "    \n",
    "    full_df = pd.DataFrame(loaded_data)\n",
    "    \n",
    "    if full_df.empty:\n",
    "        print(\"No data to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Set Academic Style\n",
    "    sns.set_theme(style=\"white\")\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "    \n",
    "    # Define Palettes: M1=Blue, M2=Orange, M3=Green\n",
    "    palette_dict = {'M1': '#4E79A7', 'M2': '#F28E2B', 'M3': '#59A14F'}\n",
    "    \n",
    "    # --- Figure 1: 2x2 Grid (Accuracy & Rounds) ---\n",
    "    fig1, axes1 = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig1.suptitle('Performance Overview', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    modes = ['Zero', 'Few']\n",
    "    agg_df = full_df.groupby(['Method', 'Mode']).agg(\n",
    "        Accuracy=('is_correct', 'mean'),\n",
    "        Avg_Rounds=('rounds', 'mean'),\n",
    "        SyntaxFixCount=('syntax_fix', 'sum')\n",
    "    ).reset_index()\n",
    "    print(\"\\nAggregated Performance Table (includes SyntaxFixCount):\")\n",
    "    print(agg_df.to_string(index=False))\n",
    "    \n",
    "    for i, mode in enumerate(modes):\n",
    "        # Top Row: Accuracy\n",
    "        ax_acc = axes1[0, i]\n",
    "        subset = agg_df[agg_df['Mode'] == mode]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Accuracy', palette=palette_dict, ax=ax_acc)\n",
    "            ax_acc.set_title(f'Accuracy ({mode}-shot)')\n",
    "            ax_acc.set_ylim(0, 1.1)\n",
    "            ax_acc.set_ylabel('Success Rate' if i==0 else '')\n",
    "            for p in ax_acc.patches:\n",
    "                ax_acc.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontweight='bold')\n",
    "\n",
    "        # Bottom Row: Rounds\n",
    "        ax_rds = axes1[1, i]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Avg_Rounds', palette=palette_dict, ax=ax_rds)\n",
    "            ax_rds.set_title(f'Avg Interaction Rounds ({mode}-shot)')\n",
    "            ax_rds.set_ylabel('Rounds' if i==0 else '')\n",
    "            # Add values on top\n",
    "            max_h = 0\n",
    "            for p in ax_rds.patches:\n",
    "                h = p.get_height()\n",
    "                if h > max_h: max_h = h\n",
    "                ax_rds.annotate(f'{h:.2f}', (p.get_x() + p.get_width() / 2., h),\n",
    "                                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "            ax_rds.set_ylim(0, max_h * 1.2 if max_h > 0 else 1) \n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Figure 2: Stacked Bar Chart (Correctness Breakdown) ---\n",
    "    breakdown_list = []\n",
    "    for (method, mode), group in full_df.groupby(['Method', 'Mode']):\n",
    "        total = len(group)\n",
    "        initial = len(group[group['Status'] == 'Initial Correct'])\n",
    "        interactive = len(group[group['Status'] == 'Interactive Correct'])\n",
    "        syntaxfix = len(group[group['Status'] == 'Syntax Fix Correct'])\n",
    "        incorrect = len(group[group['Status'] == 'Incorrect'])\n",
    "        \n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Initial Correct', 'Prop': initial/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Interactive Correct', 'Prop': interactive/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Syntax Fix Correct', 'Prop': syntaxfix/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Incorrect', 'Prop': incorrect/total})\n",
    "        \n",
    "    breakdown_df = pd.DataFrame(breakdown_list)\n",
    "    status_palette = {'Initial Correct': '#1f77b4', 'Interactive Correct': '#2ca02c', 'Syntax Fix Correct': '#17becf', 'Incorrect': '#d62728'}\n",
    "    \n",
    "    if not breakdown_df.empty:\n",
    "        fig2, axes2 = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig2.suptitle('Correctness Breakdown', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, mode in enumerate(modes):\n",
    "            ax = axes2[i]\n",
    "            subset = breakdown_df[breakdown_df['Mode'] == mode]\n",
    "            if not subset.empty:\n",
    "                pivot_df = subset.pivot(index='Method', columns='Type', values='Prop').fillna(0)\n",
    "                # Ensure all columns exist\n",
    "                for col in ['Initial Correct', 'Interactive Correct', 'Syntax Fix Correct', 'Incorrect']:\n",
    "                    if col not in pivot_df.columns: pivot_df[col] = 0\n",
    "                pivot_df = pivot_df[['Initial Correct', 'Interactive Correct', 'Syntax Fix Correct', 'Incorrect']]\n",
    "                \n",
    "                pivot_df.plot(kind='bar', stacked=True, color=[status_palette[c] for c in pivot_df.columns], ax=ax)\n",
    "                ax.set_title(f'Breakdown ({mode}-shot)')\n",
    "                ax.set_ylabel('Proportion' if i==0 else '')\n",
    "                ax.set_ylim(0, 1.0)\n",
    "                if i == 1:\n",
    "                    ax.legend(title='Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                else:\n",
    "                    ax.get_legend().remove()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    # --- Figure 3: Difficulty Distribution ---\n",
    "    if test_subset is not None and 'difficulty' in test_subset.columns:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        diff_counts = test_subset['difficulty'].value_counts()\n",
    "        colors_map = {'Hard': '#ff9999', 'Medium': '#ffff99', 'Simple': '#66b3ff'}\n",
    "        pie_colors = [colors_map.get(l, '#cccccc') for l in diff_counts.index]\n",
    "        \n",
    "        plt.pie(diff_counts, labels=diff_counts.index, autopct='%1.1f%%', startangle=140, colors=pie_colors)\n",
    "        plt.title('Test Sample Difficulty Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def select_test_samples(df, simple_n=5, medium_n=5, hard_n=30):\n",
    "    # This function is replaced by ambiguity filtering\n",
    "    return select_ambiguous_samples(df, target_n=30)\n",
    "\n",
    "def select_ambiguous_samples(df, target_n=30):\n",
    "    print(f\"--- Selecting {target_n} Ambiguous Samples using GPT-3.5-Turbo ---\")\n",
    "    \n",
    "    if 'sql_tok_len' not in df.columns:\n",
    "        print(\"Calculating SQL token lengths...\")\n",
    "        df['sql_tok_len'] = df['sql'].apply(lambda x: len(x.split()))\n",
    "        \n",
    "    df['len'] = df['sql_tok_len']\n",
    "    df_sorted = df.sort_values('len')\n",
    "    \n",
    "    # Define thresholds for difficulty (based on full dataset)\n",
    "    n = len(df)\n",
    "    t1 = n // 3\n",
    "    t2 = 2 * n // 3\n",
    "    \n",
    "    simple_pool = df_sorted.iloc[:t1]\n",
    "    medium_pool = df_sorted.iloc[t1:t2]\n",
    "    hard_pool = df_sorted.iloc[t2:]\n",
    "    \n",
    "    def get_difficulty(length):\n",
    "        if length >= hard_pool['len'].min(): return 'Hard'\n",
    "        elif length >= medium_pool['len'].min(): return 'Medium'\n",
    "        else: return 'Simple'\n",
    "    \n",
    "    selected_indices = []\n",
    "    \n",
    "    # Shuffle df to get random candidates\n",
    "    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    def check_ambiguity_row(row_tuple):\n",
    "        idx, row = row_tuple\n",
    "        nlq = row['nl']\n",
    "        db_name = row['target_db'] if 'target_db' in row else row['db_id']\n",
    "        schema = get_schema(db_name)\n",
    "        \n",
    "        prompt = f\"\"\"/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following Natural Language Question: */\n",
    "{nlq}\n",
    "\n",
    "/* Task: Determine if the question is ambiguous given the schema.\n",
    "   Ambiguity can arise from:\n",
    "   - AmbQuestion: The question phrasing is unclear.\n",
    "   - AmbTableColumn: Unclear mapping to tables/columns.\n",
    "   - AmbOutput: Unclear what columns to output.\n",
    "   - AmbValue: Unclear predicate values.\n",
    "\n",
    "   Answer \"Yes\" if the question is ambiguous, or \"No\" if it is clear.\n",
    "   Provide a brief reason.\n",
    "*/\n",
    "Is the question ambiguous? Answer: \"\"\"\n",
    "        # print(f\"Checking sample {idx}...\")\n",
    "        amb_model = os.getenv(\"AMBIGUITY_MODEL\", \"gpt-4o-mini\")\n",
    "        response, _ = LLM_generation(prompt, model=amb_model, retries=5, retry_delay=2.0, log_each_retry=False)\n",
    "        \n",
    "        resp = response.strip()\n",
    "        if resp.startswith(\"SELECT * FROM error\") or resp.lower().startswith(\"error\") or \"bad gateway\" in resp.lower():\n",
    "            return idx, None, nlq\n",
    "        is_ambiguous = False\n",
    "        u = resp.upper()\n",
    "        if u.startswith(\"YES\") or \"YES\" in u[:5]:\n",
    "            is_ambiguous = True\n",
    "        return idx, is_ambiguous, nlq\n",
    "\n",
    "    # Process in batches until target_n is reached\n",
    "    candidates = list(df_shuffled.iterrows())\n",
    "    total_candidates = len(candidates)\n",
    "    current_idx = 0\n",
    "    workers_env = os.getenv(\"AMBIGUITY_WORKERS\", \"8\")\n",
    "    try:\n",
    "        max_workers = max(1, int(workers_env))\n",
    "    except Exception:\n",
    "        max_workers = 8\n",
    "    batch_size = max_workers\n",
    "\n",
    "    print(f\"Searching for {target_n} ambiguous samples from {total_candidates} candidates...\")\n",
    "\n",
    "    while len(selected_indices) < target_n and current_idx < total_candidates:\n",
    "        batch_end = min(current_idx + batch_size, total_candidates)\n",
    "        batch_candidates = candidates[current_idx:batch_end]\n",
    "        \n",
    "        print(f\"Checking batch {current_idx}-{batch_end} (Found: {len(selected_indices)}/{target_n})...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(check_ambiguity_row, c): c[0] for c in batch_candidates}\n",
    "            \n",
    "            error_count = 0\n",
    "            reached = False\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    idx, is_ambiguous, nlq = future.result()\n",
    "                    if is_ambiguous is True:\n",
    "                        print(f\"  [Ambiguous] {nlq}\")\n",
    "                        selected_indices.append(idx)\n",
    "                        if len(selected_indices) >= target_n:\n",
    "                            reached = True\n",
    "                            print(f\"Reached target {target_n}; early stop.\")\n",
    "                            break\n",
    "                    elif is_ambiguous is False:\n",
    "                        print(f\"  [Clear] {nlq}\")\n",
    "                    else:\n",
    "                        print(f\"  [Skipped: LLM error] {nlq}\")\n",
    "                        error_count += 1\n",
    "                except Exception as e:\n",
    "                    print(\"Error processing sample: LLM failure; skipped\")\n",
    "                    error_count += 1\n",
    "            if error_count > (batch_size * 0.3):\n",
    "                print(\"High LLM error rate in batch; pausing 10s before next batch...\")\n",
    "                time.sleep(10)\n",
    "            if reached:\n",
    "                break\n",
    "        \n",
    "        current_idx += batch_size\n",
    "\n",
    "    selected_indices = selected_indices[:target_n]\n",
    "    if len(selected_indices) < target_n:\n",
    "        need = target_n - len(selected_indices)\n",
    "        selected_set = set(selected_indices)\n",
    "        hard_fill = [i for i in hard_pool.index.tolist() if i not in selected_set]\n",
    "        medium_fill = [i for i in medium_pool.index.tolist() if i not in selected_set]\n",
    "        simple_fill = [i for i in simple_pool.index.tolist() if i not in selected_set]\n",
    "        filler = []\n",
    "        for i in hard_fill:\n",
    "            if len(filler) >= need:\n",
    "                break\n",
    "            filler.append(i)\n",
    "        if len(filler) < need:\n",
    "            for i in medium_fill:\n",
    "                if len(filler) >= need:\n",
    "                    break\n",
    "                filler.append(i)\n",
    "        if len(filler) < need:\n",
    "            for i in simple_fill:\n",
    "                if len(filler) >= need:\n",
    "                    break\n",
    "                filler.append(i)\n",
    "        selected_indices = selected_indices + filler[:need]\n",
    "            \n",
    "    if not selected_indices:\n",
    "        print(\"Warning: No ambiguous samples found. Returning random samples.\")\n",
    "        result = df.sample(target_n)\n",
    "    else:\n",
    "        result = df_shuffled.loc[selected_indices].copy()\n",
    "        \n",
    "    # Assign difficulty labels\n",
    "    result['difficulty'] = result['len'].apply(get_difficulty)\n",
    "    \n",
    "    print(f\"Selected {len(result)} samples.\")\n",
    "    # Print difficulty distribution\n",
    "    print(result['difficulty'].value_counts())\n",
    "    \n",
    "    # Plot difficulty distribution\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    diff_counts = result['difficulty'].value_counts()\n",
    "    colors_map = {'Hard': '#ff9999', 'Medium': '#ffff99', 'Simple': '#66b3ff'}\n",
    "    pie_colors = [colors_map.get(l, '#cccccc') for l in diff_counts.index]\n",
    "    \n",
    "    plt.pie(diff_counts, labels=diff_counts.index, autopct='%1.1f%%', startangle=140, colors=pie_colors)\n",
    "    plt.title('Ambiguous Sample Difficulty Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if get_ipython() is None:\n",
    "        generate_notebook()\n",
    "    else:\n",
    "        print('Skipping notebook generation under Jupyter environment.')\n",
    "except Exception:\n",
    "    generate_notebook()\n",
    "    print(\"\\n--- Generating Notebook (reproduction_sphinteract_ambiguity_generated.ipynb) ---\")\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    \n",
    "    text_intro = \"\"\"# Reproduction of Sphinteract on KaggleDBQA (Ambiguity Filtered)\n",
    "\n",
    "This notebook reproduces the three methods described in the paper \"Sphinteract: ...\":\n",
    "1.  **Baseline (Method 1)**: Zero-shot generation with Self-Correction (Fix Invalid).\n",
    "2.  **Sphinteract (Method 2)**: Interactive framework with Clarification Questions (SRA) and Feedback.\n",
    "3.  **Break No Ambiguity (Method 3)**: Similar to Method 2 but with an Early Stopping mechanism.\n",
    "\n",
    "We use the **KaggleDBQA** dataset.\n",
    "\n",
    "**Modification**: Instead of random sampling, we filter for 20 \"Ambiguous\" samples using GPT-3.5-Turbo.\n",
    "\n",
    "## Requirements\n",
    "- `openai`\n",
    "- `pandas`\n",
    "- `sqlite3`\n",
    "- `numpy`\n",
    "- `python-dotenv`\n",
    "- `tiktoken`\n",
    "- `xxhash`\n",
    "\n",
    "Ensure you have a `.env` file with `OPENAI_API_KEY` and `OPENAI_BASE_URL` (optional).\n",
    "\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text_intro))\n",
    "    \n",
    "    # Read current file\n",
    "    try:\n",
    "        with open(__file__, 'r') as f:\n",
    "            content = f.read()\n",
    "    except NameError:\n",
    "        # Fallback if __file__ is not defined (e.g. interactive)\n",
    "        content = open('reproduce_sphinteract_ambiguity.py', 'r').read()\n",
    "        \n",
    "    # Split by separators\n",
    "    separator = \"# \" + \"-\" * 78\n",
    "    parts = content.split(separator)\n",
    "    \n",
    "    # Imports\n",
    "    nb.cells.append(nbf.v4.new_code_cell(parts[0].strip()))\n",
    "    \n",
    "    for part in parts[1:]:\n",
    "        if not part.strip(): continue\n",
    "        \n",
    "        lines = part.strip().split('\\n')\n",
    "        \n",
    "        # Heuristic: If it's the main block, maybe mark it?\n",
    "        if \"if __name__ == \\\"__main__\\\":\" in part:\n",
    "             nb.cells.append(nbf.v4.new_markdown_cell(\"## Execution\"))\n",
    "        elif \"Helper Functions\" in lines[0]:\n",
    "             nb.cells.append(nbf.v4.new_markdown_cell(\"## Helper Functions\"))\n",
    "        elif \"Prompts\" in lines[0]:\n",
    "             nb.cells.append(nbf.v4.new_markdown_cell(\"## Prompts\"))\n",
    "        elif \"Method\" in lines[0]:\n",
    "             nb.cells.append(nbf.v4.new_markdown_cell(f\"## {lines[0].replace('# ', '').strip()}\"))\n",
    "        elif \"Visualization\" in lines[0]:\n",
    "             nb.cells.append(nbf.v4.new_markdown_cell(\"## Visualization\"))\n",
    "             \n",
    "        nb.cells.append(nbf.v4.new_code_cell(part.strip()))\n",
    "        \n",
    "    with open('reproduction_sphinteract_ambiguity_generated.ipynb', 'w') as f:\n",
    "        nbf.write(nb, f)\n",
    "    print(\"Notebook reproduction_sphinteract_ambiguity_generated.ipynb generated successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate the notebook representation of this script\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if get_ipython() is None:\n",
    "            generate_notebook()\n",
    "        else:\n",
    "            print('Skipping notebook generation under Jupyter environment.')\n",
    "    except Exception:\n",
    "        generate_notebook()\n",
    "\n",
    "    # 1. Select Shared Test Samples\n",
    "    # Modified sample selection as per user request: Simple=5, Medium=5, Hard=30\n",
    "    test_subset = select_test_samples(df)\n",
    "\n",
    "    if len(test_subset) == 0:\n",
    "        print(\"Warning: No samples found. Using first 30 samples.\")\n",
    "        test_subset = df.head(30)\n",
    "\n",
    "    # 2. Run Experiments\n",
    "    print(\"\\\\n=== Method 1: Baseline (Zero-Shot) ===\")\n",
    "    res_m1_zero = run_simple_feedback_experiment(test_subset, df, n_shots=0)\n",
    "\n",
    "    print(\"\\\\n=== Method 1: Baseline (Few-Shot) ===\")\n",
    "    res_m1_few = run_simple_feedback_experiment(test_subset, df, n_shots=3)\n",
    "\n",
    "    print(\"\\\\n=== Method 2: Sphinteract (Zero-Shot) ===\")\n",
    "    res_m2_zero = run_sphinteract_experiment(test_subset, df, n_shots=0)\n",
    "    print(\"\\\\n=== Method 2: Sphinteract (Few-Shot) ===\")\n",
    "    res_m2_few = run_sphinteract_experiment(test_subset, df, n_shots=3)\n",
    "\n",
    "    print(\"\\\\n=== Method 3: Break No Ambiguity (Zero-Shot) ===\")\n",
    "    res_m3_zero = run_break_no_ambiguity_experiment(test_subset, df, n_shots=0)\n",
    "\n",
    "    print(\"\\\\n=== Method 3: Break No Ambiguity (Few-Shot) ===\")\n",
    "    res_m3_few = run_break_no_ambiguity_experiment(test_subset, df, n_shots=3)\n",
    "\n",
    "    # 3. Visualize\n",
    "    visualize_results(res_m1_zero, res_m1_few, res_m2_zero, res_m2_few, res_m3_zero, res_m3_few, test_subset=test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c24f4b",
   "metadata": {},
   "source": [
    "## Beatify Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9803a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redraw_from_results(json_path='experiment_results.json', save_dir=None):\n",
    "    import os, json, numpy as np, pandas as pd\n",
    "    import matplotlib.pyplot as plt, seaborn as sns\n",
    "    from matplotlib import colors as mcolors\n",
    "\n",
    "    # ‰∫ÆËâ≤ÁîüÊàêÔºöÊèêÈ´ò‰∫ÆÂ∫¶Ôºå‰∏çÊîπÂèòÈÄèÊòéÂ∫¶\n",
    "    def lighten(c, factor=0.40):\n",
    "        r, g, b = mcolors.to_rgb(c)\n",
    "        return (1 - (1 - r) * (1 - factor),\n",
    "                1 - (1 - g) * (1 - factor),\n",
    "                1 - (1 - b) * (1 - factor))\n",
    "\n",
    "    # ËØªÂèñÁªìÊûú\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(json_path)\n",
    "    with open(json_path, 'r') as f:\n",
    "        loaded = json.load(f)\n",
    "    df = pd.DataFrame(loaded)\n",
    "    if df.empty:\n",
    "        raise ValueError('Empty results')\n",
    "\n",
    "    # ÂÖ®Â±ÄÊõ¥Â§ßÂ≠ó‰Ωì‰∏éÁ¨¶Âè∑Â∞∫ÂØ∏\n",
    "    sns.set_theme(style='white')\n",
    "    sns.set_context('talk', font_scale=1.6)  # ÊØî paper 1.2 Â§ßÂæàÂ§ö\n",
    "    plt.rcParams.update({\n",
    "        'figure.dpi': 160,\n",
    "        'axes.titlesize': 18,\n",
    "        'axes.titleweight': 'bold',\n",
    "        'axes.labelsize': 14,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.title_fontsize': 12,\n",
    "        'legend.fontsize': 12,\n",
    "        'axes.linewidth': 1.2,\n",
    "        'lines.linewidth': 2.0,\n",
    "        'patch.linewidth': 0.8,\n",
    "        'grid.linewidth': 0.8,\n",
    "    })\n",
    "\n",
    "    # È¢úËâ≤ÔºöZero-shot Âü∫Á°ÄËâ≤ÔºõFew-shot ‰∫ÆËâ≤ÔºàÈÄèÊòéÂ∫¶‰∏çÂèòÔºâ\n",
    "    base_palette = {'M1': '#4E79A7', 'M2': '#F28E2B', 'M3': '#59A14F'}\n",
    "    few_palette = {k: lighten(v, 0.40) for k, v in base_palette.items()}\n",
    "    modes = ['Zero', 'Few']\n",
    "\n",
    "    # ËÅöÂêàÊåáÊ†á\n",
    "    agg = df.groupby(['Method', 'Mode']).agg(\n",
    "        Accuracy=('is_correct', 'mean'),\n",
    "        Avg_Rounds=('rounds', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Âõæ 1ÔºöÊÄßËÉΩÊÄªËßàÔºàÊôÆÈÄöÊü±Áä∂ÂõæÔºâÔºåFew ‰ΩøÁî®‰∫ÆËâ≤\n",
    "    fig1, axes1 = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig1.suptitle('Performance Overview', fontsize=18, fontweight='bold')\n",
    "\n",
    "    for i, mode in enumerate(modes):\n",
    "        sub = agg[agg['Mode'] == mode]\n",
    "        pal = base_palette if mode == 'Zero' else few_palette\n",
    "\n",
    "        # Accuracy\n",
    "        ax_acc = axes1[0, i]\n",
    "        if not sub.empty:\n",
    "            sns.barplot(data=sub, x='Method', y='Accuracy', palette=pal, ax=ax_acc)\n",
    "            ax_acc.set_title(f'Accuracy ({mode}-shot)')\n",
    "            ax_acc.set_ylim(0, 1.1)\n",
    "            ax_acc.set_ylabel('Success Rate' if i == 0 else '')\n",
    "            for p in ax_acc.patches:\n",
    "                h = p.get_height()\n",
    "                ax_acc.annotate(f'{h:.2f}', (p.get_x() + p.get_width()/2., h),\n",
    "                                ha='center', va='center', xytext=(0, 8), textcoords='offset points',\n",
    "                                fontsize=12, fontweight='bold')\n",
    "\n",
    "        # Avg Rounds\n",
    "        ax_rds = axes1[1, i]\n",
    "        if not sub.empty:\n",
    "            sns.barplot(data=sub, x='Method', y='Avg_Rounds', palette=pal, ax=ax_rds)\n",
    "            ax_rds.set_title(f'Avg Interaction Rounds ({mode}-shot)')\n",
    "            ax_rds.set_ylabel('Rounds' if i == 0 else '')\n",
    "            max_h = 0\n",
    "            for p in ax_rds.patches:\n",
    "                h = p.get_height()\n",
    "                max_h = max(max_h, h)\n",
    "                ax_rds.annotate(f'{h:.2f}', (p.get_x() + p.get_width()/2., h),\n",
    "                                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "            ax_rds.set_ylim(0, max_h * 1.25 if max_h > 0 else 1)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        fig1.savefig(os.path.join(save_dir, 'performance_overview.png'), dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "    # Âõæ 2ÔºöÊ≠£Á°ÆÊÄßÂàÜËß£ÔºàÁ¥ØÁßØÊü±Áä∂ÂõæÔºåÂàÜÊÆµÊØî‰æãÊ†áÁ≠æÔºâ\n",
    "    status_order = ['Initial Correct', 'Interactive Correct', 'Syntax Fix Correct', 'Incorrect']\n",
    "    status_palette = {'Initial Correct': '#1f77b4', 'Interactive Correct': '#2ca02c',\n",
    "                      'Syntax Fix Correct': '#17becf', 'Incorrect': '#d62728'}\n",
    "\n",
    "    breakdown = []\n",
    "    for (m, md), grp in df.groupby(['Method', 'Mode']):\n",
    "        t = len(grp)\n",
    "        vals = {s: (len(grp[grp['Status'] == s]) / t if t else 0) for s in status_order}\n",
    "        for s in status_order:\n",
    "            breakdown.append({'Method': m, 'Mode': md, 'Type': s, 'Prop': vals[s]})\n",
    "    bdf = pd.DataFrame(breakdown)\n",
    "\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig2.suptitle('Correctness Breakdown', fontsize=18, fontweight='bold')\n",
    "\n",
    "    methods = ['M1', 'M2', 'M3']\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.6\n",
    "\n",
    "    for i, mode in enumerate(modes):\n",
    "        ax = axes2[i]\n",
    "        sub = bdf[bdf['Mode'] == mode]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        pivot = sub.pivot(index='Method', columns='Type', values='Prop').reindex(\n",
    "            index=methods, columns=status_order).fillna(0)\n",
    "\n",
    "        bottoms = np.zeros(len(methods))\n",
    "        bars_info = []\n",
    "        for s in status_order:\n",
    "            h = pivot[s].values\n",
    "            b = ax.bar(x, h, width, bottom=bottoms, color=status_palette[s], label=s)\n",
    "            bars_info.append((s, h, bottoms, b))\n",
    "            bottoms = bottoms + h\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(methods)\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        ax.set_ylabel('Proportion' if i == 0 else '')\n",
    "        ax.set_title(f'Breakdown ({mode}-shot)')\n",
    "\n",
    "        # ÊØè‰∏™ÂàÜÊÆµÁöÑÊØî‰æãÊ†áÁ≠æ\n",
    "        for s, h, btm, b in bars_info:\n",
    "            for j in range(len(h)):\n",
    "                val = h[j]\n",
    "                if val > 0.02:\n",
    "                    y = btm[j] + val / 2\n",
    "                    lbl = f'{val:.0%}'\n",
    "                    ax.text(x[j], y, lbl, ha='center', va='center',\n",
    "                            fontsize=12, fontweight='bold',\n",
    "                            color='#000000' if val < 0.25 else '#ffffff')\n",
    "\n",
    "        if i == 1:\n",
    "            ax.legend(title='Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        else:\n",
    "            leg = ax.get_legend()\n",
    "            if leg is not None:\n",
    "                leg.remove()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        fig2.savefig(os.path.join(save_dir, 'correctness_breakdown.png'), dpi=160)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c630e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Empty results",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(json.load(f)))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Áõ¥Êé•ÈáçÁªòÔºàÂπ∂‰øùÂ≠òÔºâ\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m redraw_from_results(\u001b[33m'\u001b[39m\u001b[33mexperiment_results.json\u001b[39m\u001b[33m'\u001b[39m, save_dir=\u001b[33m'\u001b[39m\u001b[33m./figs\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mredraw_from_results\u001b[39m\u001b[34m(json_path, save_dir)\u001b[39m\n\u001b[32m     18\u001b[39m df = pd.DataFrame(loaded)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mEmpty results\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# ÂÖ®Â±ÄÊõ¥Â§ßÂ≠ó‰Ωì‰∏éÁ¨¶Âè∑Â∞∫ÂØ∏\u001b[39;00m\n\u001b[32m     23\u001b[39m sns.set_theme(style=\u001b[33m'\u001b[39m\u001b[33mwhite\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Empty results"
     ]
    }
   ],
   "source": [
    "\n",
    "# Á°ÆËÆ§Êñá‰ª∂Â≠òÂú®‰∏éËÆ∞ÂΩïÊï∞\n",
    "import os, json\n",
    "print(os.path.exists('experiment_results.json'))\n",
    "with open('experiment_results.json') as f:\n",
    "    print(len(json.load(f)))\n",
    "\n",
    "# Áõ¥Êé•ÈáçÁªòÔºàÂπ∂‰øùÂ≠òÔºâ\n",
    "redraw_from_results('experiment_results.json', save_dir='./figs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4e1b24be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ÂºÄÂßãË∞ÉËØïÊºîÁ§∫ (Starting Debug Demo)...\n",
      "\n",
      "--- [Step 1] ÊèêÂèñÊï∞ÊçÆÂ∫ìÁªìÊûÑ (Extract Schema) ---\n",
      "\n",
      "==================== Ê≠£Âú®Ë∞ÉÁî® (Calling) generate_db_schema ====================\n",
      ">> ËæìÂÖ•ÂèÇÊï∞ (Inputs):\n",
      "   - db_path: debug_demo.db\n",
      ">> ÊâßË°åÁªìÊûú (Output) -> CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, role TEXT)\n",
      "==================== ÁªìÊùü (End) generate_db_schema ====================\n",
      "\n",
      "\n",
      "--- [Step 2] Ê®°ÂûãÁîüÊàê SQL (Generate SQL) ---\n",
      "\n",
      "==================== Ê≠£Âú®Ë∞ÉÁî® (Calling) mock_llm_generation ====================\n",
      ">> ËæìÂÖ•ÂèÇÊï∞ (Inputs):\n",
      "   - prompt: Question: Find all users older than 20.\n",
      "Schema: CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, role TEXT)\n",
      "   (Ê®°Êãü LLM ÊÄùËÄÉ‰∏≠... Generating SQL...)\n",
      ">> ÊâßË°åÁªìÊûú (Output) -> ('', 0.0)\n",
      "==================== ÁªìÊùü (End) mock_llm_generation ====================\n",
      "\n",
      "\n",
      "--- [Step 3] Ê∏ÖÊ¥ó SQL (Clean SQL) ---\n",
      "\n",
      "==================== Ê≠£Âú®Ë∞ÉÁî® (Calling) clean_query ====================\n",
      ">> ËæìÂÖ•ÂèÇÊï∞ (Inputs):\n",
      "   - sql_query: \n",
      ">> ÊâßË°åÁªìÊûú (Output) -> SELECT\n",
      "==================== ÁªìÊùü (End) clean_query ====================\n",
      "\n",
      "\n",
      "--- [Step 4] ÊâßË°å‰∏éÈ™åËØÅ (Evaluate) ---\n",
      "   ÊâßË°åÂ§±Ë¥•: incomplete input\n",
      "\n",
      "‚úÖ ÊºîÁ§∫ÁªìÊùü (Demo Finished).\n"
     ]
    }
   ],
   "source": [
    "# === ‰∫§‰∫íÂºèË∞ÉËØï/ÊºîÁ§∫Â∑•ÂÖ∑ (Interactive Debugging Tools) ===\n",
    "import inspect\n",
    "import functools\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "def debug_wrapper(func):\n",
    "    \"\"\"\n",
    "    Ë£ÖÈ•∞Âô®ÔºöÊâìÂç∞ÂáΩÊï∞ÁöÑËæìÂÖ•ÂèÇÊï∞ÂíåËæìÂá∫ÁªìÊûúÔºåÂ∏ÆÂä©ÁêÜËß£ÊØè‰∏ÄÊ≠•ÁöÑÊï∞ÊçÆÂèòÊç¢„ÄÇ\n",
    "    Decorator: Prints function inputs and outputs to help understand data transformations at each step.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"\\n{'='*20} Ê≠£Âú®Ë∞ÉÁî® (Calling) {func.__name__} {'='*20}\")\n",
    "        \n",
    "        # 1. ÊâìÂç∞ËæìÂÖ• (Print Inputs)\n",
    "        sig = inspect.signature(func)\n",
    "        try:\n",
    "            bound_args = sig.bind(*args, **kwargs)\n",
    "            bound_args.apply_defaults()\n",
    "            print(\">> ËæìÂÖ•ÂèÇÊï∞ (Inputs):\")\n",
    "            for k, v in bound_args.arguments.items():\n",
    "                val_str = str(v)\n",
    "                # Êà™Êñ≠ËøáÈïøÁöÑËæìÂá∫ (Truncate long outputs)\n",
    "                if len(val_str) > 500: val_str = val_str[:500] + \"... (truncated)\"\n",
    "                print(f\"   - {k}: {val_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   (ÂèÇÊï∞Ëß£ÊûêÂ§±Ë¥•: {e})\")\n",
    "            \n",
    "        # 2. Ê®°ÊãüÊñ≠ÁÇπ (Optional Breakpoint)\n",
    "        # ÂèñÊ∂à‰∏ãË°åÊ≥®ÈáäÂèØ‰ª•Âú®ÊØè‰∏ÄÊ≠•ÊöÇÂÅúÁ≠âÂæÖÂõûËΩ¶ (Uncomment below to pause at each step)\n",
    "        # input(\">> ÊåâÂõûËΩ¶ÈîÆÁªßÁª≠ÊâßË°å (Press Enter to continue)...\") \n",
    "        \n",
    "        # 3. ÊâßË°åÂáΩÊï∞ (Execute)\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # 4. ÊâìÂç∞ËæìÂá∫ (Print Output)\n",
    "            res_str = str(result)\n",
    "            if len(res_str) > 500: res_str = res_str[:500] + \"... (truncated)\"\n",
    "            print(f\">> ÊâßË°åÁªìÊûú (Output) -> {res_str}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\">> ÂèëÁîüÂºÇÂ∏∏ (Exception) -> {e}\")\n",
    "            raise e\n",
    "        finally:\n",
    "            print(f\"{'='*20} ÁªìÊùü (End) {func.__name__} {'='*20}\\n\")\n",
    "    return wrapper\n",
    "\n",
    "def run_debug_demo():\n",
    "    \"\"\"\n",
    "    ËøêË°å‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ Text2SQL ÊµÅÁ®ãÊºîÁ§∫ÔºåÂ±ïÁ§∫Êï∞ÊçÆÂú®ÂêÑ‰∏™ÂáΩÊï∞Èó¥ÁöÑÊµÅÂä®„ÄÇ\n",
    "    Runs a full Text2SQL flow demo to show data flow between functions.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ ÂºÄÂßãË∞ÉËØïÊºîÁ§∫ (Starting Debug Demo)...\")\n",
    "    \n",
    "    # A. ÂáÜÂ§áÊµãËØïÁéØÂ¢ÉÔºöÂàõÂª∫‰∏Ä‰∏™‰∏¥Êó∂Êï∞ÊçÆÂ∫ì (Setup Dummy DB)\n",
    "    db_path = \"debug_demo.db\"\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "    \n",
    "    # ÂàõÂª∫Ë°®Âπ∂ÊèíÂÖ•Êï∞ÊçÆ\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, role TEXT)\")\n",
    "    conn.execute(\"INSERT INTO users (name, age, role) VALUES ('Alice', 30, 'admin'), ('Bob', 25, 'user'), ('Charlie', 35, 'user')\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # B. ÂåÖË£ÖÁé∞ÊúâÂáΩÊï∞ (Wrap existing functions)\n",
    "    # Ê≥®ÊÑèÔºöËøô‰∫õÂáΩÊï∞ÈúÄË¶ÅÂú® notebook ÂâçÈù¢ÁöÑÂçïÂÖÉÊ†º‰∏≠Â∑≤ÂÆö‰πâ\n",
    "    # Note: These functions must be defined in previous cells\n",
    "    debug_generate_schema = debug_wrapper(generate_db_schema)\n",
    "    debug_clean_query = debug_wrapper(clean_query)\n",
    "    \n",
    "    # Ê®°Êãü LLM (Mock LLM) ‰ª•ÈÅøÂÖç API Ë∞ÉÁî®\n",
    "    # ÁúüÂÆûÂú∫ÊôØ‰∏≠‰Ω†ÂèØ‰ª•ÊõøÊç¢‰∏∫ debug_wrapper(LLM_generation)\n",
    "    @debug_wrapper\n",
    "    def mock_llm_generation(prompt):\n",
    "        print(\"   (Ê®°Êãü LLM ÊÄùËÄÉ‰∏≠... Generating SQL...)\")\n",
    "        return \"\", 0.0\n",
    "\n",
    "    # C. ÊâßË°åÊµÅÁ®ã (Execute Flow)\n",
    "    \n",
    "    # Step 1: ÊèêÂèñ Schema\n",
    "    print(\"\\n--- [Step 1] ÊèêÂèñÊï∞ÊçÆÂ∫ìÁªìÊûÑ (Extract Schema) ---\")\n",
    "    schema = debug_generate_schema(db_path)\n",
    "    \n",
    "    # Step 2: ÁîüÊàê SQL\n",
    "    print(\"\\n--- [Step 2] Ê®°ÂûãÁîüÊàê SQL (Generate SQL) ---\")\n",
    "    user_question = \"Find all users older than 20.\"\n",
    "    prompt = f\"Question: {user_question}\\nSchema: {schema}\"\n",
    "    raw_sql_output, cost = mock_llm_generation(prompt)\n",
    "    \n",
    "    # Step 3: Ê∏ÖÊ¥ó SQL\n",
    "    print(\"\\n--- [Step 3] Ê∏ÖÊ¥ó SQL (Clean SQL) ---\")\n",
    "    cleaned_sql = debug_clean_query(raw_sql_output)\n",
    "    \n",
    "    # Step 4: È™åËØÅ/ÊâßË°å (Evaluate)\n",
    "    print(\"\\n--- [Step 4] ÊâßË°å‰∏éÈ™åËØÅ (Evaluate) ---\")\n",
    "    # ËøôÈáåÊàë‰ª¨Áõ¥Êé•ÊâßË°åÁúãÁúãÁªìÊûú\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        res = conn.execute(cleaned_sql).fetchall()\n",
    "        print(f\"   Êü•ËØ¢ÊâßË°åÁªìÊûú: {res}\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"   ÊâßË°åÂ§±Ë¥•: {e}\")\n",
    "\n",
    "    # Ê∏ÖÁêÜÊñá‰ª∂\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "    print(\"\\n‚úÖ ÊºîÁ§∫ÁªìÊùü (Demo Finished).\")\n",
    "\n",
    "# ËøêË°åÊºîÁ§∫\n",
    "run_debug_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84279f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-shot Full Flow ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_db_schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEqual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mok\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; Errors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m     os.remove(db_path)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m zero_shot_flow()\n\u001b[32m    113\u001b[39m few_shot_flow()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mzero_shot_flow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     65\u001b[39m db_path = \u001b[33m\"\u001b[39m\u001b[33mzero_few_demo.db\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m setup_demo_db(db_path)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m dbg_schema = debug_wrapper(generate_db_schema)\n\u001b[32m     69\u001b[39m schema = dbg_schema(db_path)\n\u001b[32m     71\u001b[39m nlq = \u001b[33m\"\u001b[39m\u001b[33mList names of users with role=\u001b[39m\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m\u001b[33m older than 20\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_db_schema' is not defined"
     ]
    }
   ],
   "source": [
    "import os, sqlite3, functools, inspect, re\n",
    "\n",
    "def debug_wrapper(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"\\n=== Calling {func.__name__} ===\")\n",
    "        try:\n",
    "            sig = inspect.signature(func)\n",
    "            bound = sig.bind(*args, **kwargs)\n",
    "            bound.apply_defaults()\n",
    "            print(\"Inputs:\")\n",
    "            for k,v in bound.arguments.items():\n",
    "                s = str(v)\n",
    "                if len(s) > 500: s = s[:500] + \"...\"\n",
    "                print(f\" - {k}: {s}\")\n",
    "        except Exception as e:\n",
    "            print(f\" (Signature parse failed: {e})\")\n",
    "        try:\n",
    "            out = func(*args, **kwargs)\n",
    "            s = str(out)\n",
    "            if len(s) > 500: s = s[:500] + \"...\"\n",
    "            print(f\"Output: {s}\")\n",
    "            return out\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            print(f\"=== End {func.__name__} ===\")\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def compose_zero_shot_prompt(nlq, schema):\n",
    "    meta = build_metadata_constraints(nlq, schema)\n",
    "    return sql_generation_selfdebug.format(schema=schema, sqls=\"\", metadata=meta, question=nlq)\n",
    "\n",
    "\n",
    "def compose_few_shot_prompt(nlq, schema, exemplars):\n",
    "    ex_snippets = \"\\n\".join([f\"/* example NLQ: {e['nlq']} */\\n/* example SQL: {e['sql']} */\" for e in exemplars])\n",
    "    meta = build_metadata_constraints(nlq, schema)\n",
    "    return sql_generation_v2.format(schema=schema, sqls=ex_snippets, cqas=\"\", metadata=meta, question=nlq)\n",
    "\n",
    "\n",
    "def safe_llm(prompt, **kwargs):\n",
    "    try:\n",
    "        return LLM_generation(prompt, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM failed ({e}); using mock.\")\n",
    "        return \"```sql\\nSELECT name FROM users WHERE role='user' AND age > 20\\n```\", 0.0\n",
    "\n",
    "\n",
    "def setup_demo_db(db_path):\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, role TEXT)\")\n",
    "    conn.execute(\"INSERT INTO users (name, age, role) VALUES ('Alice', 30, 'admin')\")\n",
    "    conn.execute(\"INSERT INTO users (name, age, role) VALUES ('Bob', 25, 'user')\")\n",
    "    conn.execute(\"INSERT INTO users (name, age, role) VALUES ('Charlie', 35, 'user')\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def zero_shot_flow():\n",
    "    print(\"\\n--- Zero-shot Full Flow ---\")\n",
    "    db_path = \"zero_few_demo.db\"\n",
    "    setup_demo_db(db_path)\n",
    "\n",
    "    dbg_schema = debug_wrapper(generate_db_schema)\n",
    "    schema = dbg_schema(db_path)\n",
    "\n",
    "    nlq = \"List names of users with role='user' older than 20\"\n",
    "    dbg_compose_zero = debug_wrapper(compose_zero_shot_prompt)\n",
    "    prompt = dbg_compose_zero(nlq, schema)\n",
    "\n",
    "    dbg_llm = debug_wrapper(safe_llm)\n",
    "    raw_sql, cost = dbg_llm(prompt, model='gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "    dbg_clean = debug_wrapper(clean_query)\n",
    "    cleaned_sql = dbg_clean(raw_sql)\n",
    "\n",
    "    gold_sql = \"SELECT name FROM users WHERE role='user' AND age > 20\"\n",
    "    print(\"Compare source vs gold (exec results):\")\n",
    "    ok, errs = evalfunc(cleaned_sql, gold_sql, db_path)\n",
    "    print(f\"Equal: {ok}; Errors: {errs}\")\n",
    "\n",
    "    os.remove(db_path)\n",
    "\n",
    "\n",
    "def few_shot_flow():\n",
    "    print(\"\\n--- Few-shot Full Flow ---\")\n",
    "    db_path = \"zero_few_demo.db\"\n",
    "    setup_demo_db(db_path)\n",
    "\n",
    "    schema = generate_db_schema(db_path)\n",
    "\n",
    "    exemplars = [\n",
    "        {\"nlq\": \"Find user names older than 30\", \"sql\": \"SELECT name FROM users WHERE age > 30\"},\n",
    "        {\"nlq\": \"Count admins\", \"sql\": \"SELECT COUNT(*) FROM users WHERE role='admin'\"}\n",
    "    ]\n",
    "\n",
    "    prompt = compose_few_shot_prompt(\"List names of users with role='user' older than 20\", schema, exemplars)\n",
    "\n",
    "    raw_sql, cost = safe_llm(prompt, model='gpt-3.5-turbo', temperature=0.0)\n",
    "    cleaned_sql = clean_query(raw_sql)\n",
    "\n",
    "    gold_sql = \"SELECT name FROM users WHERE role='user' AND age > 20\"\n",
    "    ok, errs = evalfunc(cleaned_sql, gold_sql, db_path)\n",
    "    print(f\"Equal: {ok}; Errors: {errs}\")\n",
    "\n",
    "    os.remove(db_path)\n",
    "\n",
    "zero_shot_flow()\n",
    "few_shot_flow()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
