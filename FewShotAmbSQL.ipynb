{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import collections\n",
    "import ast\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "import sqlite3\n",
    "import tiktoken\n",
    "import xxhash\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key set: True\n",
      "Base URL set: https://api.agicto.cn/v1\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\")\n",
    ")\n",
    "\n",
    "print(f\"API Key set: {bool(os.getenv('OPENAI_API_KEY'))}\")\n",
    "print(f\"Base URL set: {os.getenv('OPENAI_BASE_URL')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(fname, d):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(d, f)\n",
    "def clean_query(sql_query):\n",
    "    sql_query = sql_query.replace(\"```sql\", '')\n",
    "    sql_query = sql_query.replace(\"```\", '')\n",
    "    sql_query = sql_query.replace(';', '')\n",
    "    sql_query = sql_query.replace('\"\"\"', '')\n",
    "    if 'SELECT' not in sql_query.upper()[:10]:\n",
    "        sql_query = 'SELECT ' + sql_query\n",
    "    return sql_query\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_db_schema(database):\n",
    "    db_path = f'./databases/{database}/{database}.sqlite'\n",
    "    conn = sqlite3.connect(db_path, uri=True)\n",
    "    full_schema_prompt_list = []\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    schemas = {}\n",
    "    for table in tables:\n",
    "        if table == 'sqlite_sequence':\n",
    "            continue\n",
    "        cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{}';\".format(table[0]))\n",
    "        create_prompt = cursor.fetchone()[0]\n",
    "        schemas[table[0]] = create_prompt\n",
    "\n",
    "    for k, v in schemas.items():\n",
    "        full_schema_prompt_list.append(v)\n",
    "\n",
    "    schema_prompt = \"\\n\\n\".join(full_schema_prompt_list)\n",
    "\n",
    "    return schema_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "from multiprocessing import Process, Queue\n",
    "import query_module\n",
    "\n",
    "def evalfunc(sql_source, sql_target, database, source='kaggle'):\n",
    "    db_path = f'./databases/{database}/{database}.sqlite'\n",
    "    if not os.path.isfile(db_path):\n",
    "        print(\"cannot find file\", db_path)\n",
    "        return False\n",
    "    timeout = 120\n",
    "    output = Queue()\n",
    "    query_process = Process(target=query_module.execute_query, args=(db_path, sql_source, output))\n",
    "    query_process.start()\n",
    "    output_hash = ''\n",
    "    \n",
    "    try:\n",
    "        # Connect to sqlite db\n",
    "        # Execute both!\n",
    "        source_results = None\n",
    "        source_results = output.get(True, timeout+5)\n",
    "        query_process.join(timeout)\n",
    "        if query_process.is_alive():\n",
    "            print(\"process terminated\")\n",
    "            query_process.terminate()  # Terminate the process\n",
    "            query_process.join()  # Make sure it's cleaned up\n",
    "            return False, [Exception('SQL query took too much time to execute.')]\n",
    "        if isinstance(source_results, Exception):\n",
    "            raise source_results\n",
    "        output_hash = xxhash.xxh128_hexdigest(str(len(source_results)), seed=123)\n",
    "        connection = sqlite3.connect(db_path)\n",
    "        cursor = connection.cursor()\n",
    "        target_results = cursor.execute(sql_target).fetchall()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        # If the lengths don't match... there's no hope\n",
    "        if len(source_results) != len(target_results):\n",
    "            # (result matches or not, valid, hash)\n",
    "            return False, []\n",
    "        if 'ORDER BY' in sql_target:\n",
    "            for a, b in zip(source_results, target_results):\n",
    "                # NOTE: we are doing compares that are column-order independent\n",
    "                # hence the sorting and the weird key (since we may have mixed\n",
    "                # types in a row)\n",
    "                lhs = tuple(sorted(list(a), key=lambda x: hash(x)))\n",
    "                rhs = tuple(sorted(list(b), key=lambda x: hash(x)))\n",
    "                output_hash = xxhash.xxh128_hexdigest(output_hash + str(lhs), seed=123)\n",
    "                if lhs != rhs:\n",
    "                    # Oh no, a row doesn't match!\n",
    "                    return False, []\n",
    "        else:\n",
    "            lset, rset = set(), set()\n",
    "            for a, b in zip(source_results, target_results):\n",
    "                # NOTE: we are doing compares that are column-order independent\n",
    "                # hence the sorting and the weird key (since we may have mixed\n",
    "                # types in a row)\n",
    "                lset.add(tuple(sorted(list(a), key=lambda x: hash(x))))\n",
    "                rset.add(tuple(sorted(list(b), key=lambda x: hash(x))))\n",
    "            output_hash = xxhash.xxh128_hexdigest(str(lset), seed=123)\n",
    "            if lset != rset:\n",
    "                # Oh no, rows don't match!\n",
    "                return False, []\n",
    "    # If we hit an error, that's not a match I guess...\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return False, [ex]\n",
    "    return True, []\n",
    "\n",
    "\n",
    "def outputHash(sql_source, database):\n",
    "    db_path = f'./databases/{database}/{database}.sqlite'\n",
    "    output_hash = ''\n",
    "    try:\n",
    "        # Connect to sqlite db\n",
    "        connection = sqlite3.connect(db_path)\n",
    "        cursor = connection.cursor()\n",
    "        source_results = cursor.execute(sql_source).fetchall()\n",
    "        output_hash = xxhash.xxh128_hexdigest(str(len(source_results)), seed=123)\n",
    "        if 'ORDER BY' in sql_source:\n",
    "            for a in source_results:\n",
    "                lhs = tuple(sorted(list(a), key=lambda x: hash(x)))\n",
    "                output_hash = xxhash.xxh128_hexdigest(output_hash + str(lhs), seed=123)\n",
    "        else:\n",
    "            lset = set()\n",
    "            for a in source_results:\n",
    "                lset.add(tuple(sorted(list(a), key=lambda x: hash(x))))\n",
    "            output_hash = xxhash.xxh128_hexdigest(str(lset), seed=123)\n",
    "    except Exception as ex:\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    return output_hash\n",
    "\n",
    "\n",
    "def execute(sql, database, source='kaggle'):\n",
    "    db_path = f'./databases/{database}/{database}.sqlite'\n",
    "        \n",
    "    if not os.path.isfile(db_path):\n",
    "        print(\"cannot find file\")\n",
    "        return False\n",
    "    results = ''\n",
    "    try:\n",
    "        # Connect to sqlite db\n",
    "        connection = sqlite3.connect(db_path)\n",
    "        cursor = connection.cursor()\n",
    "        results = cursor.execute(sql).fetchall()\n",
    "    # If we hit an error, that's not a match I guess...\n",
    "    except KeyboardInterrupt:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"KeyboardInterrupt\")\n",
    "        return False\n",
    "    except Exception as ex:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(ex)\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4_turbo_generation(prompt, t = 0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        n = 1,\n",
    "        stream = False,\n",
    "        temperature=t,\n",
    "        max_tokens=4096,\n",
    "        logprobs=True,\n",
    "    )\n",
    "    logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "    perplexity_score = np.exp(-np.mean(logprobs))\n",
    "    return response.choices[0].message.content.strip(), perplexity_score\n",
    "\n",
    "def GPT4o_generation(prompt, t = 0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        n = 1,\n",
    "        stream = False,\n",
    "        temperature=t,\n",
    "        max_tokens=4096,\n",
    "        logprobs=True,\n",
    "    )\n",
    "    logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "    perplexity_score = np.exp(-np.mean(logprobs))\n",
    "    return response.choices[0].message.content.strip(), perplexity_score\n",
    "\n",
    "\n",
    "\n",
    "def GPT35_generation(prompt, t = 0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        n = 1,\n",
    "        stream = False,\n",
    "        temperature=t,\n",
    "        max_tokens=4096,\n",
    "        logprobs=True,\n",
    "    )\n",
    "    logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "    perplexity_score = np.exp(-np.mean(logprobs))\n",
    "    return response.choices[0].message.content.strip(), perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load user_study.pkl (file missing or empty). Proceeding with empty userstudy list.\n",
      "272\n",
      "272\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('kaggle_dataset.csv')\n",
    "userstudy = []\n",
    "try:\n",
    "    with open('./user_study.pkl', 'rb') as f:\n",
    "        userstudy = pickle.load(f)\n",
    "except (EOFError, FileNotFoundError, pickle.UnpicklingError):\n",
    "    print(\"Warning: Could not load user_study.pkl (file missing or empty). Proceeding with empty userstudy list.\")\n",
    "    userstudy = []\n",
    "\n",
    "survey_questions = []\n",
    "# add original gold query inside\n",
    "for d in userstudy:\n",
    "    if 'Question2Ask' in d:\n",
    "        assert len(d['Question2Ask']) == len(d['Answer2Question']), print(d['nl'])\n",
    "    q = d['nl']\n",
    "    sql = df.loc[df['nl'] == q]['sql'].values\n",
    "    d['gold'] = sql[0]\n",
    "    d[\"target_schema\"] = df.loc[df['nl'] == q]['target_schema'].values\n",
    "    survey_questions.append(q)\n",
    "\n",
    "print(len(df))\n",
    "# drop the user study questions\n",
    "df = df[~df['nl'].isin(survey_questions)]\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_prefix_v1='''/* some examples are provided */\n",
    "/* example question: */\n",
    "How many acres burned in fires in California each year between 2000 and 2005?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  SUM(FIRE_SIZE),\\n  FIRE_YEAR\\nFROM Fires\\nWHERE\\n  State = \"CA\" AND FIRE_YEAR BETWEEN 2000 AND 2005\\nGROUP BY\\n  FIRE_YEAR\n",
    "/* example clarification question*/\n",
    "What information should the output table contain? a) two columns: the total acres burned and the year, b) one column: the total acres burned for each year, c) one column: the total acres burned across all target years, d) other (please specify).\n",
    "/* example reasoning */\n",
    "Output table is determined by the SELECT clause in the gold sql query. The gold query uses \u2018SELECT  SUM(FIRE_SIZE), FIRE_YEAR\u2019. As a result, the output table has two columns, the total acres burned and the year. Hence, choice a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"a) two columns: the total acres burned and the year\"\n",
    "\n",
    "/* example question: */\n",
    "Which states had the largest number of fires in 2001?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  State\\nFROM Fires\\nWHERE\\n  FIRE_YEAR = 2001\\nGROUP BY\\n  State\\nORDER BY\\n  COUNT(*) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Is the largest number of fires referring to? a) the total size of all fire incidents, b) the number of fire incidents, c) the largest size of all fire incidents, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The clarification question is asking about how to represent the largest number of fires. The gold query uses \u2018ORDER BY COUNT(*) DESC LIMIT 1\u2019 to find the largest number of fires. As a result, choice a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) the number of fire incidents\"\n",
    "\n",
    "/* example question: */\n",
    "What was the most common cause of fire between 2000 and 2005?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  STAT_CAUSE_DESCR\\nFROM Fires\\nWHERE\\n  FIRE_YEAR BETWEEN 2000 AND 2005\\nGROUP BY\\n  STAT_CAUSE_DESCR\\nORDER BY\\n  COUNT(*) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Which information should be used to represent the 'cause of fire'? a) the code that represents the cause, b) the description of the cause, c) both the code and the description of the cause, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The clarification question is asking for which column should be used to represent the cause of fire. The gold query uses the STAT_CAUSE_DESCR to represent the cause. As a result, choice b is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) the description of the cause\"\n",
    "\n",
    "/* example question: */\n",
    "Whose CDs sells best?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  artist\\nFROM torrents\\nGROUP BY\\n  artist\\nORDER BY\\n  SUM(totalSnatched) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Which column should be used to identify music related to 'CD'? a) groupName, b) tag, c) releaseType, d) other (please specify)\n",
    "/* example reasoning */\n",
    "The gold query does not use a WHERE clause to filter the CDs. Hence, the CD information is not contained in the tag column or the release type column. As a result, choice a, b, and c are all wrong.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \u201cd) Consider all music; No filter on \u2018CD\u2019 \u201d\n",
    "\n",
    "/* example question: */\n",
    "How many people wrote comments for the question \"Any additional notes or comments.\"? */\n",
    "/* example gold sql query*/\n",
    "SELECT COUNT(T1.UserID) FROM Answer AS T1 INNER JOIN Question AS T2 ON T1.QuestionID = T2.questionid WHERE T2.questiontext LIKE 'Any additional notes or comments' AND T1.AnswerText IS NOT NULL\n",
    "/* example clarification question*/\n",
    "How to determine if a user has provided comments? a) no check needed, b) see if `AnswerText` column has empty string, c) other (please specify).\n",
    "/* example reasoning */\n",
    "In the gold SQL query, it checks \u201cT1.AnswerText IS NOT NULL\u201d. Hence, choice a and b are both wrong.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"c) \u2018wrote comments\u2019 imply `AnswerText` is not a NULL value\".\n",
    "\n",
    "/* example question: */\n",
    "Calculate the difference between the number of customers and the number of subscribers who did the trip in June 2013. \n",
    "/* example gold sql query*/\n",
    "SELECT SUM(IIF(subscription_type = 'Subscriber', 1, 0)) - SUM(IIF(subscription_type = 'Customer', 1, 0)) FROM trip WHERE start_date LIKE '6/%/2013%'\n",
    "/* example clarification question*/\n",
    "What predicate value should be used to determine a trip in June 2013? a) start_data > 06/2013, b) start_data = \u2018June 2013\u2019, c) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold sql query uses start_date LIKE '6/%/2013%' to find trips in June 2013.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"c) start_date LIKE '6/%/2013%'\"\n",
    "\n",
    "\n",
    "/* example question: */\n",
    "Identify the players who weigh 120 kg.\n",
    "/* example gold sql query*/\n",
    "SELECT T2.PlayerName FROM weight_info AS T1 INNER JOIN PlayerInfo AS T2 ON T1.weight_id = T2.weight WHERE T1.weight_in_kg = 120\n",
    "/* example clarification question*/\n",
    "What fields should be contained in the output? a) one column of player name, b) one column of player id, c) two columns of player name and player ids, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold query selects \u2018SELECT T2.PlayerName\u2019. Hence, a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"a) one column of player name\"\n",
    "\n",
    "/* example question: */\n",
    "How many reviews are created for the podcast \"Scaling Global\" under?\n",
    "/* example gold sql query*/\n",
    "SELECT COUNT(T2.content) FROM podcasts AS T1 INNER JOIN reviews AS T2 ON T2.podcast_id = T1.podcast_id WHERE T1.title = 'Scaling Global'\n",
    "/* example clarification question*/\n",
    "Which column represents the reviews? a) `podcast` column, b) `content` column, c) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold query uses \u201cCOUNT(T2.content)\u201d to determine the number of reviews. Hence, b is correct in which the `content` column represents the reviews.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) `content` column\"\n",
    "\\n\\n\n",
    "'''\n",
    "\n",
    "\n",
    "feedback_v2 = \"\"\"/* Given the following Natural Language Question: */\n",
    "{nlq}\n",
    "/* And the following Gold Query: */\n",
    "{query}\n",
    "/* Answer the following multiple choice clarification question truthfully based on the Gold Query: */\n",
    "{question}\n",
    "\n",
    "/* Follow these steps:\n",
    "1. Identify which portion of the Gold Query answers the clarification question.\n",
    "2. Evaluate the correctness of each multiple choice answer based only on the Gold Query.\n",
    "3. If none of the choices are correct or you select \"other (please specify)\", provide a short answer for the clarification question.\n",
    "4. Output the final answer in the format: answer_to_cq = \"\".\n",
    "\n",
    "Let\u2019s proceed step by step. */\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cq_prefix_v1 = '''/* some examples are provided */\n",
    "/* example question: */\n",
    "Which artist/group is most productive?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: How to rank artist/group productivity? a) rank by the number of records produced, b) rank by the total number of downloads, c) other (please specify).\n",
    "user: b) rank by the total number of downloads```\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer should use ORDER BY and LIMIT 1 based on the sum of total downloads. However, it is unclear what columns should be used to represent the 'artist/group'.  Both the `artist` and the `groupName` columns contain information about 'artist/group'. \u2019\u2018AmbTableColumn\u2019 remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \"Which columns represent the 'artist/group' information? a) the artist column only, b) the groupName column only, c) both the artist column and the groupName column, d) other (please specify).\u201d```\n",
    "\n",
    "/* example question: */\n",
    "Which Premier League matches ended in a draw in 2016?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: Is the year '2016' referring to? a) season is 2016, b) season is either 2015/2016 or 2016/2017, c) the date time is at year 2016, d) other (specify).\n",
    "user: a) season is 2016,\n",
    "clarification questions: How to find the 'Premier league'? a) consider all leagues, b) consider only the league with name 'Premier League', c) other (specify).\n",
    "user: b) consider only the league with name 'Premier League'\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer to this question needs to contain a WHERE clause for three conditions: 'Premier League', 'draw', and 'in 2016'. However, the question did not specify what fields should be contained in the output table. 'AmbOutput' remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \u201cWhat fields represent the target 'matches'? a) all fields from football data table, b) the `league` column, c) other (specify).\u201d\n",
    "\n",
    "/* example question: */\n",
    "Which type of crime has the highest rate of \u2018Investigation complete\u2019?\n",
    "/* example previous clarification questions and user replies: */\n",
    "No previous clarification questions.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer to this question needs to contain a WHERE clause to find crimes that have 'Investigation complete' outcomes, uses ORDER BY and LIMIT 1 to find the type of crime with the highest rate, and the output table has only one row. However, it needs to be clarified i) what predicate value should be used for 'Investigation complete', and ii) how to represent the 'rate', and iii) if the output table contains only the crime type column or the crime type column with the highest rate aggregate. Hence, this question is ambiguous because of 'AmbVal', 'AmbQuestion', and \u2018AmbOutput\u2019.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \u201cWhat information should be used to find 'Investigation complete'? a) see if outcome contains the phrase 'Investigation complete', b)  see if outcome is 'Investigation complete; no suspect identified', c) other (please specify).\u201d\n",
    "\n",
    "/* example question: */\n",
    "For award winners, which position has the most hall of fame players?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: How should the 'position' for players be identified? a) by the `award_id` column, b) by the `category` column, c) other (please specify).\n",
    "user: c)  by the `note` column\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the answer should use the `note` column for player \u2018positions\u2019. However, it is unclear what fields should contain in the output table. Hence \u2018AmbOutput\u2019 remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \u201cWhat fields should be contained in the output table? a) one field: the position, b) two fields: the position and the number of hall-of-fame players, c) other (please specify).\u201d\n",
    "\n",
    "/* example question: */\n",
    "How many Wisconsin school districts receive federal funding?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: How to determine if a district has received federal funding? a) based on the t_fed_rev is larger than 0, b) the answer does not need to consider this aspect, c) other (please specify).\n",
    "user: c) every school in `FINREV_FED_17` table has received federal funding.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that every school in `FINREV_FED_17` table have received federal funding. However, it is unclear if the word \u2018Wisconsin\u2019 refers to the state or the school district. Hence, \u2018AmbQuestion\u2019 remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \u201cIs 'Wisconsin school districts' referring to? a) all school districts in the state Wisconsin, b) school districts with names that contain Wisconsin, c) other (please specify).\u201d\n",
    "\n",
    "/* example question: */\n",
    "How many 2-year public schools are there in \"California\"?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: Which column(s) should be used to find \u20182-year public schools\u2019? a) `level` column, b) `control` column, c) other (please specify).\n",
    "user: c) use both `level` and `control` columns to find \u20182-year public schools\u2019 information.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the correct SQL answer should have a WHERE clause with filters based on the `level` and `control` columns. However, it is unclear what predicate values should be used for these two columns. Hence, \u2018AmbValue\u2019 remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \u201cWhat predicate values should be used for the `level` and `control` columns to find  \u20182-year public schools\u2019? a) \u20182-year\u2019 and \u2018public\u2019 b) \u20182\u2019 and \u2018public, c) other (please specify)\u2019.\u201d \n",
    "\n",
    "/* example question: */\n",
    "Calculate the total beat of the crimes reported in a community area in the central side with a population of 50,000 and above.\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: What column and predicate value should be used to determine \u2018central side\u2019? a) Column `side` in table `Community_Area` with value \u2018central\u2019, b) Column `side` in table `Community_Area` with value \u2018Central\u2019, c) other (please specify).\n",
    "user: b) Column `side` in table `Community_Area` with value \u2018Central\u2019\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the output table should contain a single number and use the predicate \u2018Central\u2019 in `Community_Area`.`side`; However, it is not clear which column of statistics is \u2018total beat\u2019 referring to. Hence, AmbTableColumn remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \u201cWhich column is related to \u2018total beat\u2019? a) `Crime`.`beat`, b) `Crime`.`report_no`, c) other (please specify).\u201d\n",
    "\n",
    "/* example question: */\n",
    "Of all the nonessential genes that are not of the motorprotein class and whose phenotype is cell cycle defects, how many do not have a physical type of interaction?\n",
    "/* example previous clarification questions and user replies: */\n",
    "No previous clarification questions.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that \u2018phenotype\u2019 is referring to the `Phenotype` column, \u2018motorprotein class\u2019 is referring to the `class` column, \u2018nonessential genes\u2019 is referring to the `essential` column, and `physical type` is referring to the `type` column. However, it is unclear what fields should be contained in the output table, and hence \u2018AmbOutput\u2019 remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \u201cWhat fields should be included in the output table? a) One column for the number of genes b) Two columns for GeneID and physical type c) Other (please specify).\u201d\n",
    "\\n\\n\n",
    "'''\n",
    "\n",
    "\n",
    "SRA = \"\"\"/* Ask the user a new multiple choice clarification question to help you find the correct SQL answer for the following question: */\n",
    "{question}\n",
    "/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following previous clarification questions and user replies: */\n",
    "{cqs}\n",
    "\n",
    "/* Consider the following ambiguity categories:\n",
    "    - AmbQuestion: Is the question itself ambiguous?\n",
    "    - AmbTableColumn: Is there ambiguity in mapping the entities from the QUESTION to tables and columns in the DATABASE SCHEMA?\n",
    "    - AmbOutput: What fields and how many fields should be included in the output table?\n",
    "    - AmbValue: What predicate value should be used to filter results?\n",
    "*/\n",
    "\n",
    "/* The clarification question should be easy to understand for people with no coding experience. */\n",
    "\n",
    "/* Let's think step by step to generate the helpful multiple choice clarification question.\n",
    "1. Summarize the clear information based on previous clarification questions and incorrect queries.\n",
    "2. Evaluate whether AmbQuestion, AmbTableColumn, AmbOutput, and AmbValue remain in formulating an SQL query, considering each category individually.\n",
    "3. Ask a new multiple-choice question to address the remaining ambiguities and assist in identifying the correct SQL query. Use format: mul_choice_cq = \"\".\n",
    "*/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SRA_ES = \"\"\"/* Ask the user a new multiple choice clarification question to help you find the correct SQL answer for the following question: */\n",
    "{question}\n",
    "/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following previous clarification questions and user replies: */\n",
    "{cqs}\n",
    "\n",
    "/* Consider the following ambiguity categories:\n",
    "    - AmbQuestion: Is the question itself ambiguous?\n",
    "    - AmbTableColumn: Is there ambiguity in mapping the entities from the QUESTION to tables and columns in the DATABASE SCHEMA?\n",
    "    - AmbOutput: What fields and how many fields should be included in the output table?\n",
    "    - AmbValue: What predicate value should be used to filter results?\n",
    "*/\n",
    "\n",
    "/* The clarification question should be easy to understand for people with no coding experience. */\n",
    "\n",
    "/* Let's think step by step to generate the helpful multiple choice clarification question.\n",
    "1. Summarize the clear information based on previous clarification questions and incorrect queries.\n",
    "2. Evaluate whether AmbQuestion, AmbTableColumn, AmbOutput, and AmbValue remain in formulating an SQL query, considering each category individually.\n",
    "3. If no remaining ambiguities are identified, then output \"NO AMBIGUITY\".\n",
    "   Else, ask a new multiple-choice question to address the remaining ambiguities and assist in identifying the correct SQL query. Use format: mul_choice_cq = \"\".\n",
    "*/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAIL SQLNoRule\n",
    "sql_generation = '''/* Given the following database schema: */\n",
    "{schema}\n",
    "\n",
    "{metadata}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "SELECT '''\n",
    "\n",
    "# update to code representation\n",
    "sql_generation_v2 = '''/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following user replies to help you write the correct sql query: */\n",
    "{cqas}\n",
    "\n",
    "{metadata}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "SELECT '''\n",
    "\n",
    "# update to code representation\n",
    "fix_invalid_v1 = \"\"\"/* Given the following database schema: */\n",
    "{schema}`\n",
    "/* And the following inexecutable sql query */\n",
    "{invalidSQL}\n",
    "/* And the following exception message */\n",
    "{ex}\n",
    "\n",
    "/* Fix the exception and write a new executable SQL query with no explanation */\n",
    "SELECT \"\"\"\n",
    "\n",
    "# update to code representation\n",
    "sql_generation_selfdebug = '''/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "\n",
    "{metadata}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "SELECT '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdebug_examples_prefix = '''/* Given the following incorrect sql asnwers: */\n",
    "SELECT creation, COUNT(*) FROM department GROUP BY creation ORDER BY\n",
    "COUNT(*) DESC LIMIT 1\n",
    "/* Answer the following with no explanation: In which year were most departments established? */\n",
    "SELECT creation FROM department GROUP BY creation ORDER BY COUNT(*) DESC LIMIT 1\n",
    "-------\n",
    "/* Given the following incorrect sql asnwers: */\n",
    "SELECT customers.customer_name FROM customers JOIN orders ON customers.customer_id = orders.customer_id WHERE orders.order_status = \"On Road\" AND orders.order_status = \"Shipped\"\n",
    "/* Answer the following with no explanation: Which customers have both \"On Road\" and \"Shipped\" as order status? List the customer names. */\n",
    "SELECT customers.customer_name FROM customers JOIN orders ON customers.customer_id = orders.customer_id WHERE orders.order_status = \"On Road\" INTERSECT SELECT customers.customer_name FROM customers JOIN orders ON customers.customer_id = orders.customer_id WHERE orders.order_status = \"Shipped\"\n",
    "-------\n",
    "/* Given the following incorrect sql asnwers: */\n",
    "SELECT origin FROM flight WHERE destination = \"HONO\"\n",
    "/* Answer the following with no explanation: Show origins of all flights with destination Honolulu. */\n",
    "SELECT origin FROM flight WHERE destination = \"Honolulu\"\n",
    "-------\n",
    "/* Given the following incorrect sql asnwers: */\n",
    "SELECT AVG(long) FROM station WHERE id IN (SELECT station_id FROM status WHERE bikes_available <= 10)\n",
    "/* Answer the following with no explanation: What is the average longitude of stations that never had bike availability more than 10? */\n",
    "SELECT origin FROM flight WHERE destination = \"Honolulu\"\n",
    "SELECT AVG(long) FROM station WHERE id NOT IN (SELECT station_id FROM status WHERE bikes_available > 10)\n",
    "-------\n",
    "/* Given the following incorrect sql asnwers: */\n",
    "SELECT name, nationality FROM host WHERE age = (SELECT MIN(age) FROM host)\n",
    "/* Answer the following with no explanation: Show the name and the nationality of the oldest host. */\n",
    "SELECT name, nationality FROM host ORDER BY age DESC LIMIT 1\n",
    "-------\n",
    "/* Given the following incorrect sql asnwers: */\n",
    "SELECT COUNT(status) FROM city\n",
    "/* How many different statuses do cities have? */\n",
    "SELECT COUNT(DISTINCT status) FROM city\n",
    "-------'''\n",
    "selfdebug_examples = selfdebug_examples_prefix.split('-------')\n",
    "\n",
    "selfdebug_few_shot = []\n",
    "for i in range(1,7):\n",
    "    prefix = []\n",
    "    for j in range(i):\n",
    "        prefix.append(selfdebug_examples[j])\n",
    "    selfdebug_few_shot.append('\\n'.join(prefix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_prefix = \"/* some examples are provided */\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings()\n",
    "# # generate vectorstore for userstudy\n",
    "# examples = []\n",
    "# for study in userstudy:\n",
    "#     t = {'nl':study['nl'], 'gold':study['gold']}\n",
    "#     feedback = \"\"\n",
    "#     if 'Question2Ask' in study:\n",
    "#         for q, a in zip(study['Question2Ask'], study['Answer2Question']):\n",
    "#             feedback += \"multiple choice clarification question: \"+q+'\\nuser: '+a+'\\n'\n",
    "#     t['feedback'] = feedback\n",
    "#     examples.append(t)\n",
    "# to_vectorize = [example['nl'] for example in examples]\n",
    "# userstudy_vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples,  persist_directory=\"./userstudy_chroma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "userstudy_vectorstore = Chroma(persist_directory=\"./userstudy_chroma\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baselineFewShot(data_frame, history_log, log_name, rounds, num_of_tests, model_name, vectorstore, num_examples, data_source, with_metadata):\n",
    "    assert model_name in ['gpt35turbo', 'gpt4turbo']\n",
    "    assert num_examples < len(selfdebug_few_shot)\n",
    "    generation = None\n",
    "    if model_name == 'gpt35turbo':\n",
    "        generation = GPT35_generation\n",
    "    else:\n",
    "        generation = GPT4_turbo_generation\n",
    "    \n",
    "    for index in range(num_of_tests):\n",
    "        if index in history_log and \"num_cq_asked\" in history_log[index]:\n",
    "            # skip tests already seen\n",
    "            continue\n",
    "        \n",
    "        assert index in history_log\n",
    "        assert len(history_log[index]['sql_log']) == 1\n",
    "        \n",
    "        d = data_frame.iloc[[index]] \n",
    "        cqs_and_answers = []\n",
    "        evidence = ''\n",
    "        query = set()\n",
    "        if data_source == 'kaggle':\n",
    "            gold = d['sql'].values[0]\n",
    "            dbname = d['target_db'].values[0]\n",
    "            nlq = d['nl'].values[0]\n",
    "            dbschema = d['target_schema'].values[0]\n",
    "        elif data_source == 'bird':\n",
    "            gold = d['SQL'].values[0]\n",
    "            nlq = d['question'].values[0]\n",
    "            dbname = d['db_id'].values[0]\n",
    "            dbschema = generate_db_schema(dbname)\n",
    "            if with_metadata:\n",
    "                evidence = d['evidence'].values[0]\n",
    "        print(nlq, index, dbname, evidence)\n",
    "        \n",
    "        order, sql_prompt, sql_query, pscore = history_log[index]['sql_log'][0]\n",
    "        order += 1\n",
    "        sql_query = clean_query(sql_query)\n",
    "        query.add(sql_query)\n",
    "        execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "        \n",
    "        if exception:\n",
    "            most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "            query.remove(most_recent_sql)\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                 invalidSQL=most_recent_sql, ex=exception[0])\n",
    "            sql, pscore= generation(invalid_prompt)\n",
    "            sql = clean_query(sql)\n",
    "            history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "            order += 1\n",
    "            query.add(sql)\n",
    "            execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "        if execution:\n",
    "            history_log[index]['num_cq_asked'] = 0\n",
    "#             print()\n",
    "#             print(\"-----execution match-----\")\n",
    "#             print()\n",
    "            continue\n",
    "            \n",
    "        for turn in range(rounds):\n",
    "            sql_prompt = sql_generation_selfdebug.format(schema=dbschema, question=nlq,\\\n",
    "                                              sqls=\";\\n\".join(query), metadata=evidence)\n",
    "            sql_prompt = fewshot_prefix + selfdebug_few_shot[num_examples-1] + sql_prompt\n",
    "            sql_query, pscore= generation(sql_prompt)\n",
    "            history_log[index]['sql_log'].append((order, sql_prompt, sql_query, pscore))\n",
    "            order += 1\n",
    "            sql_query = clean_query(sql_query)\n",
    "            query.add(sql_query)\n",
    "            execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "            if exception:\n",
    "                most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "                query.remove(most_recent_sql)\n",
    "                invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                     invalidSQL=most_recent_sql, ex=exception[0])\n",
    "                sql, pscore= generation(invalid_prompt)\n",
    "                sql = clean_query(sql)\n",
    "                query.add(sql)\n",
    "                history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "                order += 1\n",
    "                execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            if execution:\n",
    "                history_log[index]['num_cq_asked'] = turn + 1\n",
    "#                 print()\n",
    "#                 print(\"********execution match*********\")\n",
    "#                 print()\n",
    "                break\n",
    "        if 'num_cq_asked' not in history_log[index]:\n",
    "            history_log[index]['num_cq_asked'] = \"Failed\"\n",
    "#         print('')\n",
    "#         print(\"------next question------\")\n",
    "#         print('')\n",
    "    save(log_name, history_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askClarificationQuestions(data_frame, history_log, log_name, rounds, num_of_tests, model_name, vectorstore_feedback, num_examples, data_source, with_metadata):    \n",
    "    assert model_name in ['gpt35turbo', 'gpt4turbo']\n",
    "    generation = None\n",
    "    if model_name == 'gpt35turbo':\n",
    "        generation = GPT35_generation\n",
    "    else:\n",
    "        generation = GPT4_turbo_generation\n",
    "    \n",
    "    feedback_example_selector = SemanticSimilarityExampleSelector(\n",
    "        vectorstore=vectorstore_feedback,\n",
    "        k=num_examples,\n",
    "    )\n",
    "    \n",
    "    feedback_example_prompt = PromptTemplate(\n",
    "        input_variables=['nl', 'gold', 'feedback'],\n",
    "        template=\"/* Given the following user feedback on clarification questions */\\n{feedback}\\n/* Answer the following with no explanation: {nl} */\\n{gold}\",\n",
    "    )\n",
    "    sql_generation_feedback_few_shot_prompt = FewShotPromptTemplate(\n",
    "        example_selector=feedback_example_selector,\n",
    "        example_prompt=feedback_example_prompt ,\n",
    "        suffix=sql_generation_v2,\n",
    "        input_variables=[\"question\", \"schema\", \"sqls\", \"cqas\", \"metadata\"],\n",
    "    )\n",
    "    \n",
    "    for index in range(num_of_tests):\n",
    "        if index in history_log and \"num_cq_asked\" in history_log[index]:\n",
    "            # skip tests already seen\n",
    "            print('skip')\n",
    "            continue\n",
    "        \n",
    "        assert index in history_log\n",
    "        assert len(history_log[index]['sql_log']) == 1\n",
    "        \n",
    "        d = data_frame.iloc[[index]] \n",
    "        cqs_and_answers = []\n",
    "        evidence = ''\n",
    "        query = set()\n",
    "        if data_source == 'kaggle':\n",
    "            gold = d['sql'].values[0]\n",
    "            dbname = d['target_db'].values[0]\n",
    "            nlq = d['nl'].values[0]\n",
    "            dbschema = d['target_schema'].values[0]\n",
    "        elif data_source == 'bird':\n",
    "            gold = d['SQL'].values[0]\n",
    "            nlq = d['question'].values[0]\n",
    "            dbname = d['db_id'].values[0]\n",
    "            dbschema = generate_db_schema(dbname)\n",
    "            if with_metadata:\n",
    "                evidence = d['evidence'].values[0]\n",
    "        print(\"nl: \", nlq, index, dbname, evidence)\n",
    "        \n",
    "        order, sql_prompt, sql_query, pscore = history_log[index]['sql_log'][0]\n",
    "        order += 1\n",
    "        sql_query = clean_query(sql_query)\n",
    "#         print(\"sql: \", sql_query, pscore)\n",
    "        query.add(sql_query)\n",
    "        execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "        \n",
    "        if exception:\n",
    "            most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "            query.remove(most_recent_sql)\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                 invalidSQL=most_recent_sql, ex=exception[0])\n",
    "            sql, pscore= generation(invalid_prompt)\n",
    "            sql = clean_query(sql)\n",
    "            history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "            order += 1\n",
    "            query.add(sql)\n",
    "            execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            \n",
    "            \n",
    "        if execution:\n",
    "            history_log[index]['num_cq_asked'] = 0\n",
    "#             print()\n",
    "#             print(\"-----execution match-----\")\n",
    "#             print()\n",
    "            continue\n",
    "            \n",
    "        for turn in range(rounds):\n",
    "            cqas = \"\"\n",
    "            if with_metadata:\n",
    "                cqas = 'user: ' + evidence + '\\n'\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"multiple choice clarification question: \"+cqs_and_answers[i]+'\\n'\n",
    "                else:\n",
    "                    cqas += \"user: \"+cqs_and_answers[i]+'\\n'\n",
    "            if cqas == \"\":\n",
    "                cqas = \"no previous clarification question.\\n\"\n",
    "            cq_prompt = SRA.format(schema=dbschema, question=nlq,\\\n",
    "                                            sqls=\";\\n\".join(query), cqs=cqas)\n",
    "            cq_prompt = cq_prefix_v1 + cq_prompt\n",
    "            cq, pscore= generation(cq_prompt)\n",
    "            history_log[index]['cq_log'].append((order, cq_prompt, cq, pscore))\n",
    "            order += 1\n",
    "            if \"mul_choice_cq = \" in cq:\n",
    "                cq = cq.split(\"mul_choice_cq = \")[-1]\n",
    "#             print(\"cq: \", cq)\n",
    "            feedback_prompt = feedback_v2.format(query = gold, question = cq, nlq=nlq)\n",
    "            feedback_prompt = feedback_prefix_v1 + feedback_prompt\n",
    "            feedback, pscore= GPT4o_generation(feedback_prompt)\n",
    "\n",
    "            history_log[index]['feedback_log'].append((order, feedback_prompt, feedback, pscore))\n",
    "            order += 1\n",
    "            if \"answer_to_cq =\" in feedback:\n",
    "                feedback = feedback.split(\"answer_to_cq =\")[-1]\n",
    "#             print()\n",
    "#             print(\"feedback, \", feedback)\n",
    "            cqs_and_answers.append(cq)\n",
    "            cqs_and_answers.append(feedback)\n",
    "        \n",
    "            # fix incorrect sql based on user feedback\n",
    "            cqas = \"\"\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"multiple choice clarification question: \"+cqs_and_answers[i]+'\\n'\n",
    "                else:\n",
    "                    cqas += \"user: \"+cqs_and_answers[i]+'\\n'\n",
    "            if cqas == '':\n",
    "                cqas = 'no previous clarification questions are asked.\\n'\n",
    "            \n",
    "            # use examples from the user study\n",
    "            sql_prompt = sql_generation_feedback_few_shot_prompt.format(schema=dbschema, question=nlq,\\\n",
    "                                              sqls=\";\\n\".join(query), cqas=cqas, metadata=evidence)\n",
    "            sql_prompt = \"/* some examples are provided */\\n\" + sql_prompt\n",
    "            sql_query, pscore= generation(sql_prompt)\n",
    "            sql_query = clean_query(sql_query)\n",
    "#             print(\"sql: \", sql_query, pscore)\n",
    "            history_log[index]['sql_log'].append((order, sql_prompt, sql_query, pscore))\n",
    "            order += 1\n",
    "            sql_query = clean_query(sql_query)\n",
    "            query.add(sql_query)\n",
    "            execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "            if exception:\n",
    "                most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "                query.remove(most_recent_sql)\n",
    "                invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                     invalidSQL=most_recent_sql, ex=exception[0])\n",
    "                sql, pscore= generation(invalid_prompt)\n",
    "                sql = clean_query(sql)\n",
    "                query.add(sql)\n",
    "                history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "                order += 1\n",
    "                execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            if execution:\n",
    "                history_log[index]['num_cq_asked'] = turn + 1\n",
    "#                 print()\n",
    "#                 print(\"********execution match*********\")\n",
    "#                 print()\n",
    "                break\n",
    "                \n",
    "        if 'num_cq_asked' not in history_log[index]:\n",
    "            history_log[index]['num_cq_asked'] = \"Failed\"\n",
    "#         print('')\n",
    "#         print(\"------next question------\")\n",
    "#         print('')\n",
    "    save(log_name, history_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askCQsBreakNoAmb(data_frame, history_log, log_name, rounds, num_of_tests, model_name, vectorstore_feedback, num_examples, data_source, with_metadata):    \n",
    "    assert model_name in ['gpt35turbo', 'gpt4turbo']\n",
    "    generation = None\n",
    "    if model_name == 'gpt35turbo':\n",
    "        generation = GPT35_generation\n",
    "    else:\n",
    "        generation = GPT4_turbo_generation\n",
    "    \n",
    "\n",
    "    feedback_example_selector = SemanticSimilarityExampleSelector(\n",
    "        vectorstore=vectorstore_feedback,\n",
    "        k=num_examples,\n",
    "    )\n",
    "    feedback_example_prompt = PromptTemplate(\n",
    "        input_variables=['nl', 'gold', 'feedback'],\n",
    "        template=\"\\nExample Question: {nl}\\nExample Feedback:{feedback}\\nExample Answer: {gold}\",\n",
    "    )\n",
    "\n",
    "    sql_generation_feedback_few_shot_prompt = FewShotPromptTemplate(\n",
    "        example_selector=feedback_example_selector,\n",
    "        example_prompt=feedback_example_prompt ,\n",
    "        suffix=sql_generation_v2,\n",
    "        input_variables=[\"question\", \"schema\", \"sqls\", \"cqas\", \"metadata\"],\n",
    "    )\n",
    "    \n",
    "    for index in range(num_of_tests):\n",
    "        if index in history_log and \"num_cq_asked\" in history_log[index]:\n",
    "            # skip tests already seen\n",
    "            continue\n",
    "        assert index in history_log\n",
    "        assert len(history_log[index]['sql_log']) == 1\n",
    "        \n",
    "        d = data_frame.iloc[[index]] \n",
    "        cqs_and_answers = []\n",
    "        evidence = ''\n",
    "        query = set()\n",
    "        if data_source == 'kaggle':\n",
    "            gold = d['sql'].values[0]\n",
    "            dbname = d['target_db'].values[0]\n",
    "            nlq = d['nl'].values[0]\n",
    "            dbschema = d['target_schema'].values[0]\n",
    "        elif data_source == 'bird':\n",
    "            gold = d['SQL'].values[0]\n",
    "            nlq = d['question'].values[0]\n",
    "            dbname = d['db_id'].values[0]\n",
    "            dbschema = generate_db_schema(dbname)\n",
    "            if with_metadata:\n",
    "                evidence = d['evidence'].values[0]\n",
    "        print(\"nl: \", nlq, index, dbname, evidence)\n",
    "        \n",
    "        order, sql_prompt, sql_query, pscore = history_log[index]['sql_log'][0]\n",
    "        order += 1\n",
    "        sql_query = clean_query(sql_query)\n",
    "        #print(\"sql: \", sql_query, pscore)\n",
    "        query.add(sql_query)\n",
    "        execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "        \n",
    "        if exception:\n",
    "            most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "            query.remove(most_recent_sql)\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                 invalidSQL=most_recent_sql, ex=exception[0])\n",
    "            sql, pscore= generation(invalid_prompt)\n",
    "            sql = clean_query(sql)\n",
    "            history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "            order += 1\n",
    "            query.add(sql)\n",
    "            execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            \n",
    "        if execution:\n",
    "            history_log[index]['num_cq_asked'] = 0\n",
    "            #print()\n",
    "            #print(\"-----execution match-----\")\n",
    "            #print()\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        for turn in range(rounds):\n",
    "            cqas = \"\"\n",
    "            if with_metadata:\n",
    "                cqas = 'user: ' + evidence + '\\n'\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"multiple choice clarification question: \"+cqs_and_answers[i]+'\\n'\n",
    "                else:\n",
    "                    cqas += \"user: \"+cqs_and_answers[i]+'\\n'\n",
    "            if cqas == \"\":\n",
    "                cqas = \"no previous clarification question.\\n\"\n",
    "            cq_prompt = SRA_ES.format(schema=dbschema, question=nlq,\\\n",
    "                                            sqls=\";\\n\".join(query), cqs=cqas)\n",
    "            cq_prompt = cq_prefix_v1 + cq_prompt\n",
    "            cq, pscore= generation(cq_prompt)\n",
    "            history_log[index]['cq_log'].append((order, cq_prompt, cq, pscore))\n",
    "            order += 1\n",
    "            if \"NO AMBIGUITY\" in cq:\n",
    "#                 print()\n",
    "#                 print(\"-----NO AMBGUITY-----\")\n",
    "#                 print()\n",
    "                break\n",
    "            if \"mul_choice_cq = \" in cq:\n",
    "                cq = cq.split(\"mul_choice_cq = \")[-1]\n",
    "#             print(\"cq: \", cq)\n",
    "            feedback_prompt = feedback_v2.format(query = gold, question = cq, nlq=nlq)\n",
    "            feedback_prompt = feedback_prefix_v1 + feedback_prompt\n",
    "            feedback, pscore= GPT4o_generation(feedback_prompt)\n",
    "\n",
    "            history_log[index]['feedback_log'].append((order, feedback_prompt, feedback, pscore))\n",
    "            order += 1\n",
    "            if \"answer_to_cq =\" in feedback:\n",
    "                feedback = feedback.split(\"answer_to_cq =\")[-1]\n",
    "#             print()\n",
    "#             print(\"feedback, \", feedback)\n",
    "            cqs_and_answers.append(cq)\n",
    "            cqs_and_answers.append(feedback)\n",
    "        \n",
    "            # fix incorrect sql based on user feedback\n",
    "            cqas = \"\"\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"multiple choice clarification question: \"+cqs_and_answers[i]+'\\n'\n",
    "                else:\n",
    "                    cqas += \"user: \"+cqs_and_answers[i]+'\\n'\n",
    "            if cqas == '':\n",
    "                cqas = 'no previous clarification questions are asked.\\n'\n",
    "            \n",
    "            # use examples from the user study\n",
    "            sql_prompt = sql_generation_feedback_few_shot_prompt.format(schema=dbschema, question=nlq,\\\n",
    "                                              sqls=\";\\n\".join(query), cqas=cqas, metadata=evidence)\n",
    "            sql_prompt = \"/* some examples are provided */\\n\" + sql_prompt\n",
    "            sql_query, pscore= generation(sql_prompt)\n",
    "            sql_query = clean_query(sql_query)\n",
    "            #print(\"sql: \", sql_query, pscore)\n",
    "            history_log[index]['sql_log'].append((order, sql_prompt, sql_query, pscore))\n",
    "            order += 1\n",
    "            sql_query = clean_query(sql_query)\n",
    "            query.add(sql_query)\n",
    "            execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "            if exception:\n",
    "                most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "                query.remove(most_recent_sql)\n",
    "                invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                     invalidSQL=most_recent_sql, ex=exception[0])\n",
    "                sql, pscore= generation(invalid_prompt)\n",
    "                sql = clean_query(sql)\n",
    "                query.add(sql)\n",
    "                history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "                order += 1\n",
    "                execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            if execution:\n",
    "                history_log[index]['num_cq_asked'] = turn + 1\n",
    "                break\n",
    "                \n",
    "        if 'num_cq_asked' not in history_log[index]:\n",
    "            history_log[index]['num_cq_asked'] = \"Failed\"\n",
    "#         print('')\n",
    "#         print(\"------next question------\")\n",
    "#         print('')\n",
    "    save(log_name, history_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52feeead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating initial SQLs for 5 samples...\n",
      "Running Method 1 (Baseline)...\n",
      "Name the most popular release on houston. 0 WhatCDHipHop \n",
      "Which albums have been downloaded more than 100 times? 1 WhatCDHipHop \n",
      "Find me top 5 most popular releases after 2000? 2 WhatCDHipHop \n",
      "what release types are captured in this data set? 3 WhatCDHipHop \n",
      "which tags exist? 4 WhatCDHipHop \n",
      "Running Method 2 (Clarification)...\n",
      "nl:  Name the most popular release on houston. 0 WhatCDHipHop \n",
      "nl:  Which albums have been downloaded more than 100 times? 1 WhatCDHipHop \n",
      "nl:  Find me top 5 most popular releases after 2000? 2 WhatCDHipHop \n",
      "nl:  what release types are captured in this data set? 3 WhatCDHipHop \n",
      "nl:  which tags exist? 4 WhatCDHipHop \n",
      "Running Method 3 (Break No Amb)...\n",
      "nl:  Name the most popular release on houston. 0 WhatCDHipHop \n",
      "nl:  Which albums have been downloaded more than 100 times? 1 WhatCDHipHop \n",
      "nl:  Find me top 5 most popular releases after 2000? 2 WhatCDHipHop \n",
      "nl:  what release types are captured in this data set? 3 WhatCDHipHop \n",
      "nl:  which tags exist? 4 WhatCDHipHop \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'visualize_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     82\u001b[39m res_m3_few = log_to_df(log_m3)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m visualize_results(pd.DataFrame(), res_m1_few, pd.DataFrame(), res_m2_few, pd.DataFrame(), res_m3_few)\n",
      "\u001b[31mNameError\u001b[39m: name 'visualize_results' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_results(res_m1_zero, res_m1_few, res_m2_zero, res_m2_few, res_m3_zero, res_m3_few, test_subset=None):\n",
    "    print(\"\\n--- Visualization of Results ---\")\n",
    "    \n",
    "    # 1. Construct unified data structure & Save to JSON\n",
    "    data_map = {\n",
    "        'M1_Zero': res_m1_zero, 'M1_Few': res_m1_few,\n",
    "        'M2_Zero': res_m2_zero, 'M2_Few': res_m2_few,\n",
    "        'M3_Zero': res_m3_zero, 'M3_Few': res_m3_few\n",
    "    }\n",
    "    \n",
    "    json_data = []\n",
    "    for key, df in data_map.items():\n",
    "        if df.empty: continue\n",
    "        method, mode = key.split('_')\n",
    "        records = df.to_dict(orient='records')\n",
    "        for r in records:\n",
    "            r['Method'] = method\n",
    "            r['Mode'] = mode\n",
    "            # Calculate status for stacked bar\n",
    "            if r['is_correct']:\n",
    "                if r.get('rounds', 0) == 0:\n",
    "                    r['Status'] = 'Initial Correct'\n",
    "                else:\n",
    "                    r['Status'] = 'Fixed Correct'\n",
    "            else:\n",
    "                r['Status'] = 'Incorrect'\n",
    "        json_data.extend(records)\n",
    "        \n",
    "    json_path = 'experiment_results.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2, default=str)\n",
    "    print(f\"Results saved to {json_path}\")\n",
    "    \n",
    "    # 2. Read from JSON for plotting (as requested)\n",
    "    print(f\"Reading results from {json_path} for plotting...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        loaded_data = json.load(f)\n",
    "    \n",
    "    full_df = pd.DataFrame(loaded_data)\n",
    "    \n",
    "    if full_df.empty:\n",
    "        print(\"No data to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Set Academic Style\n",
    "    sns.set_theme(style=\"white\")\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "    \n",
    "    # Define Palettes: M1=Blue, M2=Orange, M3=Green\n",
    "    palette_dict = {'M1': '#4E79A7', 'M2': '#F28E2B', 'M3': '#59A14F'}\n",
    "    \n",
    "    # --- Figure 1: 2x2 Grid (Accuracy & Rounds) ---\n",
    "    fig1, axes1 = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig1.suptitle('Performance Overview', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    modes = ['Zero', 'Few']\n",
    "    agg_df = full_df.groupby(['Method', 'Mode']).agg(\n",
    "        Accuracy=('is_correct', 'mean'),\n",
    "        Avg_Rounds=('rounds', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    for i, mode in enumerate(modes):\n",
    "        # Top Row: Accuracy\n",
    "        ax_acc = axes1[0, i]\n",
    "        subset = agg_df[agg_df['Mode'] == mode]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Accuracy', palette=palette_dict, ax=ax_acc)\n",
    "            ax_acc.set_title(f'Accuracy ({mode}-shot)')\n",
    "            ax_acc.set_ylim(0, 1.1)\n",
    "            ax_acc.set_ylabel('Success Rate' if i==0 else '')\n",
    "            for p in ax_acc.patches:\n",
    "                ax_acc.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontweight='bold')\n",
    "    \n",
    "        # Bottom Row: Rounds\n",
    "        ax_rds = axes1[1, i]\n",
    "        subset = agg_df[agg_df['Mode'] == mode]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Avg_Rounds', palette=palette_dict, ax=ax_rds)\n",
    "            ax_rds.set_title(f'Avg Interaction Rounds ({mode}-shot)')\n",
    "            ax_rds.set_ylabel('Rounds' if i==0 else '')\n",
    "            # Add values on top\n",
    "            max_h = 0\n",
    "            for p in ax_rds.patches:\n",
    "                h = p.get_height()\n",
    "                if h > max_h: max_h = h\n",
    "                ax_rds.annotate(f'{h:.2f}', (p.get_x() + p.get_width() / 2., h),\n",
    "                                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "            ax_rds.set_ylim(0, max_h * 1.2 if max_h > 0 else 1) \n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Figure 2: Stacked Bar Chart (Correctness Breakdown) ---\n",
    "    breakdown_list = []\n",
    "    for (method, mode), group in full_df.groupby(['Method', 'Mode']):\n",
    "        total = len(group)\n",
    "        initial = len(group[group['Status'] == 'Initial Correct'])\n",
    "        fixed = len(group[group['Status'] == 'Fixed Correct'])\n",
    "        incorrect = len(group[group['Status'] == 'Incorrect'])\n",
    "        \n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Initial Correct', 'Prop': initial/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Fixed Correct', 'Prop': fixed/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Incorrect', 'Prop': incorrect/total})\n",
    "        \n",
    "    breakdown_df = pd.DataFrame(breakdown_list)\n",
    "    status_palette = {'Initial Correct': '#1f77b4', 'Fixed Correct': '#2ca02c', 'Incorrect': '#d62728'}\n",
    "    \n",
    "    if not breakdown_df.empty:\n",
    "        fig2, axes2 = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig2.suptitle('Correctness Breakdown', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, mode in enumerate(modes):\n",
    "            ax = axes2[i]\n",
    "            subset = breakdown_df[breakdown_df['Mode'] == mode]\n",
    "            if not subset.empty:\n",
    "                pivot_df = subset.pivot(index='Method', columns='Type', values='Prop').fillna(0)\n",
    "                # Ensure all columns exist\n",
    "                for col in ['Initial Correct', 'Fixed Correct', 'Incorrect']:\n",
    "                    if col not in pivot_df.columns: pivot_df[col] = 0\n",
    "                pivot_df = pivot_df[['Initial Correct', 'Fixed Correct', 'Incorrect']]\n",
    "                \n",
    "                pivot_df.plot(kind='bar', stacked=True, color=[status_palette[c] for c in pivot_df.columns], ax=ax)\n",
    "                ax.set_title(f'Breakdown ({mode}-shot)')\n",
    "                ax.set_ylabel('Proportion' if i==0 else '')\n",
    "                ax.set_ylim(0, 1.0)\n",
    "                if i == 1:\n",
    "                    ax.legend(title='Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                else:\n",
    "                    ax.get_legend().remove()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    # --- Figure 3: Difficulty Distribution ---\n",
    "    if test_subset is not None and 'difficulty' in test_subset.columns:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        diff_counts = test_subset['difficulty'].value_counts()\n",
    "        colors_map = {'Hard': '#ff9999', 'Medium': '#ffff99', 'Simple': '#66b3ff'}\n",
    "        pie_colors = [colors_map.get(l, '#cccccc') for l in diff_counts.index]\n",
    "        \n",
    "        plt.pie(diff_counts, labels=diff_counts.index, autopct='%1.1f%%', startangle=140, colors=pie_colors)\n",
    "        plt.title('Test Sample Difficulty Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n# --- Function Definition (Inserted) ---\n",
    "source = 'kaggle'\n",
    "modelname = 'gpt4turbo' \n",
    "k_shot = 3\n",
    "with_metadata = True\n",
    "\n",
    "# Define generation function\n",
    "if modelname == 'gpt35turbo':\n",
    "    generation = GPT35_generation\n",
    "elif modelname == 'gpt4turbo':\n",
    "    generation = GPT4_turbo_generation\n",
    "else:\n",
    "    print('error')\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "def log_to_df(history_log):\n",
    "    results = []\n",
    "    for idx, data in history_log.items():\n",
    "        rounds = data.get('num_cq_asked', 'Failed')\n",
    "        is_correct = rounds != 'Failed'\n",
    "        if not is_correct:\n",
    "            rounds = 4 # max rounds assumption\n",
    "        results.append({'id': idx, 'rounds': rounds, 'is_correct': is_correct})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Helper to generate initial logs (replacing batch file reading)\n",
    "def generate_initial_logs(dataframe, limit, generation_func, k_shot, with_metadata):\n",
    "    logs = {}\n",
    "    print(f\"Generating initial SQLs for {limit} samples...\")\n",
    "    for index in range(min(limit, len(dataframe))):\n",
    "        d = dataframe.iloc[index]\n",
    "        nlq = d['nl']\n",
    "        dbname = d['target_db']\n",
    "        dbschema = d['target_schema']\n",
    "        evidence = d.get('evidence', '') if with_metadata else ''\n",
    "        \n",
    "        # Construct prompt (Simple Few-Shot or Zero-Shot)\n",
    "        # Using a basic prompt structure similar to what might be expected\n",
    "        prompt = f\"/* Given the following database schema: */\\n{dbschema}\\n\"\n",
    "        if with_metadata and evidence:\n",
    "            prompt += f\"{evidence}\\n\"\n",
    "        prompt += f\"/* Answer the following: {nlq} */\\nSELECT \"\n",
    "        \n",
    "        # Generate\n",
    "        try:\n",
    "            sql_query, pscore = generation_func(prompt)\n",
    "            sql_query = clean_query(sql_query)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating for {index}: {e}\")\n",
    "            sql_query = \"SELECT * FROM table\" # Fallback\n",
    "            pscore = 0.0\n",
    "            \n",
    "        logs[index] = {}\n",
    "        logs[index]['sql_log'] = [[0, 'initial generation', sql_query, pscore]]\n",
    "        logs[index]['cq_log'] = []\n",
    "        logs[index]['feedback_log'] = []\n",
    "    return logs\n",
    "\n",
    "# Main Execution Parameters\n",
    "limit = 5 # Set to a small number for testing/demo. Increase to len(df) for full run.\n",
    "\n",
    "# Generate Base Logs once\n",
    "base_history_log = generate_initial_logs(df, limit, generation, k_shot, with_metadata)\n",
    "\n",
    "# Method 1: Baseline\n",
    "print(\"Running Method 1 (Baseline)...\")\n",
    "log_m1 = copy.deepcopy(base_history_log)\n",
    "baselineFewShot(df, log_m1, f'{with_metadata}_{source}_{modelname}_baseline.pkl', 4, limit, modelname, userstudy_vectorstore, k_shot, source, with_metadata)\n",
    "res_m1_few = log_to_df(log_m1)\n",
    "\n",
    "# Method 2: Clarification Questions\n",
    "print(\"Running Method 2 (Clarification)...\")\n",
    "log_m2 = copy.deepcopy(base_history_log)\n",
    "askClarificationQuestions(df, log_m2, f'{with_metadata}_{source}_{modelname}_cq.pkl', 4, limit, modelname, userstudy_vectorstore, k_shot, source, with_metadata)\n",
    "res_m2_few = log_to_df(log_m2)\n",
    "\n",
    "# Method 3: Break No Ambiguity\n",
    "print(\"Running Method 3 (Break No Amb)...\")\n",
    "log_m3 = copy.deepcopy(base_history_log)\n",
    "askCQsBreakNoAmb(df, log_m3, f'{with_metadata}_{source}_{modelname}_break_cq.pkl', 4, limit, modelname, userstudy_vectorstore, k_shot, source, with_metadata)\n",
    "res_m3_few = log_to_df(log_m3)\n",
    "\n",
    "# Visualization\n",
    "visualize_results(pd.DataFrame(), res_m1_few, pd.DataFrame(), res_m2_few, pd.DataFrame(), res_m3_few)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52feeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_results(res_m1_zero, res_m1_few, res_m2_zero, res_m2_few, res_m3_zero, res_m3_few, test_subset=None):\n",
    "    print(\"\\n--- Visualization of Results ---\")\n",
    "    \n",
    "    # 1. Construct unified data structure & Save to JSON\n",
    "    data_map = {\n",
    "        'M1_Zero': res_m1_zero, 'M1_Few': res_m1_few,\n",
    "        'M2_Zero': res_m2_zero, 'M2_Few': res_m2_few,\n",
    "        'M3_Zero': res_m3_zero, 'M3_Few': res_m3_few\n",
    "    }\n",
    "    \n",
    "    json_data = []\n",
    "    for key, df in data_map.items():\n",
    "        if df.empty: continue\n",
    "        method, mode = key.split('_')\n",
    "        records = df.to_dict(orient='records')\n",
    "        for r in records:\n",
    "            r['Method'] = method\n",
    "            r['Mode'] = mode\n",
    "            # Calculate status for stacked bar\n",
    "            if r['is_correct']:\n",
    "                if r.get('rounds', 0) == 0:\n",
    "                    r['Status'] = 'Initial Correct'\n",
    "                else:\n",
    "                    r['Status'] = 'Fixed Correct'\n",
    "            else:\n",
    "                r['Status'] = 'Incorrect'\n",
    "        json_data.extend(records)\n",
    "        \n",
    "    json_path = 'experiment_results.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2, default=str)\n",
    "    print(f\"Results saved to {json_path}\")\n",
    "    \n",
    "    # 2. Read from JSON for plotting (as requested)\n",
    "    print(f\"Reading results from {json_path} for plotting...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        loaded_data = json.load(f)\n",
    "    \n",
    "    full_df = pd.DataFrame(loaded_data)\n",
    "    \n",
    "    if full_df.empty:\n",
    "        print(\"No data to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Set Academic Style\n",
    "    sns.set_theme(style=\"white\")\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "    \n",
    "    # Define Palettes: M1=Blue, M2=Orange, M3=Green\n",
    "    palette_dict = {'M1': '#4E79A7', 'M2': '#F28E2B', 'M3': '#59A14F'}\n",
    "    \n",
    "    # --- Figure 1: 2x2 Grid (Accuracy & Rounds) ---\n",
    "    fig1, axes1 = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig1.suptitle('Performance Overview', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    modes = ['Zero', 'Few']\n",
    "    agg_df = full_df.groupby(['Method', 'Mode']).agg(\n",
    "        Accuracy=('is_correct', 'mean'),\n",
    "        Avg_Rounds=('rounds', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    for i, mode in enumerate(modes):\n",
    "        # Top Row: Accuracy\n",
    "        ax_acc = axes1[0, i]\n",
    "        subset = agg_df[agg_df['Mode'] == mode]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Accuracy', palette=palette_dict, ax=ax_acc)\n",
    "            ax_acc.set_title(f'Accuracy ({mode}-shot)')\n",
    "            ax_acc.set_ylim(0, 1.1)\n",
    "            ax_acc.set_ylabel('Success Rate' if i==0 else '')\n",
    "            for p in ax_acc.patches:\n",
    "                ax_acc.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontweight='bold')\n",
    "\n",
    "        # Bottom Row: Rounds\n",
    "        ax_rds = axes1[1, i]\n",
    "        subset = agg_df[agg_df['Mode'] == mode]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Avg_Rounds', palette=palette_dict, ax=ax_rds)\n",
    "            ax_rds.set_title(f'Avg Interaction Rounds ({mode}-shot)')\n",
    "            ax_rds.set_ylabel('Rounds' if i==0 else '')\n",
    "            # Add values on top\n",
    "            max_h = 0\n",
    "            for p in ax_rds.patches:\n",
    "                h = p.get_height()\n",
    "                if h > max_h: max_h = h\n",
    "                ax_rds.annotate(f'{h:.2f}', (p.get_x() + p.get_width() / 2., h),\n",
    "                                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "            ax_rds.set_ylim(0, max_h * 1.2 if max_h > 0 else 1) \n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Figure 2: Stacked Bar Chart (Correctness Breakdown) ---\n",
    "    breakdown_list = []\n",
    "    for (method, mode), group in full_df.groupby(['Method', 'Mode']):\n",
    "        total = len(group)\n",
    "        initial = len(group[group['Status'] == 'Initial Correct'])\n",
    "        fixed = len(group[group['Status'] == 'Fixed Correct'])\n",
    "        incorrect = len(group[group['Status'] == 'Incorrect'])\n",
    "        \n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Initial Correct', 'Prop': initial/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Fixed Correct', 'Prop': fixed/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Incorrect', 'Prop': incorrect/total})\n",
    "        \n",
    "    breakdown_df = pd.DataFrame(breakdown_list)\n",
    "    status_palette = {'Initial Correct': '#2ca02c', 'Fixed Correct': '#1f77b4', 'Incorrect': '#d62728'}\n",
    "    \n",
    "    if not breakdown_df.empty:\n",
    "        fig2, axes2 = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig2.suptitle('Correctness Breakdown', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, mode in enumerate(modes):\n",
    "            ax = axes2[i]\n",
    "            subset = breakdown_df[breakdown_df['Mode'] == mode]\n",
    "            if not subset.empty:\n",
    "                pivot_df = subset.pivot(index='Method', columns='Type', values='Prop').fillna(0)\n",
    "                # Ensure all columns exist\n",
    "                for col in ['Initial Correct', 'Fixed Correct', 'Incorrect']:\n",
    "                    if col not in pivot_df.columns: pivot_df[col] = 0\n",
    "                pivot_df = pivot_df[['Initial Correct', 'Fixed Correct', 'Incorrect']]\n",
    "                \n",
    "                pivot_df.plot(kind='bar', stacked=True, color=[status_palette[c] for c in pivot_df.columns], ax=ax)\n",
    "                ax.set_title(f'Breakdown ({mode}-shot)')\n",
    "                ax.set_ylabel('Proportion' if i==0 else '')\n",
    "                ax.set_ylim(0, 1.0)\n",
    "                if i == 1:\n",
    "                    ax.legend(title='Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                else:\n",
    "                    ax.get_legend().remove()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    # --- Figure 3: Difficulty Distribution ---\n",
    "    if test_subset is not None and 'difficulty' in test_subset.columns:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        diff_counts = test_subset['difficulty'].value_counts()\n",
    "        colors_map = {'Hard': '#ff9999', 'Medium': '#ffff99', 'Simple': '#66b3ff'}\n",
    "        pie_colors = [colors_map.get(l, '#cccccc') for l in diff_counts.index]\n",
    "        \n",
    "        plt.pie(diff_counts, labels=diff_counts.index, autopct='%1.1f%%', startangle=140, colors=pie_colors)\n",
    "        plt.title('Test Sample Difficulty Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "# visualize_results(res_m1_zero, res_m1_few, res_m2_zero, res_m2_few, res_m3_zero, res_m3_few, test_subset=test_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_results(res_m1_zero, res_m1_few, res_m2_zero, res_m2_few, res_m3_zero, res_m3_few, test_subset=None):\n",
    "    print(\"\\n--- Visualization of Results ---\")\n",
    "    \n",
    "    # 1. Construct unified data structure & Save to JSON\n",
    "    data_map = {\n",
    "        'M1_Zero': res_m1_zero, 'M1_Few': res_m1_few,\n",
    "        'M2_Zero': res_m2_zero, 'M2_Few': res_m2_few,\n",
    "        'M3_Zero': res_m3_zero, 'M3_Few': res_m3_few\n",
    "    }\n",
    "    \n",
    "    json_data = []\n",
    "    for key, df in data_map.items():\n",
    "        if df.empty: continue\n",
    "        method, mode = key.split('_')\n",
    "        records = df.to_dict(orient='records')\n",
    "        for r in records:\n",
    "            r['Method'] = method\n",
    "            r['Mode'] = mode\n",
    "            # Calculate status for stacked bar\n",
    "            if r['is_correct']:\n",
    "                if r.get('rounds', 0) == 0:\n",
    "                    r['Status'] = 'Initial Correct'\n",
    "                else:\n",
    "                    r['Status'] = 'Fixed Correct'\n",
    "            else:\n",
    "                r['Status'] = 'Incorrect'\n",
    "        json_data.extend(records)\n",
    "        \n",
    "    json_path = 'experiment_results.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2, default=str)\n",
    "    print(f\"Results saved to {json_path}\")\n",
    "    \n",
    "    # 2. Read from JSON for plotting (as requested)\n",
    "    print(f\"Reading results from {json_path} for plotting...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        loaded_data = json.load(f)\n",
    "    \n",
    "    full_df = pd.DataFrame(loaded_data)\n",
    "    \n",
    "    if full_df.empty:\n",
    "        print(\"No data to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Set Academic Style\n",
    "    sns.set_theme(style=\"white\")\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "    \n",
    "    # Define Palettes: M1=Blue, M2=Orange, M3=Green\n",
    "    palette_dict = {'M1': '#4E79A7', 'M2': '#F28E2B', 'M3': '#59A14F'}\n",
    "    \n",
    "    # --- Figure 1: 2x2 Grid (Accuracy & Rounds) ---\n",
    "    fig1, axes1 = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig1.suptitle('Performance Overview', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    modes = ['Zero', 'Few']\n",
    "    agg_df = full_df.groupby(['Method', 'Mode']).agg(\n",
    "        Accuracy=('is_correct', 'mean'),\n",
    "        Avg_Rounds=('rounds', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    for i, mode in enumerate(modes):\n",
    "        # Top Row: Accuracy\n",
    "        ax_acc = axes1[0, i]\n",
    "        subset = agg_df[agg_df['Mode'] == mode]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Accuracy', palette=palette_dict, ax=ax_acc)\n",
    "            ax_acc.set_title(f'Accuracy ({mode}-shot)')\n",
    "            ax_acc.set_ylim(0, 1.1)\n",
    "            ax_acc.set_ylabel('Success Rate' if i==0 else '')\n",
    "            for p in ax_acc.patches:\n",
    "                ax_acc.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontweight='bold')\n",
    "\n",
    "        # Bottom Row: Rounds\n",
    "        ax_rds = axes1[1, i]\n",
    "        subset = agg_df[agg_df['Mode'] == mode]\n",
    "        if not subset.empty:\n",
    "            sns.barplot(data=subset, x='Method', y='Avg_Rounds', palette=palette_dict, ax=ax_rds)\n",
    "            ax_rds.set_title(f'Avg Interaction Rounds ({mode}-shot)')\n",
    "            ax_rds.set_ylabel('Rounds' if i==0 else '')\n",
    "            # Add values on top\n",
    "            max_h = 0\n",
    "            for p in ax_rds.patches:\n",
    "                h = p.get_height()\n",
    "                if h > max_h: max_h = h\n",
    "                ax_rds.annotate(f'{h:.2f}', (p.get_x() + p.get_width() / 2., h),\n",
    "                                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "            ax_rds.set_ylim(0, max_h * 1.2 if max_h > 0 else 1) \n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Figure 2: Stacked Bar Chart (Correctness Breakdown) ---\n",
    "    breakdown_list = []\n",
    "    for (method, mode), group in full_df.groupby(['Method', 'Mode']):\n",
    "        total = len(group)\n",
    "        initial = len(group[group['Status'] == 'Initial Correct'])\n",
    "        fixed = len(group[group['Status'] == 'Fixed Correct'])\n",
    "        incorrect = len(group[group['Status'] == 'Incorrect'])\n",
    "        \n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Initial Correct', 'Prop': initial/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Fixed Correct', 'Prop': fixed/total})\n",
    "        breakdown_list.append({'Method': method, 'Mode': mode, 'Type': 'Incorrect', 'Prop': incorrect/total})\n",
    "        \n",
    "    breakdown_df = pd.DataFrame(breakdown_list)\n",
    "    status_palette = {'Initial Correct': '#1f77b4', 'Fixed Correct': '#2ca02c', 'Incorrect': '#d62728'}\n",
    "    \n",
    "    if not breakdown_df.empty:\n",
    "        fig2, axes2 = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig2.suptitle('Correctness Breakdown', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, mode in enumerate(modes):\n",
    "            ax = axes2[i]\n",
    "            subset = breakdown_df[breakdown_df['Mode'] == mode]\n",
    "            if not subset.empty:\n",
    "                pivot_df = subset.pivot(index='Method', columns='Type', values='Prop').fillna(0)\n",
    "                # Ensure all columns exist\n",
    "                for col in ['Initial Correct', 'Fixed Correct', 'Incorrect']:\n",
    "                    if col not in pivot_df.columns: pivot_df[col] = 0\n",
    "                pivot_df = pivot_df[['Initial Correct', 'Fixed Correct', 'Incorrect']]\n",
    "                \n",
    "                pivot_df.plot(kind='bar', stacked=True, color=[status_palette[c] for c in pivot_df.columns], ax=ax)\n",
    "                ax.set_title(f'Breakdown ({mode}-shot)')\n",
    "                ax.set_ylabel('Proportion' if i==0 else '')\n",
    "                ax.set_ylim(0, 1.0)\n",
    "                if i == 1:\n",
    "                    ax.legend(title='Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                else:\n",
    "                    ax.get_legend().remove()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    # --- Figure 3: Difficulty Distribution ---\n",
    "    if test_subset is not None and 'difficulty' in test_subset.columns:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        diff_counts = test_subset['difficulty'].value_counts()\n",
    "        colors_map = {'Hard': '#ff9999', 'Medium': '#ffff99', 'Simple': '#66b3ff'}\n",
    "        pie_colors = [colors_map.get(l, '#cccccc') for l in diff_counts.index]\n",
    "        \n",
    "        plt.pie(diff_counts, labels=diff_counts.index, autopct='%1.1f%%', startangle=140, colors=pie_colors)\n",
    "        plt.title('Test Sample Difficulty Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02691d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "visualize_results(res_m1_zero, res_m1_few, res_m2_zero, res_m2_few, res_m3_zero, res_m3_few, test_subset=test_subset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}