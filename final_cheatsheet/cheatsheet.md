# SDSC 5001: 统计机器学习I 知识点详细解析
## 1. Review of Probability and Statistics（概率与统计回顾）
### 1.1 Population and Sample（总体与样本）
- **核心定义**：总体是我们想要得出结论的全部个体集合（比如“全国所有大学生”），样本是总体中被观察的部分（比如“随机抽取的200名大学生”）。推断统计是借助概率论，基于样本来推导总体特征的过程。
- **关键逻辑**：现实中无法对总体全部观测（如全国大学生的成绩），通过样本的代表性（随机抽样保证），用样本统计量（如样本均值）估计总体参数（如总体均值）。
- **案例深化**：要研究某城市居民月收入，总体是该城市所有居民，样本是随机选取的500位居民。通过这500人的月收入计算均值、方差，进而推断全市居民的收入水平，这就是推断统计的应用。

### 1.2 Basics of Probability（概率基础）
- **核心定义**：样本空间（S）是实验所有可能结果的集合（如掷骰子的结果{1,2,3,4,5,6}）；事件是样本空间的子集（如“掷出偶数”={2,4,6}）。
- **集合运算**：并集（A∪B）表示A或B发生，交集（A∩B）表示A且B发生，补集（A'）表示A不发生。
- **计数技巧**：乘法原理（分步完成事件的总结果数，如从3件上衣和2条裤子中选穿搭，共3×2=6种）；排列（有序选取，如从5人中选3人排座位，P₅³=5!/(5-3)!=60）；组合（无序选取，如从5人中选3人组队，C₅³=5!/(3!×2!)=10）。
- **案例深化**：抽奖活动中，从10张奖券（3张中奖）中选2张，“至少1张中奖”的事件计算：总组合C₁₀²=45，对立事件“都不中奖”C₇²=21，故概率=1-21/45=24/45=8/15。

### 1.3 Conditional Probability and Bayes’ Theorem（条件概率与贝叶斯定理）
- **条件概率**：P(A|B)表示B发生的前提下A发生的概率，公式为P(A∩B)/P(B)（B的概率不能为0）。
- **独立性**：若A和B独立，说明B的发生不影响A的概率，即P(A|B)=P(A)，等价于P(A∩B)=P(A)P(B)（如掷硬币两次，第一次正面和第二次正面独立）。
- **贝叶斯定理**：核心是“逆概率”计算，将先验概率（P(Aᵢ)，已知的初始概率）通过似然（P(B|Aᵢ)，Aᵢ发生时B的概率）更新为后验概率（P(Aᵢ|B)，B发生后Aᵢ的概率），分母是边际似然P(B)=∑P(Aₖ)P(B|Aₖ)（全概率公式）。
- **案例深化**：医学诊断中，某病患病率P(患病)=0.02，检测灵敏度P(阳性|患病)=0.95，特异性P(阴性|未患病)=0.9（即P(阳性|未患病)=0.1）。若某人检测阳性，实际患病概率：P(患病|阳性)=（0.02×0.95）/（0.02×0.95+0.98×0.1）≈0.16，说明阳性结果不代表一定患病，需结合先验概率。

### 1.4 Random Variables（随机变量）
- **离散随机变量**：取值为离散值（如“掷骰子的点数”“中奖次数”），用概率质量函数（PMF）p(x)=P(X=x)描述概率分布，期望E(X)=∑x·p(x)（加权平均，权重为概率）。常见类型：伯努利（单次试验，成功/失败，如“一次投篮命中”）、二项（n次伯努利试验的成功次数）、泊松（稀有事件发生次数，如“一天内事故数”）。
- **连续随机变量**：取值为连续区间（如“身高”“时间”），用概率密度函数（PDF）f(x)描述，P(a≤X≤b)=∫ₐᵇf(x)dx（区间下面积），期望E(X)=∫x·f(x)dx。常见类型：均匀（区间内概率均等，如“0-10分钟内的随机到达时间”）、指数（描述事件间隔，如“设备故障间隔”）、正态（高斯分布，自然界大量现象遵循，如“人群身高”）。
- **方差**：衡量随机变量取值的离散程度，公式Var(X)=E[X-E(X)]²=E(X²)-[E(X)]²（偏差平方的期望）。
- **案例深化**：伯努利随机变量X（成功概率p=0.6），PMF为P(X=1)=0.6，P(X=0)=0.4，期望E(X)=0.6，方差Var(X)=0.6×0.4=0.24；正态分布N(μ=170, σ²=25)（身高，单位cm），P(165≤X≤175)=∫₁₆₅¹⁷⁵f(x)dx≈0.6827（68-95-99.7法则：±1σ区间概率68%）。

### 1.5 Statistical Inference and CLT（统计推断与中心极限定理）
- **随机样本**：X₁,X₂,...,Xₙ独立同分布（i.i.d.），即样本间无关联且遵循同一分布（如随机抽取的n名学生的成绩）。
- **统计量**：仅依赖样本数据的函数，不含未知参数（如样本均值$\bar{X}$=(X₁+...+Xₙ)/n、样本方差S²=∑(Xᵢ-$\bar{X}$)²/(n-1)）。
- **中心极限定理（CLT）**：核心是“大量独立同分布样本的均值近似正态分布”，无论总体分布如何，当n→∞时，$\sqrt{n}(\bar{X}-\mu)/\sigma$→N(0,1)（标准正态分布）。这是推断统计的基础，允许用正态分布近似样本均值的分布。
- **点估计**：用样本统计量估计总体参数（如$\bar{X}$估计μ）；无偏估计量满足E($\hat{\theta}$)=θ（如样本均值是μ的无偏估计，样本方差S²是σ²的无偏估计）；MVUE（最小方差无偏估计）是无偏估计中方差最小的，精度最高。
- **案例深化**：总体为均匀分布U(0,4)（μ=2，σ²=4/3），抽取n=100样本，样本均值$\bar{X}$≈N(2, (4/3)/100)=N(2, 0.0133)，P(1.9≤$\bar{X}$≤2.1)=Φ((2.1-2)/0.115) - Φ((1.9-2)/0.115)≈0.659，说明大样本下均值的分布集中在总体均值附近。

### 1.6 Estimation Methods（估计方法）
- **矩估计（MM）**：利用“总体矩=样本矩”求解未知参数，k阶总体矩E(Xᵏ)，样本矩(1/n)∑Xᵢᵏ（如一阶矩：E(X)=(1/n)∑Xᵢ，即$\bar{X}$估计μ）。
- **最大似然估计（MLE）**：找到使观测数据“最可能发生”的参数值，即最大化似然函数L(θ)=∏f(xᵢ;θ)（联合概率密度/质量函数）。由于乘积计算复杂，常取对数似然logL(θ)=∑logf(xᵢ;θ)，求导找极值。
- **案例深化**：泊松分布P(X=k)=λᵏe⁻ᵏ/k!（λ>0），样本X₁,...,Xₙ，矩估计：E(X)=λ=(1/n)∑Xᵢ=$\bar{X}$；MLE：logL(θ)=∑(xᵢlogλ - λ - logxᵢ!)，对λ求导并令导数为0，得$\hat{\lambda}$=$\bar{X}$，与矩估计一致；正态分布N(μ,σ²)，MLE：$\hat{\mu}$=$\bar{X}$，$\hat{\sigma}^2$=(1/n)∑(Xᵢ-$\bar{X}$)²（注意样本方差S²是无偏的，MLE的σ²有偏，n替换为n-1即可修正）。

### 1.7 Confidence Intervals and Hypothesis Testing（置信区间与假设检验）
- **置信区间（CI）**：用样本统计量构造区间，估计总体参数的可能范围，公式为“点估计±边际误差”。边际误差反映估计的精度（与样本量n、置信水平α、总体标准差σ相关），如正态总体σ已知时，μ的95%置信区间为$\bar{X}$±z₀.₀₂₅·σ/√n（z₀.₀₂₅=1.96，是标准正态分布97.5%分位数）。
- **假设检验**：先提出关于总体参数的假设（原假设H₀：默认成立的结论，如“μ=μ₀”；备择假设Hₐ：与H₀对立，如“μ≠μ₀”），通过样本数据判断是否拒绝H₀。
- **两类错误**：Type I错误（α）：H₀为真时拒绝H₀（“假阳性”，如无罪判有罪）；Type II错误（β）：H₀为假时未拒绝H₀（“假阴性”，如有罪判无罪），1-β为检验效能（正确拒绝H₀的概率）。
- **P值**：在H₀成立的前提下，观测到当前样本数据（或更极端数据）的概率。若P值<α（显著性水平，常用0.05），则拒绝H₀，认为结果“统计显著”。
- **案例深化**：某产品平均寿命μ，已知σ=5，样本n=25，$\bar{X}$=22，95%置信区间：22±1.96×5/5=22±1.96，即(20.04,23.96)，说明有95%把握认为总体平均寿命在该区间内；检验H₀：μ=20，Hₐ：μ≠20，计算z=(22-20)/(5/5)=2，P值=2×(1-Φ(2))=0.0455<0.05，拒绝H₀，认为平均寿命不等于20。

## 2. Exploring Data（数据探索）
### 2.1 What is Data（数据定义）
- **核心概念**：数据是数据对象及其属性的集合。数据对象（如“一名学生”“一条交易记录”）又称记录、样本；属性（如“学生的成绩”“交易的金额”）又称变量、特征。
- **属性类型**：
  - 名义型（Nominal）：无固有顺序的分类变量（如眼睛颜色、性别、地区），不能比较大小。
  - 有序型（Ordinal）：有明确顺序的分类变量（如排名、满意度等级A/B/C、教育程度小学/中学/大学），可比较顺序但不能计算差异。
  - 区间型（Interval）：数值型变量，差异有意义但无绝对零点（如温度℃、日期），可计算“20℃-10℃=10℃”，但不能说“20℃是10℃的2倍”。
  - 连续型（Continuous）：数值型变量，有绝对零点且可测量任意精度（如身高、体重、收入），可比较倍数和差异。
- **案例深化**：数据集“电商用户信息”中，属性包括：用户ID（名义型）、年龄（连续型）、会员等级（有序型：普通/黄金/钻石）、注册日期（区间型）、消费金额（连续型），不同类型属性的分析方法不同（如名义型用频数统计，连续型用均值/标准差）。

### 2.2 Types of Data（数据类型）
- **数据矩阵（Data Matrix）**：最常用格式，n行（n个对象）p列（p个属性），每个元素表示第i个对象的第j个属性值（如n=100名学生，p=3个属性：成绩、身高、体重，构成100×3矩阵），可视为p维空间中的n个点。
- **文本数据（Text Data）**：以文档为对象，属性为词汇，常用“词袋模型（Bag-of-Words）”表示，即统计每个词汇在文档中出现的频率（如文档“我爱机器学习，机器学习很有趣”→词频向量：[我:1, 爱:1, 机器学习:2, 很:1, 有趣:1]），忽略语法和语序。
- **交易数据（Transaction Data）**：每条记录是一个“物品集合”，描述一次交易中购买的所有物品（如超市购物记录：{面包、牛奶、鸡蛋}），常用于关联规则挖掘（如“购买面包的人80%会购买牛奶”）。
- **图数据（Graph Data）**：由节点（Nodes，代表实体，如用户、网页）和边（Edges，代表实体间关系，如好友关系、网页链接）组成，如社交网络（用户=节点，好友=边）、分子结构（原子=节点，化学键=边）。
- **案例深化**：外卖平台数据中，订单信息可分为：交易数据（订单ID、购买菜品集合{汉堡、可乐、薯条}）、数据矩阵（订单ID、用户ID、消费金额、配送时间）、图数据（用户-商家的下单关系、用户间的推荐关系）。

### 2.3 Data Quality（数据质量）
- **核心问题**：高质量数据是分析的基础，常见问题包括：
  - 噪声（Noise）：数据中的随机误差或异常波动（如测量身高时的仪器误差、输入数据时的手抖误输），会干扰真实信号。
  - 异常值（Outliers）：与其他数据特征显著不同的对象（如“学生成绩分布[60,65,70,95,80]中的95”“收入数据中的百万富翁”），可能是真实值（如尖子生）或错误值（如输错数字）。
  - 缺失值（Missing Values）：未收集或不适用的信息（如用户未填写年龄、问卷漏答），处理方式：删除含缺失值的记录（适用于缺失少的情况）、填充（如用均值/中位数/众数填充连续型/有序型/名义型数据）、保留缺失值并使用支持部分数据的模型（如决策树）。
  - 抽样偏差（Sampling Bias）：样本不能代表总体导致的偏差（如调查“全国网民偏好”却只抽样大学生、预测“普通用户消费”却用VIP用户数据），会使分析结果失真。
- **案例深化**：某电商用户满意度调查中，数据存在：噪声（用户评分时误点5分实际想打3分）、异常值（1名用户打0分，其余均为3-5分，经核实是恶意评分）、缺失值（20%用户未填年龄，用样本中位数30填充）、抽样偏差（仅调查APP端用户，忽略网页端用户，需补充网页端样本）。

### 2.4 Data Exploration（数据探索概述）
- **定义与目的**：探索性数据分析（EDA）是对数据的初步探索，无需预设假设，核心目的是理解数据特征（如分布、趋势、关联），为后续分析选择合适工具（如线性模型vs非线性模型），并利用人类的模式识别能力发现潜在规律。
- **关键技术**：
  - 描述统计（Summary Statistics）：用数值总结数据属性，包括：频数与众数（名义型/有序型数据，如“性别频数：男50人、女50人，众数为男”）、位置统计量（均值、中位数、百分位数，描述数据中心位置，如“成绩均值75，中位数73，25%分位数65”）、离散统计量（范围、方差、标准差，描述数据分散程度，如“收入范围2000-10000，标准差1500”）、偏度（Skewness，描述分布不对称性：正偏（右偏，均值>中位数，如收入分布）、负偏（左偏，均值<中位数，如考试成绩分布）、无偏（对称，如正态分布））。
  - 可视化（Visualization）：将数据转化为图形，直观呈现特征（如直方图、箱线图）。
- **案例深化**：分析某班级数学成绩（n=50），描述统计：均值72，中位数70，标准差8，偏度0.3（轻微正偏），25%分位数65，75%分位数78；可视化用直方图发现成绩集中在60-80分，箱线图未发现异常值，说明成绩分布较均匀，可后续用线性模型分析成绩与其他因素的关系。

### 2.5 Visualization Techniques（可视化技术）
- **基础可视化**：
  - 直方图（Histogram）：将连续变量的取值区间划分为 bins，用矩形高度表示每个 bin 的频数/频率，展示单变量的分布（如“身高直方图”呈现不同身高区间的人数）。
  - 箱线图（Boxplot）：展示五数概括（最小值、25%分位数、中位数、75%分位数、最大值），箱体上下沿为25%和75%分位数（四分位距IQR）， whiskers 通常延伸至IQR×1.5范围内的最值，超出部分为异常值（如“成绩箱线图”可快速识别高分/低分异常值）。
  - 散点图（Scatter Plot）：以x轴和y轴分别表示两个变量，每个点代表一个对象，展示变量间的关系（如“学习时间vs成绩”散点图，若点呈上升趋势则为正相关）。
- **高级可视化**：
  - 矩阵图（Matrix Plot）：展示全数据矩阵或相似度矩阵（如“10个变量的相关性矩阵图”，用颜色深浅表示相关系数大小）。
  - 平行坐标图（Parallel Coordinates）：将p维变量映射到p条平行轴，每个对象用一条折线连接各轴上的取值，适用于高维数据（如“5个特征的客户数据”，通过折线走势分组客户）。
  - 星图（Star Plots）与切尔诺夫脸（Chernoff Faces）：将每个变量映射到几何形状（星图的射线长度）或面部特征（切尔诺夫脸的眼睛大小、嘴巴宽度），通过形状/面部差异识别数据模式（如“多特征产品数据”，用星图对比不同产品的特征强弱）。
- **案例深化**：分析汽车数据（变量：价格、油耗、马力、重量），直方图展示“价格分布右偏”（低价车多）；箱线图发现“马力有2个异常值（超跑）”；散点图显示“重量与油耗正相关”（车越重油耗越高）；平行坐标图将4个变量设为平行轴，可快速识别“低价、低油耗、低马力、轻重量”的经济型车组。

## 3. Statistical Machine Learning（统计机器学习）
### 3.1 Terminologies: Statistics vs. Machine Learning（术语对比：统计与机器学习）
- **核心对应关系**：两个领域目标相近（从数据中学习规律），但术语不同，核心对应如下：
  | 统计学（Statistics） | 机器学习（Machine Learning） |
  | --- | --- |
  | 分类/回归（Classification/Regression） | 监督学习（Supervised Learning） |
  | 聚类（Clustering） | 无监督学习（Unsupervised Learning） |
  | 含缺失响应的分类/回归 | 半监督学习（Semisupervised Learning） |
  | （非线性）降维 | 流形学习（Manifold Learning） |
  | 协变量/响应变量（Covariates/Responses） | 特征/输出（Features/Outcomes） |
  | 样本/总体（Sample/Population） | 训练集/测试集（Training set/Testing set） |
  | 统计模型（Statistical model） | 学习器（Learner） |
  | 误分类/预测误差（Misclassification/Prediction error） | 泛化误差（Generalization error） |
- **关键差异**：统计学更侧重“推断”（解释变量间关系、估计参数），机器学习更侧重“预测”（提高新数据的预测准确率），但边界逐渐融合（如统计机器学习同时关注推断和预测）。
- **案例深化**：“分析广告投入与销售额的关系”，统计学视角：构建回归模型，估计广告投入的系数（推断“每增加1元广告投入，销售额增加多少”）；机器学习视角：训练监督学习模型（回归任务），用测试集评估预测准确率（泛化误差），优化模型以精准预测新广告投入对应的销售额。

### 3.2 Basic Problem Formulation（基本问题表述）
- **训练样本**：给定n个观测样本{(x₁,y₁),(x₂,y₂),...,(xₙ,yₙ)}，其中xᵢ∈ℝᵖ是p维特征向量（输入、预测变量，如“广告投入、产品价格”），yᵢ是输出变量（响应、因变量，如“销售额”），yᵢ可为标量（如连续值销售额、离散类标签）或向量（多输出任务）。
- **基本假设**：输出变量Y与特征X的关系可表示为Y=f(X)+ε，其中f是未知的“系统函数”（捕捉X和Y的确定性关系），ε是随机误差（不可预测的噪声），满足E(ε)=0（误差均值为0）且ε与X独立（误差不依赖于特征）。
- **核心目标**：从训练样本中学习f的近似函数$\hat{f}$（学习器），用于预测新特征x对应的y（预测任务）或解释f的结构（推断任务）。
- **案例深化**：预测房屋价格，训练样本是“1000套房屋的面积（x₁）、卧室数（x₂）、地段（x₃）→价格（y）”，假设Y=f(面积,卧室数,地段)+ε，f表示房屋特征与价格的内在关系（如面积越大价格越高），ε是测量误差、随机因素（如买家偏好），学习$\hat{f}$就是找到“最贴合样本数据的价格预测函数”。

### 3.3 Two Purposes: Prediction vs. Inference（两大目的：预测与推断）
- **预测（Prediction）**：核心是用学习到的$\hat{f}$，对新特征X预测输出$\hat{Y}=\hat{f}(X)$，关注预测准确性。
- **预测误差分解**：预测误差E[(Y-$\hat{Y}$)²]（均方误差）可拆分为“可还原误差”E[(f(X)-$\hat{f}(X)$)²]（$\hat{f}$与真实f的差异，可通过优化模型减小）和“不可还原误差”Var(ε)（噪声的方差，由数据本身决定，无法减小）。
- **推断（Inference）**：核心是理解f的结构，回答“Y如何受X影响”，如“哪些特征与Y相关”（如“广告投入是否影响销售额”）、“X与Y的关系形式”（如“线性相关还是非线性相关”）、“特征的重要性排序”（如“广告投入和产品价格，哪个对销售额影响更大”）。
- **案例深化**：“信用卡欺诈检测”，预测任务：训练分类模型（$\hat{f}$），输入新交易的特征（金额、交易地点、时间），预测该交易是否为欺诈（$\hat{Y}$=欺诈/正常），优化目标是降低泛化误差；推断任务：分析模型$\hat{f}$，确定“哪些特征是欺诈的关键指标”（如“单笔金额>1万元且异地交易，欺诈概率高”），为风控策略提供解释（如限制异地大额交易）。

### 3.4 Supervised Learning: Regression（监督学习：回归）
- **任务定义**：监督学习中，若输出变量Y是连续值（如销售额、价格、温度），则为回归任务，x∈ℝᵖ，Y∈ℝ。
- **精度衡量**：用均方误差（MSE）评估模型性能，MSE(f)=E[(Y-f(X))²]，表示预测值与真实值的平均平方差，MSE越小，模型拟合效果越好。
- **最优预测函数**：理论上，最小化MSE(f)的最优函数是条件期望f*(X)=E(Y|X)（给定X时Y的平均取值），称为“回归函数”。
- **实际近似**：由于Y和X的联合分布未知，无法直接计算E(Y|X)，需用训练样本近似MSE(f)，学习$\hat{f}$逼近f*。
- **案例深化**：预测股票收盘价（连续值），特征是“前一日收盘价、成交量、大盘指数”，回归任务的MSE计算为“所有测试样本（真实收盘价-预测收盘价）²的平均值”；最优预测函数f*(X)是“给定前一日特征X时，股票收盘价的平均水平”，训练模型就是通过样本数据估计这个平均水平。

### 3.5 Supervised Learning: Classification（监督学习：分类）
- **任务定义**：监督学习中，若输出变量Y是离散类标签（如“欺诈/正常”“猫/狗/鸟”），则为分类任务，Y∈{1,2,...,K}（K为类别数，K=2是二分类，K>2是多分类），x∈ℝᵖ。
- **模型与决策函数**：模型输出每个类别的概率P(Y=k|X)=fₖ(X)（k=1,...,K），决策函数$\hat{\phi}(X)=argmax_k\hat{f}_k(X)$（选择概率最大的类别作为预测结果）。
- **精度衡量**：用误分类误差（MCE）评估，MCE(f)=E[I(Y≠f(X))]，其中I(·)是指示函数（条件成立为1，否则为0），表示预测错误的平均概率。
- **最优分类器**：理论上，最小化MCE(f)的最优分类器是贝叶斯分类器f*(X)=argmax_kP(Y=k|X)（选择后验概率最大的类别），对应的误分类误差是贝叶斯误差率（最低可能误差，由数据分布决定）。
- **案例深化**：图像分类（猫/狗二分类），特征是图像的像素值（x∈ℝ¹⁰²⁴，1024维），模型输出P(Y=猫|X)=0.7，P(Y=狗|X)=0.3，决策函数选择“猫”作为预测结果；贝叶斯分类器是“给定图像特征时，概率最高的类别”，若数据中猫和狗的特征有重叠，贝叶斯误差率可能为5%（即使最优分类器也有5%的误分类概率）。

### 3.6 Training Error vs. Test Error（训练误差与测试误差）
- **训练误差**：在训练样本上计算的误差，反映模型对训练数据的拟合程度。
  - 回归任务：训练MSE=(1/n)∑(yᵢ-$\hat{f}(xᵢ)$)²（n个训练样本的平均平方误差）。
  - 分类任务：训练误分类率=(1/n)∑I(yᵢ≠$\hat{f}(xᵢ)$)（训练样本中预测错误的比例）。
  - 特点：训练误差通常低估真实误差（模型过度拟合训练数据时，训练误差很小但泛化能力差）。
- **测试误差**：在独立于训练集的测试样本{(x₀₁,y₀₁),...,(x₀ₘ,y₀ₘ)}上计算的误差，更接近模型的真实泛化误差（对新数据的预测能力）。
  - 回归任务：测试MSE=(1/m)∑(y₀ᵢ-$\hat{f}(x₀ᵢ)$)²。
  - 分类任务：测试误分类率=(1/m)∑I(y₀ᵢ≠$\hat{f}(x₀ₘ)$)。
- **核心逻辑**：训练误差用于优化模型参数，测试误差用于评估模型性能，避免“训练集上表现好但测试集上表现差”的过拟合问题。
- **案例深化**：用线性回归预测房价，训练集（800套房屋）的训练MSE=100（万元²），测试集（200套房屋）的测试MSE=300（万元²），说明模型在训练集上拟合较好，但对新房屋的预测误差较大，存在过拟合（如模型学习了训练集中的噪声，而非真实规律）。

### 3.7 Bias-Variance Tradeoff（偏差-方差权衡）
- **误差分解**：测试误差（均方误差）可拆分为三部分：E[(Y-$\hat{f}(X)$)²]=(Bias($\hat{f}(X)$))² + Var($\hat{f}(X)$) + Var(ε)，其中：
  - 偏差（Bias）：Bias($\hat{f}(X)$)=E[$\hat{f}(X)$]-f(X)，反映$\hat{f}$对真实f的系统性偏离（模型简化导致的误差，如用线性模型拟合非线性关系）。
  - 方差（Variance）：Var($\hat{f}(X)$)=E[($\hat{f}(X)$-E[$\hat{f}(X)$])²]，反映$\hat{f}$随训练集变化的波动程度（模型复杂导致的误差，如复杂决策树在不同训练集上差异大）。
  - 不可还原误差（Var(ε)）：噪声方差，固定不变。
- **权衡关系**：模型复杂度与偏差、方差呈反向关系：
  - 低复杂度模型（如线性回归）：偏差大（难以拟合复杂关系），方差小（训练集变化对模型影响小）→欠拟合（测试误差高，因偏差大）。
  - 高复杂度模型（如深度决策树）：偏差小（能拟合复杂关系），方差大（训练集变化对模型影响大）→过拟合（测试误差高，因方差大）。
  - 最优模型：找到偏差和方差的平衡点，使测试误差最小。
- **案例深化**：拟合“二次函数Y=X²+ε”：
  - 线性模型（$\hat{f}(X)=\beta_0+\beta_1X$）：复杂度低，偏差大（无法捕捉二次关系），方差小，测试误差高（欠拟合）。
  - 10次多项式模型（$\hat{f}(X)=\beta_0+\beta_1X+...+\beta_{10}X^{10}$）：复杂度高，偏差小（能拟合二次关系），方差大（训练集微小变化导致多项式系数大幅波动），测试误差高（过拟合）。
  - 二次多项式模型（$\hat{f}(X)=\beta_0+\beta_1X+\beta_2X²$）：复杂度适中，偏差和方差平衡，测试误差最小（最优）。

### 3.8 Cross Validation (CV)（交叉验证）
- **核心目的**：当缺乏足够大的独立测试集时，用交叉验证估计模型的泛化误差（测试误差），同时避免浪费训练数据（如直接拆分训练/测试集会减少训练样本量）。
- **常见方法**：
  - 留一交叉验证（LOOCV）：将n个训练样本依次分为“n-1个训练集+1个验证集”，共进行n次训练和验证，平均n次验证误差作为泛化误差估计。优点：无随机拆分，结果稳定，偏差小；缺点：计算量大（n次训练），适用于小样本。
  - K折交叉验证（K-Fold CV）：将训练集随机分为K个大小相近的折（Fold），依次用K-1个折作为训练集，1个折作为验证集，共K次训练和验证，平均K次验证误差作为泛化误差估计。常用K=5或10（兼顾计算量和稳定性）。优点：计算效率高于LOOCV，结果稳定；缺点：存在随机拆分的微小波动（可通过多次重复取平均缓解）。
- **误差计算**：K折CV误差CV($\hat{f}$)=(1/n)∑Eₖ($\hat{f}$)，其中Eₖ是第k个验证折的误差（如回归任务的平方误差和）。
- **案例深化**：用5折CV评估线性回归模型的泛化误差，训练集n=1000，分为5个折（每折200个样本）：第1次用折1-4（800样本）训练，折5（200样本）验证，计算验证MSE；重复5次后，平均5个验证MSE，得到CV误差，作为模型泛化误差的估计（无需额外测试集）。

## 4. Linear Regression（线性回归）
### 4.1 Linear Regression Model（线性回归模型）
- **模型形式**：
  - 简单线性回归（单特征）：f(x)=β₀+β₁x，其中β₀是截距（x=0时的f(x)值），β₁是斜率（x每增加1单位，f(x)的变化量），如“销售额=100+2×广告投入”（β₀=100，β₁=2）。
  - 多元线性回归（多特征）：yᵢ=β₀+β₁xᵢ₁+β₂xᵢ₂+...+βₚxᵢₚ+εᵢ，其中xᵢⱼ是第i个样本的第j个特征，βⱼ是第j个特征的系数（其他特征固定时，xⱼ每增加1单位，y的平均变化量）。
  - 矩阵表示：y=Xβ+ε，其中y是n×1输出向量，X是n×(p+1)设计矩阵（第一列为全1，对应截距β₀，其余列是特征值），β是(p+1)×1系数向量（β=(β₀,β₁,...,βₚ)ᵀ），ε是n×1误差向量。
- **模型假设**：误差项ε满足i.i.d.正态分布，即ε~N(0,σ²I)，其中I是n×n单位矩阵，意味着：误差均值为0、方差恒定（同方差性）、误差间独立、误差服从正态分布。这些假设是后续参数估计和假设检验的基础。
- **案例深化**：多元线性回归预测销售额（y），特征是广告投入（x₁）、产品价格（x₂），模型为yᵢ=β₀+β₁xᵢ₁+β₂xᵢ₂+εᵢ，假设β₁=1.5（广告投入每增加1万元，销售额平均增加1.5万元），β₂=-0.8（价格每增加1元，销售额平均减少0.8万元），ε~N(0,2²I)（误差方差为4，独立同分布）。

### 4.2 Least Squares Estimation（最小二乘估计）
- **核心思想**：通过最小化“残差平方和（RSS）”估计系数β，残差eᵢ=yᵢ-$\hat{y}_i$（真实值与预测值的差异），RSS=∑(yᵢ-$\hat{y}_i$)²=∑(yᵢ-β₀-∑βⱼxᵢⱼ)²。
- **矩阵形式**：RSS=(y-Xβ)ᵀ(y-Xβ)（向量内积表示平方和），最小化RSS的系数估计为$\hat{\beta}=(X^TX)^{-1}X^Ty$（通过对RSS求β的偏导并令导数为0推导得出）。
- **高斯-马尔可夫定理**：在模型假设（误差均值为0、方差恒定、误差独立）下，$\hat{\beta}$是最优线性无偏估计（BLUE），即$\hat{\beta}$是线性估计（关于y的线性函数）、无偏（E($\hat{\beta}$)=β），且在所有线性无偏估计中方差最小。
- **拟合值与帽子矩阵**：拟合值$\hat{y}=X\hat{\beta}=Hy$，其中H=X(XᵀX)⁻¹Xᵀ是帽子矩阵（Hat Matrix），作用是将输出向量y映射为拟合值$\hat{y}$，H是对称幂等矩阵（Hᵀ=H，H²=H）。
- **案例深化**：简单线性回归中，X=[[1,x₁],[1,x₂],...,[1,xₙ]]，y=[y₁,y₂,...,yₙ]ᵀ，$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$，$\hat{\beta}_1=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$（与矩阵公式一致）；如样本x=[1,2,3]，y=[3,5,7]，$\bar{x}=2$，$\bar{y}=5$，$\hat{\beta}_1=( (1-2)(3-5)+(2-2)(5-5)+(3-2)(7-5) ) / ( (1-2)²+(2-2)²+(3-2)² )= (2+0+2)/2=2，$\hat{\beta}_0=5-2×2=1$，模型为$\hat{y}=1+2x$，RSS=(3-3)²+(5-5)²+(7-7)²=0（完全拟合）。

### 4.3 Inference on Coefficient（系数推断）
- **系数协方差矩阵**：$\hat{\beta}$的协方差矩阵为cov($\hat{\beta}$)=σ²(XᵀX)⁻¹，反映系数估计的波动程度（对角线元素是各βⱼ的方差）。由于σ²未知，用残差均方误差（MSE=RSS/(n-p-1)）估计σ²，得到估计的协方差矩阵s²($\hat{\beta}$)=MSE(XᵀX)⁻¹，其中s($\hat{\beta}_j$)是$\hat{\beta}_j$的标准误（方差的平方根）。
- **单个系数的t检验**：检验第j个特征的系数是否为0（H₀:βⱼ=0，即该特征与y无线性关系；Hₐ:βⱼ≠0），检验统计量t*=$\hat{\beta}_j$/s($\hat{\beta}_j$)，服从自由度为n-p-1的t分布。若|t*|>t(α/2,n-p-1)（t分布的α/2分位数），则拒绝H₀，认为该特征对y有显著线性影响。
- **模型整体的F检验**：检验所有特征系数是否都为0（H₀:β₁=β₂=...=βₚ=0，即模型无意义；Hₐ:至少有一个βⱼ≠0），检验统计量F*=MSR/MSE，其中MSR=SSR/p（回归平方和的均方），SSR=SSTO-SSE（SSTO是总平方和，SSE=RSS是残差平方和），F*服从自由度为(p,n-p-1)的F分布。若F*>F(α,p,n-p-1)，则拒绝H₀，认为模型整体显著（至少有一个特征对y有线性影响）。
- **案例深化**：多元线性回归模型（p=2，n=50），$\hat{\beta}_1=1.5$，s($\hat{\beta}_1$)=0.5，t*=3.0，t(0.025,47)=2.01，|t*|>2.01，拒绝H₀:β₁=0，认为广告投入对销售额有显著影响；SSR=100，SSE=50，F*=(100/2)/(50/47)=47，F(0.05,2,47)=3.19，F*>3.19，拒绝H₀，认为模型整体显著。

### 4.4 Model Diagnosis（模型诊断）
- **残差分析**：通过残差eᵢ=yᵢ-$\hat{y}_i$的图形分析，验证模型假设是否成立：
  - 非线性检验：绘制残差vs拟合值$\hat{y}_i$的散点图，若点随机分布在0附近，说明线性假设成立；若呈明显趋势（如曲线），说明存在非线性关系（需调整模型，如添加多项式项）。
  - 异方差检验：若残差散点图呈“漏斗形”（如$\hat{y}_i$增大时，残差波动增大），说明误差方差不恒定（异方差性），违反同方差假设（需用加权最小二乘等方法修正）。
  - 非正态性检验：绘制残差的Q-Q图（分位数-分位数图），若点近似在直线上，说明残差服从正态分布；若偏离直线，说明非正态（大样本下影响较小，小样本需调整模型或数据变换）。
  - 独立性检验：若数据是时间序列，绘制残差vs时间的散点图，若点随机分布，说明误差独立；若呈趋势或周期性，说明误差存在自相关（需用时间序列模型修正）。
- **异常值与高杠杆点**：
  - Y异常值：指残差较大的样本（y值偏离模型预测），用学生化残差（考虑残差方差的异质性）或删除残差（删除该样本后的残差）识别，绝对值大于2或3的通常视为异常值。
  - X异常值（高杠杆点）：指特征值偏离其他样本的样本（如身高数据中的2.5米个体），用帽子矩阵的对角线元素hᵢᵢ衡量（hᵢᵢ是第i个样本对拟合值的影响程度），经验法则：hᵢᵢ>2(p+1)/n（p是特征数，n是样本数）的为高杠杆点，可能影响系数估计（需验证是否为数据错误或真实异常）。
- **案例深化**：残差vs拟合值散点图呈“U形”，说明模型存在非线性关系（需添加x²项）；Q-Q图中残差点偏离直线，且样本量小（n=30），需对y进行对数变换（使残差近似正态）；某样本hᵢᵢ=0.4，p=2，n=20，2(p+1)/n=0.3，hᵢᵢ>0.3，是高杠杆点，经核实是输入错误（x₁=1000，其他样本x₁<100），修正后系数估计更合理。

### 4.5 Additional Issues（其他问题）
- **ANOVA分解**：总平方和SSTO=∑(yᵢ-$\bar{y}$)²（y的总波动），可分解为回归平方和SSR=∑($\hat{y}_i$-$\bar{y}$)²（模型解释的波动）和残差平方和SSE=∑(yᵢ-$\hat{y}_i$)²（模型未解释的波动），即SSTO=SSR+SSE。决定系数R²=SSR/SSTO，衡量模型的拟合优度（R²∈[0,1]，R²越接近1，模型解释力越强），但R²会随特征数增加而增大（即使添加无关特征），需用调整后R²（Rₐⱼⱼ²=1-(SSE/(n-p-1))/(SSTO/(n-1))）修正（惩罚过多特征）。
- **多重共线性**：特征间存在高度线性相关（如“身高”和“体重”、“广告投入”和“营销费用”），会导致系数估计的方差增大（标准误变大）、系数符号异常（如本应正相关的特征系数为负）。用方差膨胀因子（VIF）检测：VIFⱼ=1/(1-Rⱼ²)，其中Rⱼ²是第j个特征对其他特征的回归R²，VIFⱼ越大，多重共线性越严重，经验法则：平均VIF>1或最大VIF>10时，存在严重多重共线性（解决方法：删除冗余特征、主成分回归、岭回归）。
- **定性预测变量**：特征为分类变量（如性别、学历）时，需用哑变量（Dummy Variables）编码。如性别（男/女），编码为X₂=1（男）、X₂=0（女），模型中β₂表示“男性相对于女性的y平均差异”；多分类变量（如学历：小学/中学/大学），编码为k-1个哑变量（k为类别数），避免多重共线性。
- **数据变换**：当模型违反假设（如非线性、异方差、非正态）时，对y或特征进行变换，常用Box-Cox变换（Y^λ，λ通过数据估计，λ=1时无需变换，λ=0时为对数变换），可同时修正非正态和异方差。
- **案例深化**：模型R²=0.8，调整后R²=0.78（p=3，n=100），说明模型解释了78%的y波动（拟合较好）；特征x₁和x₂的VIF=15，存在严重多重共线性，删除x₂后VIF=1.2，系数估计稳定；定性特征“学历”（小学/中学/大学），编码为X₃=1（中学）、0（其他），X₄=1（大学）、0（其他），β₃表示“中学学历相对于小学学历的销售额差异”，β₄表示“大学学历相对于小学学历的销售额差异”。

## 5. Model Selection and Regularization（模型选择与正则化）
### 5.1 Problems with Ordinary Least Squares (OLS)（普通最小二乘的问题）
- **多重共线性**：特征间高度相关时，OLS系数估计的方差会急剧增大（$(X^TX)^{-1}$的对角线元素变大），导致系数估计不稳定（不同训练集得到差异很大的系数）、假设检验不显著（标准误变大，t值变小），即使模型整体拟合较好，单个特征的解释也不可靠。
- **过拟合与预测精度**：当特征数p接近或大于样本数n（高维数据）时，OLS会“完美拟合”训练数据（甚至拟合噪声），导致训练误差极小但测试误差极大（过拟合），泛化能力差。这是“维度灾难”的体现：高维空间中数据稀疏，模型易学习到训练集的特有模式，而非普遍规律。
- **可解释性差**：当p较大时，OLS模型包含大量特征（包括无关特征），难以判断哪些特征对输出真正重要，模型解释性降低（如“100个特征的线性回归，无法确定关键影响因素”）。
- **核心目标**：解决OLS的上述问题，通过模型选择（筛选关键特征）或正则化（约束系数），获得“稀疏、稳定、可解释、预测精度高”的模型。
- **案例深化**：用p=50个特征（含40个无关特征）预测n=100个样本的销售额，OLS模型的训练MSE=50，测试MSE=500（过拟合）；特征x₁（广告投入）和x₂（营销费用）高度相关（相关系数0.95），OLS估计β₁=3.2，β₂=-2.8（符号不合理，因两者都应正向影响销售额），标准误分别为1.5和1.6（显著水平低），说明多重共线性导致系数异常。

### 5.2 Alternative Methods: Overview（替代方法概述）
- **子集选择（Subset Selection）**：从p个特征中筛选出一个子集（含k个特征，k<p），用OLS拟合该子集的模型，通过评估准则选择最优子集（如AIC、BIC、调整后R²）。核心是“剔除无关特征”，降低模型复杂度，提高可解释性和稳定性。
- **收缩方法（正则化，Shrinkage/Regularization）**：保留所有p个特征，但通过添加惩罚项约束系数的绝对值，使系数“收缩”到0附近（岭回归）或部分系数收缩到0（Lasso），减少方差，避免过拟合。核心是“不剔除特征，而是约束系数”，适用于高维数据。
- **降维方法（Dimension Reduction）**：将p个原始特征通过线性或非线性变换，投影到k维低维空间（k<p），用k个新特征（成分）拟合模型，降低数据维度，避免维度灾难。核心是“特征变换与压缩”，保留原始特征的关键信息。
- **方法对比**：子集选择得到稀疏模型（仅含部分特征），可解释性强，但计算量大（p大时）；收缩方法中Lasso可实现稀疏，岭回归不可；降维方法不直接解释原始特征，可解释性弱，但计算效率高。
- **案例深化**：p=50特征预测销售额，子集选择筛选出5个关键特征（广告投入、价格、地段等），用OLS拟合（可解释性强）；Lasso正则化使40个无关特征的系数收缩到0，仅保留10个特征的非零系数（稀疏模型）；PCA降维将50个特征投影到3个主成分，用主成分回归拟合（计算效率高）。

### 5.3 Variable Selection（变量选择）
- **最优子集选择（Best Subset Selection）**：枚举所有可能的特征子集（共2ᵖ-1个，不含空集），对每个子集用OLS拟合模型，计算评估准则（如AIC、BIC、调整后R²），选择准则最优的子集。评估准则：
  - 调整后R²：越大越好（惩罚过多特征）。
  - AIC（赤池信息准则）：AIC=2k - 2logL，k是模型参数个数，logL是对数似然，越小越好（平衡拟合优度和复杂度）。
  - BIC（贝叶斯信息准则）：BIC=kln(n) - 2logL，比AIC更惩罚复杂模型（n越大，惩罚越强），越小越好。
  - 缺点：p较大时（如p>20），枚举所有子集计算量极大（2²⁰≈10⁶个子集），难以实现。
- **逐步选择（Sequential Selection）**：通过迭代方式筛选特征，避免枚举所有子集，计算效率高：
  - 前向选择（Forward Selection）：从空模型（无特征）开始，每次添加“使模型准则最优”的特征（如使残差平方和最小的特征），直到添加特征不再改善准则或达到预设特征数。适用于p>n（高维数据），因无需拟合含所有特征的模型。
  - 后向选择（Backward Selection）：从全模型（含所有p个特征）开始，每次删除“使模型准则最优”的特征（如删除后残差平方和增加最少的特征），直到删除特征不再改善准则。不适用于p>n（全模型无法用OLS拟合，XᵀX不可逆）。
  - 逐步选择（Stepwise Selection）：结合前向和后向选择，前向添加特征后，检查已选特征中是否有“不再显著”的特征，若有则删除，迭代直到无特征可添加或删除。平衡了前向和后向选择的优点，是实际中常用的变量选择方法。
- **案例深化**：p=10特征预测房价，最优子集选择枚举2¹⁰-1=1023个子集，计算每个子集的BIC，选择BIC最小的子集（含3个特征：面积、卧室数、地段）；前向选择从空模型开始，第一次添加“面积”（使RSS最小），第二次添加“卧室数”，第三次添加“地段”，之后添加其他特征BIC不再降低，停止选择（结果与最优子集一致）；后向选择从10个特征的全模型开始，依次删除“车库数”“装修年份”等无关特征，最终保留3个特征。

### 5.4 Shrinkage Methods (Regularization)（收缩方法/正则化）
- **核心思想**：在OLS的残差平方和（RSS）基础上，添加惩罚项J(β)，目标函数为min(RSS + λJ(β))，其中λ≥0是正则化参数（惩罚强度）：λ=0时，退化为OLS；λ越大，惩罚越强，系数收缩越明显（方差越小，偏差越大）。
- **岭回归（Ridge Regression，L₂惩罚）**：
  - 目标函数：$\hat{\beta}^{ridge}=argmin[\sum(y_i - x_i^T\beta)^2 + \lambda\sum\beta_j^2]$，惩罚项是系数的L₂范数平方（∑βⱼ²）。
  - 矩阵形式：$\hat{\beta}^{ridge}=(X^TX + \lambda I)^{-1}X^Ty$，添加λI使XᵀX+λI可逆（即使p>n或存在多重共线性），解决OLS的可逆问题。
  - 特点：仅收缩系数，不将系数设为0（无变量选择功能），保留所有特征；对多重共线性鲁棒（惩罚项降低了系数方差）；系数估计是有偏的，但方差更小，整体预测误差（偏差²+方差）可能更低。
- **Lasso（L₁惩罚）**：
  - 目标函数：$\hat{\beta}^{lasso}=argmin[\sum(y_i - x_i^T\beta)^2 + \lambda\sum|\beta_j|]$，惩罚项是系数的L₁范数（∑|βⱼ|）。
  - 特点：L₁惩罚的“稀疏性”（绝对值惩罚导致部分系数精确收缩到0），实现变量选择（系数为0的特征被剔除）；同时解决多重共线性和过拟合问题；模型稀疏，可解释性强。
- **λ的选择**：通过交叉验证（如K折CV）选择最优λ，即选择使CV误差最小的λ（平衡偏差和方差）。
- **案例深化**：p=50特征（含40个无关特征）预测销售额，λ=0.1时，岭回归将所有系数收缩到较小值（如无关特征系数从OLS的0.5收缩到0.02），但未归零；Lasso将40个无关特征的系数收缩到0，仅保留10个相关特征的非零系数（如广告投入系数=1.2，价格系数=-0.7）；通过10折CV选择λ=0.05（Lasso），CV误差最小，模型泛化能力最优。

### 5.5 Dimension Reduction（降维方法）
- **核心思想**：将p个原始特征X通过线性变换，生成k个新特征Z₁,Z₂,...,Zₖ（k<p，称为成分），Z是X的线性组合（Z=XA，A是p×k变换矩阵），用Z替代X拟合模型（如线性回归），降低数据维度，避免过拟合。
- **主成分分析（PCA，Principal Component Analysis）**：
  - 定义：无监督降维方法（仅利用特征X，不利用输出Y），寻找k个线性组合Z₁,...,Zₖ（主成分），使得Z₁是X的线性组合中方差最大的（第一主成分），Z₂是与Z₁正交的线性组合中方差最大的（第二主成分），依此类推。
  - 原理：主成分是X的协方差矩阵的前k个特征向量（对应最大的k个特征值），特征值越大，该主成分捕捉的方差越多。
  - 特点：无监督（不考虑Y），可能导致主成分与Y无关（如第一主成分捕捉的是X的噪声方差）；仅适用于线性降维。
- **主成分回归（PCR，Principal Component Regression）**：
  - 步骤：1. 对X标准化（消除量纲影响）；2. 用PCA提取前k个主成分Z₁,...,Zₖ；3. 用Z₁,...,Zₖ作为特征，拟合线性回归模型$\hat{y}=\hat{\beta}_0+\hat{\beta}_1Z_1+...+\hat{\beta}_kZ_k$。
  - 特点：通过k<p降低模型复杂度，避免过拟合；k的选择通过交叉验证（选择使CV误差最小的k）；缺点：主成分是X的线性组合，难以解释原始特征与Y的关系。
- **偏最小二乘（PLS，Partial Least Squares）**：
  - 定义：有监督降维方法（同时利用X和Y），寻找k个线性组合Z₁,...,Zₖ（PLS成分），使得Z₁与Y的相关性最大（第一PLS成分），Z₂与Z₁正交且与Y的相关性最大（第二PLS成分），依此类推。
  - 特点：监督学习（考虑Y），PLS成分与Y相关，预测精度通常高于PCR；适用于X存在多重共线性或p>n的情况；同样难以解释原始特征。
- **案例深化**：p=100特征（含80个无关特征）预测销售额（Y），PCA提取前5个主成分（累计方差解释率90%），PCR用这5个主成分拟合回归，测试MSE=120；PLS提取前3个PLS成分（与Y的累计相关性85%），测试MSE=80（高于PCR，因PLS成分与Y相关）；k=3时PLS的CV误差最小，为最优k值。

## 6. Classification（分类）
### 6.1 Basic Setting: Classification Function & Boundary（基本设置：分类函数与决策边界）
- **核心定义**：分类任务中，输入x∈ℝᵖ（p维特征），输出y∈{1,2,...,K}（K个类别标签，如K=2为二分类：“正类/负类”，K=3为多分类：“猫/狗/鸟”）。
- **分类函数与决策函数**：分类函数hₖ(x)表示x属于类别k的得分或后验概率P(Y=k|x)（k=1,...,K）；决策函数G(x)=argmaxₖhₖ(x)，即选择得分最高或后验概率最大的类别作为x的预测标签。
- **决策边界**：特征空间中，两类得分相等的点的集合，即{x:hₖ(x)=hₗ(x)}（k≠l）。二分类任务中，决策边界常简化为{x:h₁(x)=0.5}（后验概率等于0.5的点），边界一侧为类别1，另一侧为类别2。
- **边界类型**：决策边界的形状由分类函数hₖ(x)决定，如线性分类器（LDA、逻辑回归）的决策边界是线性超平面（p=2时为直线），非线性分类器（QDA、SVM核方法）的决策边界是曲线或曲面。
- **案例深化**：二分类任务（预测邮件是否为垃圾邮件，y=1=垃圾邮件，y=0=正常邮件），特征x=(x₁=关键词频率，x₂=发送时间），分类函数h₁(x)=P(Y=1|x)=σ(β₀+β₁x₁+β₂x₂)（σ是逻辑函数），决策边界为{x:β₀+β₁x₁+β₂x₂=0}（p=2时为直线），直线一侧h₁(x)>0.5（预测为垃圾邮件），另一侧h₁(x)<0.5（预测为正常邮件）。

### 6.2 Popular Classification Methods: Bayes Classifier（常用分类方法：贝叶斯分类器）
- **定义**：理论上的“最优分类器”，决策函数G*(x)=argmaxₖP(Y=k|x)，即对每个x，预测为后验概率最大的类别。
- **贝叶斯误差率**：贝叶斯分类器的误分类误差，是所有分类器中最低的误差率（不可超越的下界），计算公式为1 - E[maxₖP(Y=k|x)]（对所有x，取最大后验概率的平均，再用1减去）。贝叶斯误差率由数据分布决定，无法通过模型优化降低（仅能逼近）。
- **核心挑战**：真实后验概率P(Y=k|x)未知，需通过数据估计，后续分类方法（LDA、QDA、逻辑回归）的本质都是“估计后验概率P(Y=k|x)”，再基于估计的后验概率进行决策。
- **案例深化**：二分类任务中，x=“关键词频率=0.8，发送时间=23点”，P(Y=1|x)=0.9（垃圾邮件概率90%），P(Y=0|x)=0.1，贝叶斯分类器预测为y=1（垃圾邮件）；若数据中P(Y=1|x)的均值为0.8，贝叶斯误差率=1-0.8=0.2（20%），任何分类器的误差率都不可能低于20%。

### 6.3 Popular Classification Methods: LDA and QDA（常用分类方法：LDA与QDA）
- **判别分析思想**：基于贝叶斯定理，P(Y=k|x)∝πₖfₖ(x)，其中πₖ是类别k的先验概率（P(Y=k)，如“垃圾邮件的先验概率=0.3”），fₖ(x)是类别k的类条件密度（P(x|Y=k)，如“垃圾邮件中关键词频率x的密度函数”）。通过估计πₖ和fₖ(x)，即可得到后验概率P(Y=k|x)。
- **线性判别分析（LDA，Linear Discriminant Analysis）**：
  - 假设：每个类别的类条件密度fₖ(x)服从正态分布N(μₖ,Σ)，即所有类别共享同一个协方差矩阵Σ（简化假设，降低模型复杂度）。
  - 推导：将正态密度代入贝叶斯定理，取对数后，后验概率的对数比为线性函数（关于x的一次函数），因此决策边界是线性超平面（p=2时为直线），称为“线性判别函数”。
  - 优点：模型简单、计算高效、对小样本鲁棒；缺点：协方差矩阵相同的假设可能不成立（如不同类别特征的波动差异大），此时分类效果下降。
- **二次判别分析（QDA，Quadratic Discriminant Analysis）**：
  - 假设：每个类别的类条件密度fₖ(x)服从正态分布N(μₖ,Σₖ)，即每个类别有独立的协方差矩阵Σₖ（更灵活的假设）。
  - 推导：对数后验概率比为二次函数（关于x的二次函数），决策边界是二次超曲面（p=2时为椭圆、抛物线等曲线），称为“二次判别函数”。
  - 优点：比LDA灵活，能拟合非线性决策边界；缺点：参数更多（每个类别需估计协方差矩阵），计算量更大，小样本下易过拟合。
- **案例深化**：二分类任务（类别1：垃圾邮件，类别2：正常邮件），特征x=关键词频率，LDA假设f₁(x)~N(0.8,0.1²)，f₂(x)~N(0.2,0.1²)（共享协方差），决策边界为直线x=0.5（x>0.5预测为类别1）；QDA假设f₁(x)~N(0.8,0.1²)，f₂(x)~N(0.2,0.05²)（独立协方差），决策边界为二次曲线x=0.45+0.02x²（非线性边界），更贴合数据分布。

### 6.4 Popular Classification Methods: Logistic Regression（常用分类方法：逻辑回归）
- **适用场景**：二分类任务（可扩展至多分类），直接建模后验概率P(Y=1|x)（避免假设类条件密度，与LDA/QDA的“生成式建模”不同，逻辑回归是“判别式建模”）。
- **逻辑函数（Sigmoid函数）**：确保后验概率落在[0,1]区间，公式为p(x)=P(Y=1|x)=e^(β₀+xᵀβ)/(1+e^(β₀+xᵀβ))，其图像是S形曲线（x→-∞时p(x)→0，x→+∞时p(x)→1，x=-β₀/β₁时p(x)=0.5）。
- **对数几率（Logit）**：logit(p(x))=log(p(x)/(1-p(x)))=β₀+xᵀβ，即对数几率与特征x呈线性关系（逻辑回归的核心假设），βⱼ表示“xⱼ每增加1单位，对数几率的变化量”（可解释为特征对分类的影响程度）。
- **参数估计**：无法用OLS估计β（因p(x)与x是非线性关系），采用最大似然估计（MLE）：似然函数L(β)=∏[p(xᵢ)]^yᵢ[1-p(xᵢ)]^(1-yᵢ)，取对数似然后，通过迭代重加权最小二乘（IRLS）求解β的估计值$\hat{\beta}$（使似然函数最大化）。
- **多分类扩展**：One-vs-Rest（对每个类别k，训练“k类vs其他类”的二分类逻辑回归，共K个模型，预测时选择概率最大的类别）；One-vs-One（对每对类别训练二分类模型，共K(K-1)/2个模型，预测时投票决定类别）。
- **案例深化**：二分类预测客户是否购买产品（y=1=购买，y=0=不购买），特征x₁=年龄，x₂=收入，逻辑回归模型为p(x)=e^( -2 + 0.05x₁ + 0.001x₂ )/(1+e^( -2 + 0.05x₁ + 0.001x₂ ))。logit(p(x))=-2+0.05x₁+0.001x₂，说明“年龄每增加1岁，购买的对数几率增加0.05”“收入每增加1元，对数几率增加0.001”；某客户x₁=30，x₂=50000，p(x)=e^( -2 + 1.5 + 50 )/(1+e^(49.5))≈1，预测为购买。

### 6.5 Popular Classification Methods: Separating Hyperplane（常用分类方法：分离超平面）
- **核心思想**：二分类任务中，寻找一个超平面（p维空间中的p-1维平面），将不同类别的样本分隔开，超平面方程为β₀+xᵀβ=0，其中β是超平面的法向量（垂直于超平面），β₀是截距。
- **分离超平面**：若存在超平面使得所有类别1的样本满足β₀+xᵀβ>0，所有类别2的样本满足β₀+xᵀβ<0，则数据是线性可分的，该超平面称为“分离超平面”；若数据线性不可分（存在重叠），则无完美分离超平面，需允许部分样本错分（软间隔）。
- **最优分离超平面（最大间隔超平面）**：线性可分数据中，存在多个分离超平面，最优超平面是“Margin最大”的超平面。Margin定义为“超平面到最近样本点的距离”，公式为Margin=2/||β||（||β||是β的L₂范数）。最大化Margin等价于最小化||β||²/2（简化计算），约束条件为yᵢ(β₀+xᵢᵀβ)≥1（所有样本点在Margin之外）。
- **核心优势**：最大间隔超平面具有更好的泛化能力（Margin越大，对新样本的分类稳定性越高），是支持向量机（SVM）的基础。
- **案例深化**：二分类数据（p=2，类别1：(1,1),(2,2)；类别2：(-1,-1),(-2,-2)），分离超平面可设为x₁+x₂=0（β₀=0，β=(1,1)ᵀ），样本到超平面的距离分别为|1+1|/√2=√2，|2+2|/√2=2√2，|-1-1|/√2=√2，|-2-2|/√2=2√2，Margin=2√2（最小距离的2倍）；若超平面设为x₁+x₂-1
### 6.5 Popular Classification Methods: Separating Hyperplane（常用分类方法：分离超平面）
- 核心补充：线性可分数据中，最优分离超平面的“最大间隔”特性可降低泛化误差——Margin越大，样本点离边界越远，对新数据的分类容错性越强。例如上述案例中，超平面\(x_1+x_2=0\)的Margin=2√2，若换超平面\(x_1+x_2-1=0\)，类别1样本(1,1)满足\(1+1-1=1>0\)，类别2样本(-1,-1)满足\(-1-1-1=-3<0\)，但最小距离为|1+1-1|/√2=√2/2，Margin=√2，远小于前者，新样本靠近边界时易错分。
- 优化求解：最大间隔超平面的优化问题可转化为凸二次规划（目标函数凸、约束线性），通过拉格朗日乘数法求解，最终解仅依赖于“支持向量”（离超平面最近的样本点），后续SVM即基于此扩展。

### 6.6 Performance Measure（性能衡量）
- 误分类误差（Misclassification Error）：最直观指标，公式为“错误预测数/总样本数”，适用于类别均衡数据；但类别不平衡时（如90%负类、10%正类），模型全预测负类也能达到90%准确率，无法反映真实性能。
- 混淆矩阵（Confusion Matrix）：二分类任务中，将预测结果分为四类——真阳性（TP：正类预测为正类）、假阳性（FP：负类预测为正类）、真阴性（TN：负类预测为负类）、假阴性（FN：正类预测为负类），基于此衍生核心指标：
  - 灵敏度（Sensitivity/Recall）：正类识别率，\(TP/(TP+FN)\)（如疾病诊断中，不漏诊真实患者的概率）。
  - 特异性（Specificity）：负类识别率，\(TN/(TN+FP)\)（如疾病诊断中，不误诊健康人的概率）。
  - 精确率（Precision）：预测为正类的样本中真实正类的比例，\(TP/(TP+FP)\)（如搜索引擎中，检索结果的准确率）。
  - F1分数（F1-score）：精确率和召回率的调和平均，\(2×(Precision×Recall)/(Precision+Recall)\)，平衡两者（适用于精确率和召回率不可兼得的场景，如垃圾邮件检测：召回率高易误判正常邮件，精确率高易漏判垃圾邮件）。
- ROC曲线与AUC：
  - ROC曲线（受试者工作特征曲线）：以假阳性率（FPR=FP/(TN+FP)）为横轴，真阳性率（TPR=Recall）为纵轴，通过调整分类阈值（如逻辑回归的0.5概率阈值）绘制的曲线，曲线越靠近左上角，模型性能越好。
  - AUC（ROC曲线下面积）：量化ROC曲线的优劣，AUC∈[0.5,1]，AUC=0.5表示模型随机猜测，AUC越接近1表示模型区分能力越强（如AUC=0.95的模型远优于AUC=0.7的模型）。
- 案例深化：垃圾邮件检测（正类=垃圾邮件，样本数：TP=80，FP=5，TN=95，FN=10）：
  - 误分类误差=(5+10)/(80+5+95+10)=15/190≈7.89%。
  - 召回率=80/(80+10)=88.89%（漏判11.11%垃圾邮件），精确率=80/(80+5)=94.12%（误判5.88%正常邮件），F1=2×(0.8889×0.9412)/(0.8889+0.9412)≈0.914。
  - ROC曲线中，当阈值降低（更多邮件判为垃圾邮件），TPR和FPR均升高；AUC=0.92，说明模型区分垃圾邮件和正常邮件的能力较强。

## 7. Moving beyond Linearity（超越线性）
### 7.1 Step Function and Piecewise Polynomials（阶梯函数与分段多项式）
- 阶梯函数（Step Functions）：将连续特征x划分为K个区间（通过断点\(c_1,c_2,...,c_K\)），每个区间拟合一个常数βₖ，模型为\(y_i=\beta_0+\sum_{k=1}^K\beta_kI(c_k≤x_i<c_{k+1})+\epsilon_i\)（I(·)为指示函数）。例如将年龄x划分为[0,30)、[30,50)、[50,+∞)，模型为\(y=\beta_0+\beta_1I(30≤x<50)+\beta_2I(x≥50)+\epsilon\)，β₁表示30-50岁相对于<30岁的y差异，β₂表示≥50岁的差异。
- 特点：局部拟合（每个区间内y为常数），模型简单易解释，但存在“跳跃不连续”（区间边界处y值突变），无法捕捉区间内的线性或非线性趋势。
- 分段多项式（Piecewise Polynomials）：将x划分为多个区间（每个区间称为一个“段”），每个段内拟合低次多项式（如一次、三次），例如分段三次多项式：x< c时\(y=a_0+a_1x+a_2x²+a_3x³\)，x≥c时\(y=b_0+b_1x+b_2x²+b_3x³\)（c为断点）。
- 问题与约束：未加约束时，多项式在断点处可能不连续（y值、一阶导数、二阶导数突变），导致模型不稳定；需添加连续性约束（如断点处y值相等、一阶导数相等），使模型平滑过渡。
- 案例深化：预测商品销量（y）与促销力度（x），x∈[0,10]，阶梯函数断点c=5：x<5时y=100（β₀=100），x≥5时y=100+50=150（β₁=50），但无法反映x=3和x=4的销量差异；分段线性多项式（断点c=5）：x<5时y=80+5x，x≥5时y=50+10x，断点处x=5时y=105（连续），更贴合“促销力度越大，销量增长越快”的趋势。

### 7.2 Basis Functions（基函数）
- 核心思想：将线性模型扩展为“基函数的线性组合”，模型形式为\(y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+...+\beta_Kb_K(x_i)+\epsilon_i\)，其中\(b_k(·)\)是预先定义的基函数（如多项式、阶梯函数、样条），通过基函数对原始特征x进行变换，将非线性关系转化为“基函数空间”的线性关系，仍可用OLS估计β。
- 常见基函数：
  - 多项式基函数：\(b_k(x)=x^k\)（k=1,2,...,K），模型为多项式回归（如K=2时为二次回归\(y=\beta_0+\beta_1x+\beta_2x²+\epsilon\)）。
  - 阶梯基函数：\(b_k(x)=I(c_k≤x<c_{k+1})\)（对应阶梯函数模型）。
  - 样条基函数：如截断幂基\(b_k(x)=(x-\xi_k)_+^3\)（\(\xi_k\)为样条断点，\((x-\xi)_+=\max(x-\xi,0)\)），对应三次样条模型。
- 优势：灵活适配非线性关系，无需手动设计复杂模型，仅需选择合适的基函数类型和数量；本质是“线性模型的非线性扩展”，保留了线性模型的计算高效性。
- 案例深化：预测房屋价格（y）与面积（x），真实关系为\(y=50+2x+0.01x²+\epsilon\)（二次非线性），选择多项式基函数\(b_1(x)=x\)，\(b_2(x)=x²\)，模型为\(y=\beta_0+\beta_1b_1(x)+\beta_2b_2(x)+\epsilon\)，OLS估计得\(\hat{\beta}_0=51\)，\(\hat{\beta}_1=1.98\)，\(\hat{\beta}_2=0.0102\)，接近真实关系，拟合效果远优于线性模型。

### 7.3 Cubic Spline（三次样条）
- 定义：分段三次多项式+连续性约束，即x被划分为K+1个区间（K个断点\(\xi_1<\xi_2<...<\xi_K\)），每个区间内是三次多项式，且在所有断点处满足：y值连续、一阶导数连续、二阶导数连续，确保模型整体平滑（无跳跃、无尖角）。
- 模型形式：基于截断幂基函数，\(y_i=\beta_0+\beta_1x_i+\beta_2x_i²+\beta_3x_i³+\sum_{k=1}^K\beta_{k+3}(x_i-\xi_k)_+^3+\epsilon_i\)，其中\((x-\xi)_+^3=\max(x-\xi,0)^3\)（x<\(\xi\)时为0，x≥\(\xi\)时为三次函数）。
- 断点（Knots）选择：
  - 数量：K越多，模型越灵活（可拟合复杂曲线），但易过拟合；K越少，模型越刚性（接近线性），易欠拟合，需通过交叉验证选择最优K。
  - 位置：通常选择x的分位数（如四分位数、五分位数），确保每个区间内样本量均匀（避免部分区间样本过少导致拟合不稳定）。
- 优势：相比全局多项式回归（如五次、七次多项式），三次样条仅在局部拟合三次多项式，避免了全局多项式在边界处的“震荡现象”（过拟合），同时保持平滑性。
- 案例深化：拟合\(y=\sin(x)+\epsilon\)（x∈[0,10]，\(\epsilon~N(0,0.1²)\)），选择3个断点（\(\xi_1=2.5,\xi_2=5,\xi_3=7.5\)），三次样条模型在每个区间内拟合三次多项式，断点处y值、一阶导数、二阶导数连续，拟合曲线平滑贴合\(\sin(x)\)的波动，而全局五次多项式在x=10附近出现明显震荡（过拟合）。

### 7.4 Natural Spline（自然样条）
- 问题背景：普通三次样条在特征x的边界区间（x<\(\xi_1\)或x>\(\xi_K\)）可能出现高方差（曲线波动剧烈），因边界区间无样本约束，多项式易“失控”。
- 定义：在三次样条基础上添加“边界约束”——要求模型在边界区间（x<\(\xi_1\)和x>\(\xi_K\)）内为线性函数（二阶导数为0），即边界外无弯曲，仅在中间区间保持三次多项式特性。
- 优势：解决了普通三次样条的边界波动问题，使边界预测更稳定，同时保留了中间区间的非线性拟合能力，是实际应用中最常用的样条类型（默认首选）。
- 案例深化：延续上述\(y=\sin(x)+\epsilon\)案例，自然样条在x<2.5和x>7.5的区间内为线性函数，避免了普通三次样条在x=0和x=10附近的轻微震荡，边界处预测值更合理（如x=11时，自然样条预测值基于x>7.5的线性趋势，而普通三次样条可能出现异常波动）。

### 7.5 Smoothing Spline（平滑样条）
- 核心思想：不预先指定断点，而是以所有独特的x值为断点，通过“粗糙度惩罚”控制模型平滑度，目标函数为\(\min_g\sum_{i=1}^n(y_i-g(x_i))^2+\lambda\int(g''(t))^2dt\)，其中g(x)是待估计的平滑函数，第一项是拟合误差（RSS），第二项是粗糙度惩罚（\(g''(t)\)是二阶导数，积分越大表示曲线越粗糙），λ≥0是惩罚参数。
- 解的形式：目标函数的最优解是“自然三次样条”（断点为所有独特x值），即平滑样条本质是带粗糙度惩罚的自然三次样条。
- 惩罚参数λ的影响：
  - λ=0：无惩罚，模型插值所有样本点（g(x_i)=y_i），曲线极度粗糙（过拟合，高方差、低偏差）。
  - λ→∞：惩罚极强，二阶导数积分趋近于0，g(x)为直线（欠拟合，低方差、高偏差）。
  - 最优λ选择：通过交叉验证（LOOCV或K-Fold CV），选择使CV误差最小的λ（平衡拟合优度和平滑度）。
- 优势：无需手动选择断点数量和位置，模型自适应数据分布；仅需调整λ即可控制灵活度，操作简单。
- 案例深化：拟合房价与面积数据（含噪声），λ=0.01时，平滑样条曲线贴合大部分样本点（含噪声，过拟合）；λ=0.1时，曲线平滑，剔除噪声干扰，清晰呈现房价随面积增长的非线性趋势；λ=10时，曲线近似直线（欠拟合，未捕捉非线性）；通过LOOCV选择λ=0.08，CV误差最小，模型泛化能力最优。

### 7.6 Local Regression（局部回归）
- 核心思想：“近邻拟合”，即预测某点\(x_0\)的y值时，仅使用\(x_0\)附近的局部样本（而非全部样本），用简单模型（如线性模型、常数模型）拟合局部样本，权重为“距离越近，权重越大”（避免远样本干扰）。
- 步骤：
  1. 确定邻域：选择一个“跨度（Span）s”（0<s<1），选取与\(x_0\)最近的s×n个样本（如s=0.3，n=1000，选取300个最近样本）。
  2. 赋予权重：用核函数计算权重\(K_{i0}\)，常用高斯核\(K_{i0}=\exp(-\frac{(x_i-x_0)^2}{2h^2})\)（h为带宽，控制权重衰减速度），\(x_i\)越靠近\(x_0\)，\(K_{i0}\)越大。
  3. 加权拟合：最小化加权残差平方和\(\min_{\beta_0,\beta_1}\sum_{i=1}^nK_{i0}(y_i-\beta_0-\beta_1x_i)^2\)，得到局部线性模型，预测\(\hat{y}_0=\hat{\beta}_0+\hat{\beta}_1x_0\)。
- 跨度s的影响：
  - s越小：邻域越窄，仅用少量近样本拟合，曲线越灵活（高方差、低偏差，易过拟合）。
  - s越大：邻域越宽，用更多样本拟合，曲线越平滑（低方差、高偏差，易欠拟合）。
- 优势：无需预设基函数或断点，完全数据驱动，能拟合复杂非线性关系；模型直观，仅关注局部数据规律。
- 案例深化：预测温度（y）与时间（x）的非线性关系（含季节波动），s=0.1时，局部回归仅用10%最近样本，曲线贴合短期波动（如单日温度变化）；s=0.5时，用50%样本拟合，曲线平滑呈现季节趋势（如夏季高温、冬季低温）；s=0.3时，平衡短期波动和长期趋势，拟合效果最优。

### 7.7 Generalized Additive Model (GAM)（广义加性模型）
- 定义：扩展线性模型，允许每个特征的影响为非线性函数，同时保持“加性结构”，模型形式为\(y_i=\beta_0+f_1(x_{i1})+f_2(x_{i2})+...+f_p(x_{ip})+\epsilon_i\)，其中\(f_j(·)\)是单变量非线性函数（可选择平滑样条、局部回归、多项式等），β₀是截距。
- 核心特点：
  - 灵活性：每个特征独立建模非线性关系（如\(f_1(x_1)\)用平滑样条拟合收入的非线性影响，\(f_2(x_2)\)用局部回归拟合年龄的影响），无需假设特征间的交互作用。
  - 可解释性：加性结构使每个特征对y的影响独立可分离，可通过绘制\(f_j(x_j)\)的曲线，直观观察该特征与y的关系（如\(f_1(x_1)\)呈上升曲线，说明收入越高，y越大）。
  - 局限性：仅考虑单个特征的非线性，忽略特征间的交互作用（如收入和教育程度的协同影响），需手动添加交互项（如\(f_{12}(x_1,x_2)\)）弥补。
- 拟合方法：反向拟合算法（Backfitting）——迭代拟合每个\(f_j\)，固定其他\(f_k(k≠j)\)，用当前残差拟合\(f_j\)，重复直到收敛。
- 案例深化：预测客户消费金额（y），特征为收入（x₁）、年龄（x₂）、消费频率（x₃），GAM模型为\(y=\beta_0+f_1(x_1)+f_2(x_2)+f_3(x_3)+\epsilon\)：
  - \(f_1(x_1)\)用平滑样条拟合，曲线呈“先快速上升，后平缓”（低收入时消费随收入快速增长，高收入后增长放缓）。
  - \(f_2(x_2)\)用局部回归拟合，曲线呈“倒U形”（25-45岁消费最高，年轻和老年消费较低）。
  - \(f_3(x_3)\)用线性函数拟合（消费频率越高，消费金额线性增长）。
  - 模型既捕捉了单个特征的非线性，又通过加性结构保持了解释性，比线性模型拟合效果好（测试MSE降低30%）。

## 8. Tree Methods（树方法）
### 8.1 Basic Idea of Tree-Based Methods（树方法基本思想）
- 核心概念：将p维特征空间划分为M个互不重叠的矩形区域（“盒子”），每个区域内的预测值为该区域训练样本的均值（回归任务）或众数（分类任务），模型本质是“区域-wise常数函数”，完全非参数（无需假设数据分布）。
- 直观理解：模拟人类决策过程，通过一系列“if-then”规则分割特征空间，例如预测房屋价格的决策树：“if 面积<100㎡ and 卧室数<2 → 价格=80万；if 面积≥100㎡ and 地段=核心区 → 价格=150万”。
- 树结构组成：
  - 根节点（Root Node）：包含所有训练样本，无父节点。
  - 内部节点（Internal Node）：对应特征的分割规则（如“面积<100㎡”），分割后产生子节点。
  - 叶节点（Leaf Node）：对应最终区域，输出预测值（如80万），无子节点。
  - 分支（Branch）：连接父节点和子节点的路径，对应一条决策规则。
- 优势：模型直观、易解释（可绘制决策树图），无需特征预处理（如标准化），自然处理定性和定量特征。
- 案例深化：回归树预测二手车价格，特征为车龄（x₁）、里程（x₂），分割规则：根节点“车龄<3年”→ 左子节点“里程<5万公里”（叶节点：价格=15万）、右子节点“里程≥5万公里”（叶节点：价格=12万）；根节点“车龄≥3年”→ 左子节点“里程<8万公里”（叶节点：价格=8万）、右子节点“里程≥8万公里”（叶节点：价格=5万），模型通过4个区域覆盖特征空间，预测简单直接。

### 8.2 Tree Building (Recursive Binary Splitting)（树构建：递归二分法）
- 核心过程：自上而下（Top-down）的贪心算法（Greedy Algorithm），逐步分割特征空间：
  1. 初始化：所有样本在根节点，特征空间为整个p维空间。
  2. 分割选择：对每个特征x_j，遍历所有可能的截断点s，计算“分割后区域的不纯度降低量”，选择“降低量最大”的（x_j,s）作为当前节点的分割规则（如分类任务选择Gini系数降低最多的分割，回归任务选择RSS降低最多的分割）。
  3. 递归分割：将当前节点的样本按分割规则分为左、右子节点，对每个子节点重复步骤2，继续分割。
  4. 停止条件：当子节点样本数小于最小阈值（如5个），或分割后不纯度降低量小于阈值（如0.01），停止分割，子节点成为叶节点。
- 贪心特性：每个节点选择“当前最优”的分割，不考虑未来分割的效果（如当前分割使本节点不纯度降低最多，但可能导致后续子节点无法有效分割），优点是计算高效，缺点是可能得到局部最优树（非全局最优）。
- 案例深化：分类树预测客户是否流失（y=1=流失，y=0=留存），特征为消费频率（x₁）、满意度（x₂）：
  - 根节点（样本数=1000）：遍历x₁的截断点（如5次、6次、...）和x₂的截断点（如3分、4分、...），计算分割后Gini系数降低量，发现x₁=5次时降低最多（Gini从0.48降至0.32），选择分割规则“消费频率<5次”。
  - 左子节点（消费频率<5次，样本数=400）：继续遍历分割，选择“满意度<3分”（Gini降低最多），分割为两个子节点（样本数分别为200和200）。
  - 右子节点（消费频率≥5次，样本数=600）：选择“满意度<4分”分割，最终生成含6个叶节点的决策树。

### 8.3 Split Goodness Measures（分割优度衡量）
- 分类任务（不纯度指标：衡量节点内样本类别的混杂程度，不纯度越低，分割效果越好）：
  - 基尼系数（Gini Index）：\(G=\sum_{k=1}^Kp_{mk}(1-p_{mk})\)，其中\(p_{mk}\)是节点m中类别k的比例，范围[0,0.5]（K=2时），G=0表示节点内全为同一类别（纯节点），G越大表示类别越混杂。
  - 交叉熵（Cross-Entropy）：\(D=-\sum_{k=1}^Kp_{mk}\log p_{mk}\)，范围[0,+∞)，D=0表示纯节点，与基尼系数趋势一致，但对类别比例变化更敏感（如p=0.1和p=0.9时，D=0.32，G=0.18）。
  - 误分类误差（Misclassification Error）：\(E=1-\max_{k}p_{mk}\)，即节点内非主要类别的样本比例，范围[0,1]，但对分割的敏感性低于基尼系数和交叉熵（如分割前后误分类误差变化小，难以区分优劣），实际应用较少。
- 回归任务（不纯度指标：衡量节点内样本y值的离散程度，离散程度越低，分割效果越好）：
  - 残差平方和（RSS）：\(RSS=\sum_{i\in R_m}(y_i-\hat{y}_{R_m})^2\)，其中\(R_m\)是节点m的样本集，\(\hat{y}_{R_m}\)是节点m的预测值（样本均值），分割后总RSS=左子节点RSS+右子节点RSS，分割优度=分割前RSS-分割后总RSS（降低量越大越好）。
- 案例深化：分类节点m含样本：类别1=60，类别2=40（\(p_{m1}=0.6\)，\(p_{m2}=0.4\)）：
  - 基尼系数G=0.6×0.4+0.4×0.6=0.48。
  - 交叉熵D=-(0.6×log0.6 + 0.4×log0.4)≈0.673。
  - 误分类误差E=1-0.6=0.4。
  - 分割后左子节点（类别1=50，类别2=10，G=0.278），右子节点（类别1=10，类别2=30，G=0.375），总G=0.278×(60/100)+0.375×(40/100)=0.318，降低量=0.48-0.318=0.162，分割有效。

### 8.4 Tree Pruning (Cost-Complexity)（树剪枝：成本-复杂度剪枝）
- 问题背景：递归二分法生成的树通常“过深过大”（如含100个叶节点），模型过度拟合训练数据（对训练集噪声敏感，样本微小变化导致树结构大幅改变，测试误差高），需通过剪枝（Pruning）简化树结构，降低方差，提高泛化能力。
- 核心思想：生成一棵最大树\(T_{max}\)（直到无法分割），然后从叶节点向上剪枝，删除子树，用父节点替代，选择“惩罚后误差最小”的子树作为最优树。
- 成本-复杂度准则：定义子树T的惩罚误差为\(C_\alpha(T)=\sum_{m=1}^{|T|}RSS_m+\alpha|T|\)，其中\(|T|\)是子树T的叶节点数（复杂度），\(RSS_m\)是叶节点m的RSS，α≥0是惩罚参数（控制复杂度与拟合优度的权衡）。
- α的影响与选择：
  - α=0：无惩罚，最优树为\(T_{max}\)（过拟合）。
  - α→∞：惩罚极强，最优树为仅含根节点的树（欠拟合）。
  - 最优α选择：通过交叉验证，选择使CV误差最小的α（如K-Fold CV，对每个α对应的剪枝树计算CV误差，取最小值）。
- 案例深化：最大树\(T_{max}\)含20个叶节点，训练RSS=500；α=10时，剪枝为10个叶节点的树，惩罚误差=300+10×10=400；α=20时，剪枝为5个叶节点的树，惩罚误差=450+20×5=550；通过10折CV，α=10时CV误差最小（250），选择10个叶节点的剪枝树作为最优模型。

### 8.5 Pros and Cons of Trees（树方法的优缺点）
- 优点：
  1. 易解释、可视化：决策树的“if-then”规则符合人类思维，可绘制树图直观展示决策过程（如业务人员无需专业知识即可理解）。
  2. 无需特征预处理：对特征量纲不敏感（无需标准化、归一化），自然处理定性特征（如性别、学历）和定量特征（如收入、年龄），无需编码转换。
  3. 自动变量选择：分割过程优先选择“不纯度降低最多”的特征，本质是自动筛选关键特征（如根节点分割的特征通常是最重要的特征）。
  4. 适配非线性关系：无需假设特征与输出的线性关系，能捕捉复杂非线性和交互作用（如“高收入且低年龄”的客户流失风险高）。
- 缺点：
  1. 高方差、不稳定：训练集微小变化可能导致树结构大幅改变（如删除一个样本，分割规则变化），泛化能力弱（单棵树预测精度通常低于回归、SVM等方法）。
  2. 缺乏平滑性：预测值是区域常数，呈阶梯状，无法输出连续平滑的预测（如回归任务中，相邻特征值的预测值可能突变）。
  3. 易过拟合：未剪枝的树深度大、叶节点多，易拟合训练集噪声。
  4. 偏向多类别特征：分割时，取值多的特征（如ID类特征）更易找到“最优截断点”，导致模型过度依赖此类特征（需提前处理）。

### 8.6 Methods to Improve Trees: Bagging（树改进方法：装袋）
- 核心思想：Bootstrap Aggregation（自助聚合），通过“重采样+集成”降低单棵树的方差，提高稳定性和预测精度。
- 步骤：
  1. 自助采样（Bootstrap Sampling）：从原始训练集（n个样本）中有放回地随机抽取n个样本，生成B个自助样本集（每个样本集可能包含重复样本，约63.2%的原始样本被选中）。
  2. 单树训练：对每个自助样本集，训练一棵决策树（不剪枝或轻度剪枝，保留高方差特性）。
  3. 集成预测：回归任务取B棵树预测值的均值（\(\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^BB\hat{f}^{*b}(x)\)），分类任务取多数投票（预测为B棵树中占比最高的类别）。
- 优势：
  - 降低方差：多棵树的预测结果平均/投票后，抵消单棵树的随机波动（如一棵树叶节点预测10万，另一棵预测12万，平均为11万，更稳定）。
  - 无需额外调参：仅需选择B（树的数量，通常B=100-1000，B足够大后性能趋于稳定）。
  - 对过拟合鲁棒：自助样本集的随机性使单棵树过拟合不同噪声，集成后噪声相互抵消。
- 袋外误差（Out-of-Bag Error）：每个原始样本约36.8%的概率未被某自助样本集选中（袋外样本），用袋外样本评估对应树的误差，平均所有树的袋外误差，可替代交叉验证估计泛化误差（无需额外测试集）。
- 案例深化：用1000个样本训练Bagging模型（B=500棵树），单棵树的测试MSE=300，Bagging的测试MSE=150（方差降低50%）；袋外误差=145，与测试误差接近，验证模型泛化能力。

### 8.7 Methods to Improve Trees: Random Forest（树改进方法：随机森林）
- 问题背景：Bagging的单棵树高度相关（均基于特征空间的最优分割，共享关键特征），集成后方差降低有限；随机森林通过“特征随机选择” decorrelate 树，进一步降低方差。
- 核心改进：在每棵树的每个节点分割时，从p个特征中随机选择m个特征（m<<p，通常m=√p或log2p），仅在这m个特征中选择最优分割点（而非所有p个特征）。
- 优势：
  - 降低树相关性：随机选择特征使单棵树的分割规则不同（如一棵树根节点用特征x₁，另一棵用x₃），树之间相关性降低，集成后方差进一步减小（预测精度高于Bagging）。
  - 特征重要性评估：通过计算“特征被选中分割的次数”或“分割后不纯度降低总量”，量化每个特征的重要性（如特征x₁在80%的树中被选中，且平均降低Gini系数0.3，说明x₁是关键特征）。
- 参数选择：
  - B：树的数量（B=1000足够，再增加无明显提升）。
  - m：每节点随机选择的特征数（m=√p是默认值，如p=100，m=10），m越小，树相关性越低，但方差可能增大，需通过交叉验证优化。
- 案例深化：p=100特征预测销售额，Bagging（m=100，全特征选择）的测试MSE=120，随机森林（m=10）的测试MSE=90（精度提升25%）；特征重要性排名显示，广告投入（x₁）、价格（x₂）、地段（x₃）的重要性得分最高（前3名），与业务逻辑一致。

### 8.8 Methods to Improve Trees: Boosting（树改进方法：提升）
- 核心思想：迭代生成弱学习器（简单决策树，如深度为1的“决策树桩”、深度为3的“小棵树”），每棵树聚焦于前序模型的预测误差（残差），逐步修正模型，最终集成所有弱学习器的预测结果（“慢学习”策略）。
- 步骤（梯度提升，Gradient Boosting）：
  1. 初始化：用简单模型拟合初始预测值（如回归任务取训练集y的均值\(\hat{f}_0(x)=\bar{y}\)，分类任务取多数类别）。
  2. 迭代训练（t=1到B）：
     a. 计算残差：\(r_{it}=y_i-\hat{f}_{t-1}(x_i)\)（前序模型的预测误差，即当前模型需要修正的部分）。
     b. 拟合弱学习器：用残差\(r_{it}\)训练一棵小决策树\(h_t(x)\)（如深度d=3，限制复杂度）。
     c. 计算步长（学习率）λ：通过线搜索选择λ（0<λ≤1，通常λ=0.01-0.1），最小化损失函数（如RSS）。
     d. 更新模型：\(\hat{f}_t(x)=\hat{f}_{t-1}(x)+\lambda h_t(x)\)（缓慢修正，避免过拟合）。
  3. 最终预测：\(\hat{f}(x)=\hat{f}_B(x)\)。
- 关键参数：
  - B：弱学习器数量（B过大易过拟合，需通过交叉验证选择）。
  - λ：学习率（λ越小，需越多B，模型越稳定，泛化能力越强；λ越大，模型收敛快，但易过拟合）。
  - d：弱学习器深度（d越小，模型越简单，如d=1为决策树桩，d=3为常用值）。
- 优势：
  - 预测精度高：逐步修正误差，聚焦难样本（残差大的样本），拟合能力强（通常优于Bagging和随机森林）。
  - 灵活性高：可自定义损失函数（如回归用MSE，分类用对数损失，异常值检测用Huber损失）。
- 缺点：对参数敏感（需精细调参B、λ、d），训练时间长（迭代生成B棵树），对异常值敏感（残差易受异常值影响）。
- 案例深化：回归任务预测房价，Boosting模型（B=1000，λ=0.05，d=3）的测试MSE=80，低于随机森林的90；学习率λ=0.1时，B=500即可达到相近精度（测试MSE=85），但λ=0.2时，B=300就出现过拟合（测试MSE=110）。

## 9. SVM（支持向量机）
### 9.1 Maximal Margin Classifier (Separable Case)（最大间隔分类器：可分情况）
- 适用场景：数据线性可分（存在超平面完全分隔两类样本）。
- 核心定义：超平面是p维空间中的p-1维平面，方程为\(\beta_0+x^T\beta=0\)，其中β是法向量（垂直于超平面），决定超平面方向；β₀是截距，决定超平面位置。对于线性可分数据，存在多个分离超平面，最大间隔分类器是“Margin最大”的超平面（Margin=超平面到最近样本点的距离）。
- 距离公式：样本点xᵢ到超平面的距离为\(d_i=\frac{|β_0+x_i^Tβ|}{||β||}\)（||β||是β的L₂范数），所有样本点满足\(y_i(β_0+x_i^Tβ)>0\)（y_i=1或-1，代表两类），定义最小距离为\(d=\min_i\frac{y_i(β_0+x_i^Tβ)}{||β||}\)，Margin=2d（超平面两侧最近样本点的距离）。
- 优化问题：最大化Margin等价于最小化\(\frac{1}{2}||β||^2\)（简化计算），约束条件为\(y_i(β_0+x_i^Tβ)≥1\)（将最小距离d标准化为1/||β||，Margin=2/||β||）。
- 几何意义：最大间隔超平面位于两类样本的“中间”，离两类最近样本点的距离相等，且距离最大，对新样本的分类容错性最强（如新样本轻微偏离，仍能正确分类）。
- 案例深化：二分类数据（p=2）：类别1=(1,2),(2,3)，类别2=(-1,-2),(-2,-3)，分离超平面为\(x_1+x_2=0\)（β₀=0，β=(1,1)^T），样本点(1,2)到超平面的距离=|1+2|/√2=3/√2，(-1,-2)的距离=|-1-2|/√2=3/√2，Margin=2×(3/√2)=3√2，是所有分离超平面中Margin最大的。

### 9.2 Support Vector Classifier (Non-Separable Case)（支持向量分类器：不可分情况）
- 问题背景：现实数据通常线性不可分（存在重叠样本或异常值），无完美分离超平面，需放松约束，允许部分样本违反Margin或超平面（软间隔）。
- 核心改进：引入松弛变量\(\xi_i≥0\)（衡量样本点的违反程度），约束条件改为\(y_i(β_0+x_i^Tβ)≥1-\xi_i\)：
  - \(\xi_i=0\)：样本点在Margin外侧，分类正确。
  - \(0<\xi_i≤1\)：样本点在Margin内侧，分类正确但违反Margin。
  - \(\xi_i>1\)：样本点在超平面另一侧，分类错误。
- 优化问题：最小化\(\frac{1}{2}||β||^2+C\sum_{i=1}^n\xi_i\)，其中C≥0是惩罚参数（控制软间隔的“严格程度”）：
  - C越小：惩罚越轻，允许更多样本违反Margin，模型偏简单（高偏差、低方差，易欠拟合）。
  - C越大：惩罚越重，不允许样本违反Margin，模型偏复杂（低偏差、高方差，易过拟合，接近最大间隔分类器）。
- 支持向量：满足\(y_i(β_0+x_i^Tβ)=1-\xi_i\)或\(\xi_i>0\)的样本点（即Margin上、Margin内或分类错误的样本），模型仅依赖支持向量（其他样本对超平面无影响）。
- 案例深化：线性不可分数据（类别1=(1,2),(2,3),(1,1)，类别2=(-1,-2),(-2,-3),(0,0)），(0,0)为重叠样本：
  - C=0.1：惩罚轻，允许(0,0)违反Margin（\(\xi_i=0.5\)），超平面更靠近两类中心，Margin较大，测试误差低。
  - C=10：惩罚重，强制(0,0)分类正确（\(\xi_i=0\)），超平面向(0,0)偏移，Margin变小，模型过拟合训练集。
  - 通过交叉验证选择C=1，平衡Margin和违反样本数，测试误差最小。

### 9.3 Optimization and Duality（优化与对偶问题）
- 原始问题：支持向量分类器的优化是“带约束的凸二次规划问题”（目标函数是凸函数，约束是线性函数），变量为β、β₀、\(\xi_i\)，变量个数为p+1+n（p为特征数，n为样本数），当n很大时（如n=10000），直接求解效率低。
- 对偶问题：通过拉格朗日乘数法将原始问题转化为对偶问题，变量为拉格朗日乘数\(\alpha_i≥0\)（个数为n），目标函数仅依赖样本间的内积\(<x_i,x_j>=x_i^Tx_j\)，优化问题为：
  \[
  \max_\alpha\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j<x_i,x_j>
  \]
  约束条件：\(0≤\alpha_i≤C\)，\(\sum_{i=1}^n\alpha_iy_i=0\)。
- 优势：
  - 变量个数为n，当p很大（高维数据）时，对偶问题求解效率更高（内积计算可通过核函数简化）。
  - 导出核函数 trick：对偶问题仅依赖内积，可通过核函数替代原始特征的内积，实现高维特征空间的线性分类（无需显式映射到高维空间）。
- 原始变量与对偶变量的关系：最优β满足\(\beta=\sum_{i=1}^n\alpha_iy_ix_i\)（β是支持向量的线性组合，非支持向量的\(\alpha_i=0\)，对β无贡献）；β₀通过支持向量求解（满足\(y_i(β_0+x_i^Tβ)=1-\xi_i\)）。

### 9.4 KKT Conditions（KKT条件）
- 定义：凸优化问题中，原始问题和对偶问题的最优解需满足的互补松弛条件，支持向量分类器的KKT条件包括：
  1. 平稳性：\(\beta=\sum_{i=1}^n\alpha_iy_ix_i\)，\(\frac{\partial L}{\partial\beta_0}=0\implies\sum_{i=1}^n\alpha_iy_i=0\)，\(\frac{\partial L}{\partial\xi_i}=0\implies\alpha_i+\mu_i=C\)（\(\mu_i≥0\)是\(\xi_i≥0\)的拉格朗日乘数）。
  2. 原始可行性：\(y_i(β_0+x_i^Tβ)≥1-\xi_i\)，\(\xi_i≥0\)。
  3. 对偶可行性：\(0≤\alpha_i≤C\)。
  4. 互补松弛性：\(\alpha_i[y_i(β_0+x_i^Tβ)-(1-\xi_i)]=0\)，\(\mu_i\xi_i=(C-\alpha_i)\xi_i=0\)。
- 关键推论（稀疏性）：
  - 若\(\alpha_i=0\)：样本点满足\(y_i(β_0+x_i^Tβ)≥1\)（在Margin外侧，分类正确），对β无贡献，非支持向量。
  - 若\(0<\alpha_i<C\)：样本点满足\(y_i(β_0+x_i^Tβ)=1\)（在Margin上），\(\xi_i=0\)，是支持向量。
  - 若\(\alpha_i=C\)：样本点满足\(y_i(β_0+x_i^Tβ)≤1\)（在Margin内侧或分类错误），\(\xi_i≥0\)，是支持向量。
- 意义：模型仅依赖支持向量（\(\alpha_i>0\)的样本），非支持向量的位置变化（只要不进入Margin）不影响超平面，体现了SVM的稀疏性和鲁棒性。

### 9.5 Support Vectors（支持向量）
- 定义：满足\(\alpha_i>0\)或\(\xi_i>0\)的样本点，即Margin上、Margin内或分类错误的样本，是决定超平面的关键样本（其他样本对超平面无影响）。
- 核心特性：
  - 稀疏性：支持向量的数量远小于总样本数（尤其是高维数据），使模型存储和计算高效（仅需保存支持向量）。
  - 鲁棒性：超平面由支持向量决定，非支持向量的微小扰动（如删除、移动）不改变超平面，模型稳定性高。
- 识别方法：通过对偶问题的最优解\(\alpha_i\)识别，\(\alpha_i>0\)的样本即为支持向量（\(\alpha_i=0\)的样本为非支持向量）。
- 案例深化：n=1000的二分类数据，支持向量分类器的最优\(\alpha_i\)中，仅80个\(\alpha_i>0\)（支持向量），其余920个\(\alpha_i=0\)（非支持向量）；删除非支持向量后，重新训练模型，超平面与原模型完全一致，说明支持向量是模型的核心。

### 9.6 Support Vector Machine (Kernel Method)（支持向量机：核方法）
- 问题背景：支持向量分类器是线性分类器，无法处理非线性可分数据（如特征空间中两类样本呈圆形分布），需将原始特征映射到高维特征空间，使数据在高维空间线性可分。
- 核函数 trick：直接映射到高维空间（如p维→p²维）会导致“维度灾难”（计算量爆炸），核函数可替代高维空间的内积计算，即\(K(x_i,x_j)=<h(x_i),h(x_j)>\)，其中h(x)是原始特征到高维空间的映射，K(x_i,x_j)是核函数，无需显式计算h(x)。
- 常见核函数：
  - 多项式核（Polynomial Kernel）：\(K(x_i,x_j)=(1+x_i^Tx_j)^d\)，d是多项式次数（d=1时退化为线性核，对应支持向量分类器；d≥2时为非线性核，拟合曲线边界）。
  - 径向基核（RBF Kernel/Gaussian Kernel）：\(K(x_i,x_j)=\exp(-\gamma||x_i-x_j||^2)\)，γ>0是带宽参数（控制核函数的局部性）：
    - γ越大：核函数局部性越强，仅近样本对预测有影响，模型易过拟合（边界复杂，贴合训练集）。
    - γ越小：核函数局部性越弱，远样本也有影响，模型易欠拟合（边界平滑）。
  - Sigmoid核：\(K(x_i,x_j)=\tanh(\gamma x_i^Tx_j+\theta)\)，模拟神经网络，较少使用。
- 非线性SVM：将支持向量分类器的内积替换为核函数，优化问题和预测函数均通过核函数计算，实现高维空间的线性分类（原始空间的非线性分类）。
- 预测函数：\(\hat{y}(x)=sign(\sum_{i=1}^n\alpha_iy_iK(x_i,x)+\beta_0)\)，其中\(\alpha_i\)是对偶问题的最优解（支持向量的\(\alpha_i>0\)）。
- 案例深化：非线性数据（类别1：x₁²+x₂²≤1，类别2：x₁²+x₂²≥2），原始空间线性不可分，用RBF核（γ=1）映射到高维空间后线性可分，SVM的决策边界为圆形（原始空间），测试误差=5%；γ=0.1时，决策边界过于平滑（接近直线），测试误差=20%；γ=10时，决策边界过于复杂（过拟合噪声），测试误差=15%，通过交叉验证选择γ=1为最优。

### 9.7 Summary & Comparison（总结与对比）
- 方法对比：
  | 方法 | 核心特性 | 适用场景 |
  |---|---|---|
  | 最大间隔分类器 | 线性、仅适用于可分数据、Margin最大 | 线性可分、无异常值 |
  | 支持向量分类器 | 线性、适用于不可分数据（软间隔）、稀疏性 | 线性不可分、有少量异常值 |
  | 支持向量机（核方法） | 非线性、核函数映射、泛化能力强 | 非线性可分、高维数据 |
- SVM核心优势：
  1. 泛化能力强：最大Margin准则+核函数 trick，平衡拟合能力和泛化能力，对高维数据（如文本分类、图像特征）表现优异。
  2. 稀疏性：仅依赖支持向量，模型高效。
  3. 鲁棒性：对异常值不敏感（软间隔），核函数适配非线性。
- 扩展应用：
-  - 多分类SVM：通过One-vs-One（每对类别训练二分类SVM，共K(K-1)/2个模型，投票决定类别）或One-vs-Rest（每个类别训练“类别vs其他”的二分类SVM，共K个模型，取概率最大类别）实现。
-  - 支持向量回归（SVR）：将分类的Margin概念扩展到回归，最小化预测误差，同时最大化Margin，支持线性和非线性回归（通过核函数）。
-
## 附录：例题通俗解释与知识点对应

### 1. 概率与统计回顾
- 1.1 城市居民月收入：随机抽样的“样本均值”是总体均值的无偏估计；样本越大，均值越稳定，推断越可靠。
- 1.2 抽奖至少1张中奖：先算“都不中奖”的概率，再用1减去它；这是“反事件”思路，计算更简洁。
- 1.3 医学诊断阳性：阳性不等于患病，需结合“基准患病率（先验）+检测准确性（似然）”，再用贝叶斯公式得到后验概率。
- 1.4 伯努利与正态：伯努利的期望就是成功率p，方差是不确定性p(1-p)；正态的“68%-95%-99.7%法则”帮你用标准差直观理解概率范围。
- 1.5 U(0,4)样本均值近似正态：样本均值在大样本下服从近似正态，能用正态分布算“均值落在区间内”的概率。
- 1.6 泊松的矩估计与MLE一致：泊松的平均发生次数λ既是矩估计又是最大似然的最优值，直觉是“平均次数越接近数据，整体出现的可能性越大”。
- 1.7 平均寿命的CI与z检验：CI展示“合理范围”，假设检验回答“是否显著不同”，P值小于显著性阈值就拒绝原假设。

### 2. 数据探索
- 2.1 电商用户信息属性：不同属性类型用不同方法分析；名义型看频数，有序型看排名，连续型看均值/方差。
- 2.2 外卖平台数据类型：同一业务数据可按任务拆解为“交易集合”“矩阵”“图”，分别服务于关联、统计、关系分析。
- 2.3 满意度调查数据质量：噪声会扰动真实信号，异常值可能是真/错值，缺失值需恰当处理，抽样偏差要从源头纠正。
- 2.4 班级成绩EDA：用数值+图像快速摸底分布中心、离散程度和异常点，决定后续模型选择和数据清洗方向。
- 2.5 汽车数据多图：直方图看分布形态，箱线图看异常点，散点图看变量关系，平行坐标看多维综合差异。

### 3. 统计机器学习基础
- 3.1 广告投入与销售额：统计的“推断”关心系数是否显著，机器学习的“预测”关心新样本是否准确，两者目标不同但互补。
- 3.2 房价基本表述：Y=f(X)+ε，f体现“确定性规律”，ε体现“随机波动”；学习的核心是逼近f，控制噪声影响。
- 3.3 信用卡欺诈：预测用于自动判别交易是否欺诈，推断用于解释“哪些特征最重要”，帮助制定运营策略。
- 3.4 股票回归的MSE：MSE是平均平方误差，越小说明预测越接近真实；但要同时关注训练/测试两端，避免只在训练集好看。

### 4–6. 线性模型与评估
- 猫狗分类的贝叶斯误差（3.5延伸）：即使最优分类器也会错，因为两类的特征分布重叠；这部分误差是“不可还原”的。
- 训练/测试MSE差异（过拟合）：训练好测试差，说明模型记住了训练噪声；需正则化、简化模型或更多数据。
- 拟合Y=X²+ε：线性模型会系统性偏差；加入x²项的多项式回归能捕捉弯曲关系，解释残差形状的“U型”。
- 5折交叉验证：重复“训练/验证”5次并取平均，稳定评估模型泛化能力，避免偶然划分带来的偏差。
- 多元线性回归系数：β₁>0表示变量正向影响，β₂<0表示负向影响；系数大小是平均影响强度，需结合显著性检验解释。
- 简单线性回归闭式解：斜率是“协方差/方差”，截距让直线过“均值点”；与矩阵公式一致，直观体现线性关系。
- t检验与F检验：t检验看单个系数是否显著，F检验看整体模型是否显著；两者回答“这个变量/这个模型有用吗？”
- 残差诊断与杠杆点：残差形状提示模型错配，Q-Q图看正态性，杠杆点是“极端X”，会强烈影响回归线，需核查。
- R²、VIF与虚拟变量：R²解释拟合好不好，VIF诊断共线性（系数不稳），虚拟变量让分类特征进入线性模型。
- 高维回归问题：特征多、样本少会过拟合和系数不稳定；需特征选择、正则化或降维缓解。
- 特征选择与降维：子集选择强调可解释性，Lasso做稀疏、PCA做无监督压缩，PCR/PLS分别基于方差/相关性选成分。
- 子集选择流程：最优子集是“全枚举找最好”，前向/后向是“贪心近似”，实际常以信息准则或CV来定停点。
- 岭回归vs Lasso：岭收缩但不断零，适合多共线；Lasso能自动选特征（系数归零），适合稀疏真实信号。
- PCR vs PLS：PCR先解释X方差再回归；PLS直接找与Y强相关的方向，一般在“X与Y强相关”场景更优。
- 逻辑回归决策边界：线性边界将空间分成两侧，概率>阈值的一侧归为正类；边界位置由系数决定。
- 贝叶斯分类器误差率：给定x的真实正类概率就是最优分类的上限表现，误差率由数据本身决定，任何模型无法超越。
- LDA vs QDA：LDA共享协方差，边界线性、稳定；QDA协方差独立，边界非线性、灵活，样本少时易过拟合。
- 逻辑回归系数解释：系数是“对数几率”的变化率；正系数提高成为正类的几率，负系数降低。
- 最大间隔超平面：Margin是到边界的最小距离的两倍；Margin越大，泛化越强，抗噪能力越好。

### 7. 超越线性
- 阶梯函数vs分段多项式：阶梯只会“跳台阶”，分段多项式能“画斜坡”，后者在边界做连续约束，拟合更平滑。
- 基函数二次回归：把非线性关系拆成“基函数的线性组合”，既灵活又能用线性回归的高效计算。
- 三次样条拟合sin：在各段用三次多项式并保证在断点处连续/可微，得到“光滑贴合”的曲线，避免全局多项式的震荡。
- 自然样条的边界：强制边界为线性，减少边缘“乱飘”，让外推更稳健。
- 平滑样条的λ权衡：λ小拟合噪声，λ大变直线；用CV选λ，平衡拟合度与平滑度，提升泛化能力。
- 局部回归的跨度s：s小专注局部、灵活但易过拟合；s大更平滑但易欠拟合；需根据任务选择平衡点。
- GAM加性结构：每个特征一条曲线，互不干扰，解释性强；若需要交互，需显式添加交互项。

### 8. 树方法
- 二手车回归树：把特征空间切成“盒子”，每个盒子里用均值预测；规则直观，便于业务沟通。
- 客户流失分类树：逐层用“降低不纯度最多”的规则切分，快速得到可解释的决策路径。
- 不纯度衡量示例：基尼/交叉熵比误分类误差更敏感，能更好指导分割优劣。
- 成本复杂度剪枝：先长出最大树，再用α惩罚复杂度剪短，配合CV选出“最稳”的树。
- Bagging袋外误差：不需要额外测试集，袋外样本天然评估泛化，体现方差降低的收益。
- 随机森林重要性：随机子特征分割减少相关性，提升精度，同时给出合理的特征重要性排序。
- Boosting参数影响：学习率越小越稳但需更多弱学习器；深度控制单棵树的复杂度，防止过拟合。

### 9. 支持向量机
- 最大间隔示例：选择能让最近点距离最大的超平面，泛化更好。
- 软间隔与松弛变量：允许少量错误来换取更稳的边界，参数C权衡“少错”与“边界宽”。
- 支持向量稀疏性：只有"+"号的样本（α>0）影响边界，其余样本不影响，训练和预测都更高效。
- RBF核的直觉：γ决定“看多远”；γ太小看得太远变线性，γ太大只看很近会过拟合，用CV选一个“刚刚好”的γ。

## 核心公式速查表

### 概率与统计
- 条件概率：\(P(A|B)=\frac{P(A\cap B)}{P(B)}\)
- 全概率：\(P(B)=\sum_i P(A_i)P(B|A_i)\)
- 贝叶斯：\(P(A_i|B)=\frac{P(A_i)P(B|A_i)}{\sum_j P(A_j)P(B|A_j)}\)
- 期望与方差：离散 \(E[X]=\sum x\,p(x)\)，连续 \(E[X]=\int x f(x)\,dx\)；\(Var(X)=E[X^2]-E[X]^2\)
- 二项/泊松：二项 \(E[X]=np,\ Var(X)=np(1-p)\)；泊松 \(P(X=k)=\frac{\lambda^k e^{-\lambda}}{k!},\ E[X]=Var(X)=\lambda\)
- 标准化与CLT：\(Z=\frac{X-\mu}{\sigma}\)；\(\sqrt{n}(\bar{X}-\mu)/\sigma \sim N(0,1)\)

### 估计与检验
- MLE：\(\hat{\theta}=\arg\max_\theta \prod_{i=1}^n f(x_i;\theta)\)
- μ的95%置信区间：已知σ：\(\bar{X} \pm 1.96\frac{\sigma}{\sqrt{n}}\)；未知σ：\(\bar{X} \pm t_{n-1,0.025}\frac{S}{\sqrt{n}}\)
- 单样本检验：\(z=\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}},\ t=\frac{\bar{X}-\mu_0}{S/\sqrt{n}}\)
- 两样本t检验（等方差）：\(t=\frac{\bar{X}_1-\bar{X}_2}{S_p\sqrt{1/n_1+1/n_2}}\)，\(S_p^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}\)
- 比例的95%CI：\(\hat{p} \pm 1.96\sqrt{\hat{p}(1-\hat{p})/n}\)
- 卡方拟合优度：\(\chi^2=\sum \frac{(O-E)^2}{E}\)

### 线性回归
- OLS：\(\hat{\beta}=(X^\top X)^{-1}X^\top y\)，\(\hat{y}=X\hat{\beta}\)
- RSS/TSS/R^2：\(RSS=\sum(y_i-\hat{y}_i)^2\)，\(TSS=\sum(y_i-\bar{y})^2\)，\(R^2=1-\frac{RSS}{TSS}\)
- t统计：\(t_j=\frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}\)，\(SE(\hat{\beta})=\sqrt{\sigma^2(X^\top X)^{-1}}\)
- F统计：\(F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\)
- 岭回归：\(\hat{\beta}^{ridge}=(X^\top X+\lambda I)^{-1}X^\top y\)
- Lasso：\(\min_\beta \sum (y_i-x_i^\top\beta)^2 + \lambda \sum |\beta_j|\)

### 分类模型
- 逻辑回归：\(p(x)=\frac{1}{1+e^{-x^\top\beta}}\)，\(\log\frac{p}{1-p}=x^\top\beta\)
- 交叉熵损失：\(-\sum [y\log p + (1-y)\log(1-p)]\)
- LDA判别：\(\delta_k(x)=x^\top\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^\top\Sigma^{-1}\mu_k+\log\pi_k\)
- QDA判别：\(\delta_k(x)=-\frac{1}{2}\log|\Sigma_k|-\frac{1}{2}(x-\mu_k)^\top\Sigma_k^{-1}(x-\mu_k)+\log\pi_k\)
- KNN：分类 \(\hat{y}=\text{mode}\)；回归 \(\hat{y}=\frac{1}{K}\sum_{i\in \mathcal{N}_K(x)} y_i\)

### 树方法
- 回归树分割：最小化\(\sum_{m}\sum_{i\in R_m}(y_i-\bar{y}_{R_m})^2\)
- 不纯度：误分类率 \(1-\max_k p_{mk}\)；基尼 \(1-\sum_k p_{mk}^2\)；交叉熵 \(-\sum_k p_{mk}\log p_{mk}\)
- 成本复杂度剪枝：\(C_\alpha(T)=\sum_m N_m Q_m(T) + \alpha |T|\)
- Bagging聚合：\(\hat{f}(x)=\frac{1}{B}\sum_{b=1}^B \hat{f}^{(b)}(x)\)
- Boosting更新：\(f_m(x)=f_{m-1}(x)+\nu\cdot \hat{g}_m(x)\)

### 超越线性与样条
- 三次样条：节点处函数与一/二阶导数连续
- 自然样条：边界二阶导数为0
- 平滑样条：\(\min_f \sum (y_i-f(x_i))^2 + \lambda \int (f''(x))^2 dx\)
- 局部回归权重：\(w_i=K\!\left(\frac{\|x-x_i\|}{h}\right)\)
- GAM：\(y=\alpha+\sum_j f_j(x_j)+\epsilon\)

### 支持向量机
- 线性可分：约束 \(y_i(w^\top x_i+b)\ge 1\)，目标 \(\min \frac{1}{2}\|w\|^2\)，间隔 \(2/\|w\|\)
- 软间隔：\(\min \frac{1}{2}\|w\|^2 + C\sum \xi_i\)，约束 \(y_i(w^\top x_i+b)\ge 1-\xi_i\)
- 对偶：\(\max_\alpha \sum \alpha_i - \frac{1}{2}\sum \alpha_i\alpha_j y_i y_j x_i^\top x_j\)，约束 \(0\le \alpha_i\le C, \sum \alpha_i y_i=0\)
- 核方法：内积替换为核 \(K(x,z)\)，常用线性/多项式/RBF
- 决策函数：\(\hat{y}(x)=\text{sign}\!\left(\sum \alpha_i y_i K(x_i,x) + b\right)\)

### 评估与验证
- 回归误差：训练/测试 \(MSE=\frac{1}{n}\sum (y_i-\hat{y}_i)^2\)
- 分类误差：训练/测试 \(\frac{1}{n}\sum I(\hat{y}_i\neq y_i)\)
- K折交叉验证：\(CV=\frac{1}{K}\sum_{k=1}^K Err^{(k)}\)
- ROC与AUC：\(TPR=\frac{TP}{TP+FN},\ FPR=\frac{FP}{FP+TN}\)，AUC为ROC曲线下面积
- 混淆矩阵指标：Precision/Recall/F1
- 偏差-方差分解：\(E[(Y-\hat{f}(X))^2]=Bias^2+Var+\sigma^2\)
